@comment{elbers-EtAl:2018:clarin,
    abstract = {This paper is describing the needs and technological preconditions of the CLARIN ERIC infrastructure. It introduces how containerization using Docker can help to meet these requirements and fleshes out the build and deployment workflow that CLARIN ERIC is employing to ensure that all the goals of their infrastructure are met in an efficient and sustainable way. In a second step, it is also shown how these same workflows can help researchers, especially in the fields of computational and corpus linguistics, to provide for more easily reproducible research by creating a virtual environment that can provide specific versions of data, programs and algorithms used for certain research questions and make sure that the exact same versions can still be used at a later stage to reproduce the results.},
    author = {Elbers, Willem and Stemle, Egon W. and Moreira, Andr{\'{e}} and K{\"{o}}nig, Alexander and Cattani, Luca and Palma, Martin},
    publisher = {Submitted},
    title = {{The CLARIN ERIC deployment infrastructure and its applicability to reproducible research}},
    type = {Conference proceedings},
    year = {2018}
}
@unpublished{frey-EtAl:2019:cmc2019,
    author = {Frey, Jennifer-Carmen and K\"{o}nig, Alexander and Stemle Egon W.},
    publisher = {Submitted},
    title = {How {FAIR} are {CMC} corpora},
    type = {inproceedings},
    year = {2019}
}
@unpublished{koenig-Stemle:2019:clarin,
    author = {K\"{o}nig, Alexander and Stemle, Egon W.},
    publisher = {Submitted},
    title = {Technical Solutions for Reproducible Research},
    type = {inproceedings},
    year = {2019}
}
@unpublished{abel-Stemle:2019:elex,
    author = {Abel, Andrea and Stemle, Egon W.},
    publisher = {Accepted},
    title = {Language varieties meet {One-Click Dictionary}},
    type = {inproceedings},
    year = {2019}
}
@unpublished{frey-EtAl:2019:cmc2017-postconfbook,
    abstract = {Multilingual speakers communicate in more than one language in daily life and on social media. In order to process or investigate multilingual communication, there is a need for language identification. This study compares the performance of human annotators with automatic ways of language identification on a multilingual (mainly German-Italian-English) social media data set collected in Italy (i.e. South Tyrol). Our results indicate that humans and NLP systems follow their individual techniques to make a decision about multilingual text messages. This results in low agreement when different annotators or NLP systems execute the same task. In general, annotators agree with each other more than NLP systems. However, there is also variation in human agreement depending on the prior establishment of guidelines for the annotation task or not.},
    author = {Frey, Jennifer-Carmen and Stemle, Egon W. and Doğru{\"{o}}z, A. Seza},
    editor = {Wigham, Ciara R. and Stemle, Egon W.},
    publisher = {In Press},
    title = {{Comparison of Automatic vs. Manual Language Identification in Multilingual Social Media Texts}},
    type = {Post-conference volume (cmc-corpora2017)},
    year = {2019}
}
@unpublished{cmc2017-postconfbook,
    abstract = {Communication between humans via networked devices has become an everyday part of people's lives across different generations, cultures, geographical areas, and social classes. Shaped by the specific social and technical context in which it is produced, synchronous and asynchronous computer-mediated communication (CMC) has become increasingly participatory, interactive, and multimodal. User interactions and user-generated social media content offer a wide range of research opportunities for a growing multidisciplinary research community to examine themes that often relate to - but are not limited to - the interaction between language, CMC, and society. The ambition of this still-growing research community is for the research into CMC to be based on the availability of large, structured data sets, as is the case for many scientific communities. These data sets (corpora) are often built collaboratively from the work of different research teams and disseminated across the research community so that they may form the basis for new analyses and comparative or counter-analyses. With this in mind, in the mid-2000s, a growing number of projects started to collect and structure CMC corpora and diffuse these empirical resources that cover a broad range of CMC genres and languages to both the wider scientific community and business enterprises that develop approaches and tools for web mining, opinion and trend detection, semantic content analysis, or machine translation. Since 2013, the CMC and Social Media Corpora conference series1 has brought together researchers, principally in the fields of social sciences and digital humanities, with interests ranging from the collection, development of methodology, annotation, processing to the analysis of CMC corpora, and representatives of language resource infrastructure initiatives2 and business enterprises. Held annually, the conference series has helped animate discussions around the linguistic, technical, and ethical challenges involved in building and analysing CMC corpora and diffuse best practices across Europe and beyond concerning approaches, resources, tools, and methodologies when working with large collections of CMC data. The ambition has been to encourage researchers to diffuse the CMC corpora created by local project teams and facilitate dialogue that will help the community to work towards standards in building and using CMC corpora so as to encourage interoperability between the resources created within different research teams. Recent editions of the conference have also seen the inclusion of workshops organised as practical introductions to coding schemes for CMC data. The results of previous conferences have been published in the form of a special issue of the Journal of Language Technology and Computational Linguistics (Bei{\ss}wenger et al., 2014) and as monographs: Corpus de communication m{\'{e}}di{\'{e}}e par les r{\'{e}}seaux : Construction, structuration, analyse (Wigham {\&} Ledegen, 2017) and Investigating Computer-Mediated Communication: Corpus-Based Approaches to Language in the Digital World (Fi{\v{s}}er {\&} Bei{\ss}wenger, 2017). The call for papers for this edited volume was also open to authors who did not present papers at the conference. It includes seven contributions, all subject to double peer review, written by 14 authors from eight different institutions in seven countries. One paper is an original paper and the other six are extended papers from the 2017 edition of the CMC and Social Media Corpora Conference held in Bolzano, Italy. Online proceedings of all papers presented at the conference were published as an open-access resource (Stemle {\&} Wigham, 2017). The contributions to this edited volume include two methodological papers that focus on building and annotating CMC corpora and five contributions that offer a sociolinguistic analysis of different CMC corpora. In the latter contributions, distinct CMC genres are represented, including the social media platform Twitter, the social network Facebook, online newspapers and wikis (Wikipedia talk pages and Wikipedia articles). The volume is divided into three thematic sections: CMC Corpus repurposing, Language representation in CMC Corpora, and CMC Language use.},
    author = {Wigham, Ciara R. and Stemle, Egon W.},
    publisher = {In Press},
    series = {Cahiers du Laboratoire de Recherche sur le Langage},
    title = {{Building computer-mediated communication corpora for socio-linguistic analysis (Post-conference volume)}},
    type = {Book},
    year = {2019}
}
@unpublished{stemle-EtAl:2019:lcr-postconf,
    abstract = {In this article we give an overview of first-hand experiences and starting points for best practices from projects in seven European countries dedicated to learner corpus research and the creation of language learner corpora. The corpora and tools involved in LCR are becoming more and more important, and the careful preparation and easy retrieval, and reusability of corpora and tools has likewise become more important. But with a lack of agreed solutions for many aspects of LCR, interoperability between learner corpora or exchanging data from different learner corpus projects is still challenging. We will illustrate how concepts like metadata, anonymization, error taxonomies and linguistic annotations, as well as tools, toolchains or data formats can individually pose challenges and how they might be solved.},
    author = {Stemle, Egon W. and Boyd, Adriane and Janssen, Maarten and {Lindstr{\"{o}}m Tiedemann}, Therese and {Mikeli{\'{c}} Preradovi{\'{c}}}, Nives and Rosen, Alexandr and Ros{\'{e}}n, Dan and Volodina, Elena},
    publisher = {In Press},
    title = {{Working together towards an ideal infrastructure for language learner corpora}},
    type = {Post-conference volume},
    year = {2019}
}
@inproceedings{abel-stemle:2018:euralex,
    abstract = {The goal of the project STyrLogisms is to semi-automatically extract neologism (new lexemes) candidates for the German standard variety used in South Tyrol. We use a list of manually vetted URLs from news, magazines and blog websites of South Tyrol and regularly crawl their data, clean and process it and compare this new data to reference corpora and additional regional word lists and the formerly crawled data sets. Our reference corpora are DECOW14 with around 60m types, and the South Tyrolean Web Corpus with around 2.4m types; the additional word lists consist of named entities, terminological terms from the region, and specific terms of the German standard variety used in South Tyrol (altogether around 53k unique types). Here, we will report on the employed method, a first round of candidate extraction with an approach for a classification schema for the selected candidates, and some remarks on a second extraction round.},
    address = {Ljubljana, SI},
    author = {Abel, Andrea and Stemle, Egon W.},
    booktitle = {Proceedings of the XVIII EURALEX International Congress: Lexicography in Global Contexts},
    doi = {10.4312/9789610600961},
    editor = {{\v{C}}ibej, Jaka and Gorjanc, Vojko and Kosem, Iztok and Krek, Simon},
    isbn = {978-961-06-0097-8},
    month = aug,
    pages = {535--544},
    publisher = {Ljubljana University Press, Faculty of Arts},
    title = {{On the Detection of Neologism Candidates as Basis for Language Observation and Lexicographic Endeavours: The STyrLogism Project}},
    type = {Paper},
    year = {2018}
}
@inproceedings{stemle-onysko:2018:naacl-flpst,
    abstract = {This article describes the system that participated in the shared task (ST) on metaphor detection on the Vrije University Amsterdam Metaphor Corpus (VUA). The ST was part of the workshop on processing figurative language at the 16th annual conference of the North American Chapter of the Association for Computational Linguistics (NAACL2018). The system combines a small assertion of trending techniques, which implement matured methods from NLP and ML; in particular, the system uses word embeddings from standard corpora and from corpora representing different proficiency levels of language learners in a LSTM BiRNN architecture. The system is available under the APLv2 open-source license.},
    address = {Stroudsburg, PA, USA},
    author = {Stemle, Egon and Onysko, Alexander},
    booktitle = {Proceedings of the Workshop on Figurative Language Processing},
    doi = {10.18653/v1/W18-0918},
    month = jun,
    pages = {133--138},
    publisher = {Association for Computational Linguistics},
    title = {{Using Language Learner Data for Metaphor Detection}},
    url = {http://aclweb.org/anthology/W18-0918},
    year = {2018}
}
@misc{egartervigl-stemle:2018:academia,
    abstract = {Interview in Academia (science magazine by EURAC and unibz), Bolzano, Italy},
    author = {{Egarter Vigl}, Lukas and Stemle, Egon},
    booktitle = {Academia-Interview Titelthema},
    month = may,
    pages = {10--11},
    title = {{Was darf Forschung mit Social Media Daten?}},
    type = {magazine},
    url = {http://www.academia.bz.it/articles/was-darf-forschung-mit-social-media-daten},
    volume = {78},
    year = {2018}
}
@book{cmc-corpora:2017,
    abstract = {This volume presents the proceedings of the 5th edition of the annual conference series on CMC and Social Media Corpora for the Humanities (cmc-corpora2017). This conference series is dedicated to the collection, annotation, processing, and exploitation of corpora of computer-mediated communication (CMC) and social media for research in the humanities. The annual event brings together language-centered research on CMC and social media in linguistics, philologies, communication sciences, media and social sciences with research questions from the fields of corpus and computational linguistics, language technology, text technology, and machine learning. The 5th Conference on CMC and Social Media Corpora for the Humanities was held at Eurac Research on October, 4th and 5th, in Bolzano, Italy. This volume contains extended abstracts of the invited talks, papers, and extended abstracts of posters presented at the event. The conference attracted 26 valid submissions. Each submission was reviewed by at least two members of the scientific committee. This committee decided to accept 16 papers and 8 posters of which 14 papers and 3 posters were presented at the conference. The programme also includes three invited talks: two keynote talks by Aivars Glaznieks (Eurac Research, Italy) and A. Seza Doğru{\"{o}}z (Independent researcher) and an invited talk on the Common Language Resources and Technology Infrastructure (CLARIN) given by Darja Fi{\v{s}}er, the CLARIN ERIC Director of User Involvement.},
    address = {Bolzano, Italy},
    doi = {10.5281/zenodo.1040875},
    editor = {Stemle, Egon W. and Wigham, Ciara R.},
    month = oct,
    title = {{Proceedings of the 5th Conference on CMC and Social Media Corpora for the Humanities}},
    type = {proceedings},
    url = {https://zenodo.org/record/1040875},
    year = {2017}
}
@inproceedings{beisswenger-EtAl:2017:cmc-corpora,
    abstract = {The paper reports on the results of a scientific colloquium dedicated to the creation of standards and best practices which are needed to facilitate the integration of language resources for CMC stemming from different origins and the linguistic analysis of CMC phenomena in different languages and genres. The key issue to be solved is that of interoperability – with respect to the structural representation of CMC genres, linguistic annotations metadata, and anonymization/pseudonymization schemas. The objective of the paper is to convince more projects to partake in a discussion about standards for CMC corpora and for the creation of a CMC corpus infrastructure across languages and genres. In view of the broad range of corpus projects which are currently underway all over Europe, there is a great window of opportunity for the creation of standards in a bottom-up approach.},
    address = {Bolzano, Italy},
    author = {Bei{\ss}wenger, Michael and Wigham, Ciara R. and Etienne, Carole and Fi{\v{s}}er, Darja and Su{\'{a}}rez, Holger Grumt and Herzberg, Laura and Hinrichs, Erhard and Horsmann, Tobias and Karlova-Bourbonus, Natali and Lemnitzer, Lothar and Longhi, Julien and L{\"{u}}ngen, Harald and Ho-Dac, Lydia-Mai and Parisse, Christophe and Poudat, C{\'{e}}line and Schmidt, Thomas and Stemle, Egon and Storrer, Angelika and Zesch, Torsten},
    booktitle = {Proceedings of the 5th Conference on CMC and Social Media Corpora for the Humanities},
    doi = {10.5281/zenodo.1041877},
    editor = {Stemle, Egon W. and Wigham, Ciara R.},
    month = oct,
    title = {{Connecting Resources: Which Issues have to be Solved to Integrate CMC Corpora from Heterogeneous Sources and for Different Languages?}},
    year = {2017},
    url = {https://zenodo.org/record/1041877}
}
@inproceedings{beisswenger-EtAl:2016:clarin-long,
    abstract = {The paper presents best practices and results from projects dedicated to the creation of corpora of computer-mediated communication and social media interactions (CMC) from four different countries. Even though there are still many open issues related to building and annotating corpora of this type, there already exists a range of tested solutions which may serve as a starting point for a comprehensive discussion on how future standards for CMC corpora could (and should) be shaped like.},
    author = {Bei{\ss}wenger, Michael and Chanier, Thierry and Erjavec, Toma{\v{z}} and Fi{\v{s}}er, Darja and Herold, Axel and Lube{\v{s}}i{\'{c}}, Nikola and L{\"{u}}ngen, Harald and Poudat, C{\'{e}}line and Stemle, Egon and Storrer, Angelika and Wigham, Ciara},
    booktitle = {Selected Papers from the CLARIN Annual Conference 2016, Aix-en-Provence, 26–28 October 2016, CLARIN Common Language Resources and Technology Infrastructure},
    issn = {1650-3740},
    keywords = {CMC corpora,TEI,community building,computer-mediated communication,corpus annotation,language resources,social media corpora},
    month = may,
    pages = {1--18},
    publisher = {Link{\"{o}}ping University Electronic Press, Link{\"{o}}pings universitet},
    title = {{Closing a Gap in the Language Resources Landscape: Groundwork and Best Practices from Projects on Computer-mediated Communication in four European Countries}},
    url = {http://www.ep.liu.se/ecp/article.asp?issue=136&article=001},
    year = {2017}
}
@inproceedings{stemle:2016:evalita,
    abstract = {This article describes the system that participated in the POS tagging for Italian Social Media Texts (PoSTWITA) task of the 5th periodic evaluation campaign of Natural Language Processing (NLP) and speech tools for the Italian language EVALITA 2016. The work is a continuation of Stemle (2016) with minor modifications to the system and different data sets. It combines a small assertion of trending techniques, which implement matured methods, from NLP and ML to achieve competitive results on PoS tagging of Italian Twitter texts; in particular, the system uses word embeddings and character-level representations of word beginnings and endings in a LSTM RNN architecture. Labelled data (Italian UD corpus, DiDi and PoSTWITA) and unlabbelled data (Italian C4Corpus and PAISA') were used for training. The system is available under the APLv2 open-source license.},
    address = {Napoli, Italy},
    author = {Stemle, Egon W},
    booktitle = {Proceedings of Third Italian Conference on Computational Linguistics (CLiC-it 2016) {\&} Fifth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2016)},
    editor = {Basile, Pierpaolo and Corazza, Anna and Cutugno, Franco and Montemagni, Simonetta and Nissim, Malvina and Patti, Viviana and Semeraro, Giovanni and Sprugnoli, Rachele},
    month = dec,
    title = {{bot.zen @ EVALITA 2016 - A minimally-deep learning PoS-tagger (trained for Italian Tweets)}},
    year = {2016}
}
@inproceedings{frey-glaznieks-stemle:2016:didi,
    abstract = {The DiDi corpus of South Tyrolean data of computer-mediated communication (CMC) is a multilingual sociolinguistic language corpus. It consists of around 600,000 tokens collected from 136 profiles of Facebook users residing in South Tyrol, Italy. In conformity with the multilingual situation of the territory, the main languages of the corpus are German and Italian (followed by English). The data has been manually anonymised and provides manually corrected part-of-speech tags for the Italian language texts and manually normalised data for German texts. Moreover, it is annotated with user-provided socio-demographic data (among others L1, gender, age, education, and internet communication habits) from a questionnaire, and linguistic annotations regarding CMC phenomena, languages and varieties. The anonymised corpus is freely available for research purposes.},
    address = {Napoli, Italy},
    author = {Frey, Jennifer-Carmen and Glaznieks, Aivars and Stemle, Egon W.},
    booktitle = {Proceedings of Third Italian Conference on Computational Linguistics (CLiC-it 2016) {\&} Fifth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2016)},
    editor = {Basile, Pierpaolo and Corazza, Anna and Cutugno, Franco and Montemagni, Simonetta and Nissim, Malvina and Patti, Viviana and Semeraro, Giovanni and Sprugnoli, Rachele},
    month = dec,
    title = {{The DiDi Corpus of South Tyrolean CMC Data: A multilingual corpus of Facebook texts}},
    year = {2016}
}
@inproceedings{abel-EtAl:2016:koko,
    abstract = {This paper describes an extended version of the KoKo corpus (version KoKo4, Dec 2015), a corpus of written German L1 learner texts from three different German-speaking regions in three different countries. The KoKo corpus is richly annotated with learner language features on different linguistic levels such as errors or other linguistic characteristics that are not deficit-oriented, and is enriched with a wide range of metadata. This paper complements a previous publication (Abel et al., 2014a) and reports on new textual metadata and lexical annotations and on the methods adopted for their manual annotation and linguistic analyses. It also briefly introduces some linguistic findings that have been derived from the corpus.},
    address = {Napoli, Italy},
    author = {Abel, Andrea and Glaznieks, Aivars and Nicolas, Lionel and Stemle, Egon},
    booktitle = {Proceedings of Third Italian Conference on Computational Linguistics (CLiC-it 2016) {\&} Fifth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2016)},
    editor = {Basile, Pierpaolo and Corazza, Anna and Cutugno, Franco and Montemagni, Simonetta and Nissim, Malvina and Patti, Viviana and Semeraro, Giovanni and Sprugnoli, Rachele},
    month = dec,
    title = {{An extended version of the KoKo German L1 Learner corpus}},
    year = {2016}
}
@inproceedings{beisswenger-EtAl:2016:clarin,
    author = {Bei{\ss}wenger, Michael and Chanier, Thierry and Chiari, Isabella and Erjavec, Toma{\v{z}} and Fi{\v{s}}er, Darja and Herold, Axel and Lube{\v{s}}i{\'{c}}, Nikola and L{\"{u}}ngen, Harald and Poudat, C{\'{e}}line and Stemle, Egon and Storrer, Angelika and Wigham, Ciara},
    booktitle = {Proceedings of the CLARIN Annual Conference 2016},
    keywords = {CMC corpora,TEI,community building,computer-mediated communication,corpus annotation,language resources,social media corpora},
    abstract = {The paper presents best practices and results from projects in four CLARIN member countries dedicated to the creation of corpora of computer-mediated communication and social media interactions (CMC). Even though there are still many open issues related to building and annotating corpora of that type, there already exists a range of accessible solutions which have been tested in projects and which may serve as a starting point for a more precise discussion of how future standards for CMC corpora may (and should) be shaped like.},
    month = oct,
    title = {{Integrating corpora of computer-mediated communication into the language resources landscape: Initiatives and best practices from French, German, Italian and Slovenian projects}},
    year = {2016}
}
@inproceedings{stemle:2016:WAC-X,
    title = {{bot.zen @ EmpiriST 2015 - A minimally-deep learning PoS-tagger (trained for German CMC and Web data)}},
    abstract = {This article describes the system that participated in the Part-of-speech tagging subtask of the "EmpiriST 2015 shared task on automatic linguistic annotation of computer-mediated communication / social media". The system combines a small assertion of trending techniques, which implement matured methods, from NLP and ML to achieve competitive results on PoS tagging of German CMC and Web corpus data; in particular, the system uses word embeddings and character-level representations of word beginnings and endings in a LSTM RNN architecture. Labelled data (Tiger v2.2 and EmpiriST) and unlabelled data (German Wikipedia) were used for training. The system is available under the APLv2 open-source license.},
    author = {Stemle, Egon W.},
    booktitle = {Proceedings of the 10th Web as Corpus Workshop (WAC-X) and the EmpiriST Shared Task},
    month = aug,
    pages = {115--119},
    publisher = {Association for Computational Linguistics},
    url = {http://anthology.aclweb.org/W/W16/W16-2614},
    year = {2016}
}
@book{WAC-X:2016,
    title = {{Proceedings of the 10th Web as Corpus Workshop (WAC-X) and the EmpiriST Shared Task}},
    abstract = {The World Wide Web has become increasingly popular as a source of linguistic data, not only within the NLP communities, but also with theoretical linguists facing problems of data sparseness or data diversity. Accordingly, web corpora continue to gain importance, given their size and diversity in terms of genres/text types. The field is still new, though, and a number of issues in web corpus construction need much additional research, both fundamental and applied. These issues range from questions of corpus design (e.g., assessment of corpus composition, sampling strategies and their relation to crawling algorithms, and handling of duplicated material) to more technical aspects (e.g., efficient implementation of individual post-processing steps in document cleaning and linguistic annotation, or large-scale parallelization to achieve web-scale corpus construction). Similarly, the systematic evaluation of web corpora, for example in the form of task based comparisons to traditional corpora, has only recently shifted into focus. For almost a decade, the ACL SIGWAC (http://www.sigwac.org.uk/), and especially the highly successful Web as Corpus (WAC) workshops have served as a platform for researchers interested in compilation, processing and application of web-derived corpora. Past workshops were co-located with major conferences on computational linguistics and/or corpus linguistics (such as EACL, NAACL, LREC, WWW, and Corpus Linguistics). WAC-X also featured the final workshop of the EmpiriST 2015 shared task "Automatic Linguistic Annotation of Computer-Mediated Communication / Social Media" (see https://sites.google.com/site/empirist2015/ for details) and the panel discussion "Corpora, open science, and copyright reforms" (see https://www.sigwac.org.uk/wiki/WAC-X{\#}paneldisc for details).},
    annote = {Workshop at ACL2016, Berlin, Germany},
    editor = {Cook, Paul and Evert, Stefan and Sch{\"{a}}fer, Roland and Stemle, Egon},
    month = aug,
    publisher = {Association for Computational Linguistics},
    type = {proceedings},
    url = {http://anthology.aclweb.org/W/W16/W16-26},
    year = {2016}
}
@incollection{NicolasStemleGlaznieksAbel2014,
    abstract = {We present an abstract and generic workflow, and detail how it has been implemented to build and annotate learner corpora. This workflow has been developed through an interdisciplinary collaboration between linguists, who annotate and use corpora, and computational linguists and computer scientists, who are responsible for providing technical support and adaptation or implementation of software components.},
    address = {Bern, Switzerland},
    author = {Nicolas, Lionel and Stemle, Egon and Glaznieks, Aivars and Abel, Andrea},
    booktitle = {Studies in Learner Corpus Linguistics: Research and Applications for Foreign Language Teaching and Assessment},
    chapter = {18},
    doi = {10.3726/978-3-0351-0736-4},
    editor = {Castello, Erik and Ackerley, Katherine and Coccetta, Francesca},
    isbn = {978-3-0351-0736-4},
    month = jun,
    pages = {337--351},
    publisher = {Peter Lang},
    series = {Linguistic Insights},
    title = {{A Generic Data Workflow for Building Annotated Text Corpora}},
    volume = {190},
    year = {2015}
}
@inproceedings{FreyGlaznieksStemle2015,
    abstract = {This paper presents the DiDi Corpus, a corpus of South Tyrolean Data of Computer-mediated Communication (CMC). The corpus comprises around 650,000 tokens from Facebook wall posts, comments on wall posts and private messages, as well as socio-demographic data of participants. All data was automatically annotated with language information (de, it, en and others), and manually normalised and anonymised. Furthermore, semi-automatic token level annotations include part-of-speech and CMC phenomena (e.g. emoticons, emojis, and iteration of graphemes and punctuation). The anonymised corpus without the private messages is freely available for researchers; the complete and anonymised corpus is available after signing a non- disclosure agreement.},
    address = {Essen},
    author = {Frey, Jennifer-Carmen and Glaznieks, Aivars and Stemle, Egon W.},
    booktitle = {Proceedings of the 2nd Workshop on Natural Language Processing for Computer-Mediated Communication / Social Media at GSCL2015 (NLP4CMC2015)},
    month = sep,
    publisher = {German Society for Computational Linguistics \& Language Technology},
    title = {{The DiDi Corpus of South Tyrolean CMC Data}},
    url = {https://sites.google.com/site/nlp4cmc2015/NLP4CMC-2015.pdf},
    type = {article},
    year = {2015}
}
@inproceedings{KranebitterStemle2013,
    abstract = {Graphical tools to organise and represent knowledge are useful in terminology work to facilitate building concept systems. Creating and maintaining hierarchically structured concept relation maps while manually gathering data for terminological databases helps to gain and maintain an overview of concept relations, supports terminology work in groups, and helps new team members catching up on the subject field. This article describes our approach to support the building of concept systems in comparative legal terminology using the concept mapping software CmapTools (IHMC): we build hierarchically structured concept relation maps where linking lines with arrowheads between concepts of the same legal system represent generic-specific relations, and combined concept relation maps where dashed lines without arrowheads connect similar concepts in different legal systems.},
    address = {Cham\'{e}bry, France},
    author = {Kranebitter, Klara and Stemle, Egon W.},
    booktitle = {Terminologie \& Ontologie: Th\'{e}ories et Applications. Actes de la septi\`{e}me conf\'{e}rence TOTh 2013},
    editor = {Roche, Christophe and Costa, Rute and Depecker, Lo\"{\i}c and Thoiron, Philippe},
    month = jun,
    pages = {97--116},
    publisher = {Institut Porphyre, Savoir et Connaissance},
    title = {{Constructing concept relation maps to support building concept systems in comparative legal terminology}},
    year = {2013}
}
@article{stemle-onysko:2014,
    abstract = {This article focuses on automatic text classification which aims at identifying the first language (L1) background of learners of English. A particular question arising in the context of automated L1 identification is whether any features that are informative for a machine learning algorithm relate to L1-specific transfer phenomena. In order to explore this issue further, we discuss the results of a study carried out in the wake of a Native Language Identification Task. The task is based on the TOEFL11 corpus (cf. Blanchard et al. 2013), which involves a sample of 12,100 essays written by participants in the TOEFL® test from 11 different language backgrounds (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, and Turkish). The article will show our results in automatic L1 detection in the TOEFL11 corpus. These results are discussed in light of relevant transfer features which turned out to be particularly informative for automatic detection of L1 German and L1 Italian.},
    author = {Stemle, Egon and Onysko, Alexander},
    _doi = {10.1075/hsld.4.13ste},
    editor = {Peukert, Hagen},
    journal = {Transfer Effects in Multilingual Language Development},
    month = apr,
    pages = {297--321},
    publisher = {John Benjamins},
    series = {Hamburg Studies on Linguistic Diversity},
    title = {{Automated L1 identification in English learner essays and its implications for language transfer}},
    url = {https://benjamins.com/catalog/hsld.4.13ste},
    volume = {4},
    year = {2015}
}
@inproceedings{ABEL14.934,
    abstract = {We introduce the KoKo corpus, a collection of German L1 learner texts annotated with learner errors, along with the methods and tools used in its construction and evaluation. The corpus contains both texts and corresponding survey information from 1,319 pupils and amounts to around 716,000 tokens. The evaluation of the quality of the performed transcriptions and annotations shows an accuracy of orthographic error annotations of approximately 80\% as well as high accuracy of transcriptions (> 99\%), automatic tokenisation (> 99\%), sentence splitting (> 96\%) and POS-tagging (> 94\%). The KoKo corpus will be published at the end of 2014 and be the first accessible linguistically annotated German L1 learner corpus. It will represent a valuable source for research and teaching on German as L1 language, in particular with regards to writing skills.},
    address = {Reykjavik, Iceland},
    author = {Abel, Andrea and Glaznieks, Aivars and Nicolas, Lionel and Stemle, Egon},
    booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)},
    editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Stelios, Piperidis},
    isbn = {978-2-9517408-8-4},
    keywords = {German Language,Learner Corpora},
    month = may,
    pages = {2414--2421},
    publisher = {European Language Resources Association (ELRA)},
    title = {{KoKo: An L1 Learner Corpus for German}},
    url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/934\_Paper.pdf},
    year = {2014}
}
@inproceedings{LYDING14.517,
    abstract = {In this article, we present interHist, a compact visualization for the interactive exploration of results to complex corpus queries. Integrated with a search interface to the PAIS\`{A} corpus of Italian web texts, interHist aims at facilitating the exploration of large results sets to linguistic corpus searches. This objective is approached by providing an interactive visual overview of the data, which supports the user-steered navigation by means of interactive filtering. It allows to dynamically switch between an overview on the data and a detailed view on results in their immediate textual context, thus helping to detect and inspect relevant hits more efficiently. We provide background information on corpus linguistics and related work on visualizations for language and linguistic data. We introduce the architecture of interHist, by detailing the data structure it relies on, describing the visualization design and providing technical details of the implementation and its integration with the corpus querying environment. Finally, we illustrate its usage by presenting a use case for the analysis of the composition of Italian noun phrases.},
    address = {Reykjavik, Iceland},
    author = {Lyding, Verena and Nicolas, Lionel and Stemle, Egon},
    booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)},
    editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Stelios, Piperidis},
    isbn = {978-2-9517408-8-4},
    keywords = {corpus linguistics,language analysis,visualization},
    month = may,
    pages = {635--641},
    publisher = {European Language Resources Association (ELRA)},
    title = {{'interHist' - an interactive visual interface for corpus exploration}},
    url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/517\_Paper.pdf},
    year = {2014}
}
@inproceedings{GenereuxStemleNicolasLyding2014,
    abstract = {In this paper, we present ongoing experiments for correcting OCR errors on German newspapers in Fraktur font. Our approach borrows from techniques for spelling correction in context using a probabilistic edit-operation error model and lexical resources. We highlight conditions in which high error reduction rates can be obtained and where the approach currently stands with real data.},
    address = {Pisa, Italy},
    author = {G\'{e}n\'{e}reux, Michel and Stemle, Egon W. and Nicolas, Lionel and Lyding, Verena},
    booktitle = {Proceedings of the First Italian Conference on Computational Linguistics (CLiC-it 2014)},
    editor = {Basili, Roberto and Lenci, Alessandro and Magnini, Bernardo},
    month = dec,
    title = {{Correcting OCR errors for German in Fraktur font}},
    url = {http://clic.humnet.unipi.it/proceedings/vol1/CLICIT2014136.pdf},
    year = {2014}
}
@article{GlaznieksNicolasStemleAbelLyding2014,
    abstract = {Decisions at the outset of preparing a learner corpus are of crucial importance for how the corpus can be built and how it can be analysed later on. This paper presents a generic workflow to build learner corpora while taking into account the needs of the users. The workflow results from an extensive collaboration between linguists that annotate and use the corpus and computer linguists that are responsible for providing technical support. The paper addresses the linguists' research needs as well as the availability and usability of language technology tools necessary to meet them. We demonstrate and illustrate the relevance of the workflow using results and examples from our L1 learner corpus of German ("KoKo").},
    annote = {Learner Language, Learner Corpora 2012 (LLLC2012), Oct 5-6, University of Oulu, Finland},
    author = {Glaznieks, Aivars and Abel, Andrea and Lyding, Verena and Nicolas, Lionel and Stemle, Egon},
    editor = {Nikula, Tarja and Takala, Sauli and Yl\"{o}nen, Sabine},
    issn = {1457-9863},
    journal = {Apples - Journal of Applied Language Studies},
    keywords = {German as a first language,L1 learner corpus,corpus building workflow},
    month = dec,
    number = {3},
    pages = {5--20},
    publisher = {Centre for Applied Language Studies, University of Jyv\"{a}skyl\"{a}},
    title = {{Establishing a Standardised Procedure for Building Learner Corpora}},
    url = {http://apples.jyu.fi/ArticleFile/download/535},
    volume = {8},
    year = {2014}
}
@article{GlaznieksStemle2014,
    abstract = {Special Issue: Building and annotating corpora of computer-mediated discourse. Issues and Challenges at the Inteface of Corpus and Computational Linguistics},
    author = {Glaznieks, Aivars and Stemle, Egon},
    editor = {Bei{\ss}wenger, Michael and Oostdijk, Nellek and Storrer, Angelika and van den Heuvel, Henk},
    issn = {2190-6858},
    journal = {Journal for Language Technology and Computational Linguistics (JLCL)},
    month = dec,
    number = {2},
    pages = {31--57},
    publisher = {JLCL},
    title = {{Challenges of building a CMC corpus for analyzing writer's style by age: The DiDi project}},
    url = {http://www.jlcl.org/2014\_Heft2/2GlaznieksStemle.pdf},
    volume = {29},
    year = {2014}
}
@inproceedings{FreyStemleGlaznieks2014,
    abstract = {In this paper, we propose an integrated web strategy for mixed sociolinguistic research methodologies in the context of social media corpora. After stating the particular challenges for building corpora of private, non-public computer-mediated communication, we will present our solution to these problems: a Facebook web application for the acquisition of such data and the corresponding meta data. Finally, we will discuss positive and negative implications for this method.},
    address = {Hildesheim, Germany},
    author = {Frey, Jennifer-Carmen and Stemle, Egon W. and Glaznieks, Aivars},
    booktitle = {Workshop Proceedings of the 12th Edition of the KONVENS Conference},
    editor = {Faa{\ss}, Gertrud and Ruppenhofer, Josef},
    month = oct,
    pages = {11--15},
    publisher = {Universitatsverlag Hildesheim, Germany},
    title = {{Collecting language data of non-public social media profiles}},
    url = {http://www.uni-hildesheim.de/konvens2014/data/konvens2014-workshop-proceedings.pdf},
    year = {2014}
}
@inproceedings{paisa2014,
    abstract = {PAIS\`{A} is a Creative Commons licensed, large web corpus of contemporary Italian. We describe the design, harvesting, and processing steps involved in its creation.},
    address = {Gothenburg, Sweden},
    author = {Lyding, Verena and Stemle, Egon and Borghetti, Claudia and Brunello, Marco and Castagnoli, Sara and Orletta, Felice Dell and Dittmann, Henrik and Lenci, Alessandro and Pirrelli, Vito},
    booktitle = {Proceedings of the 9th Web as Corpus Workshop (WaC-9)},
    month = apr,
    pages = {36--43},
    publisher = {Association for Computational Linguistics},
    title = {{The PAIS\`{A} Corpus of Italian Web Texts}},
    url = {http://aclweb.org/anthology/W14-0406},
    year = {2014}
}
@inproceedings{Nicolas2013a,
    abstract = {In this paper, we report on an unsupervised greedy-style process for acquiring phrase translations from sentence-aligned parallel corpora. Thanks to innovative selection strategies, this process can acquire multiple translations without size criteria, i.e. phrases can have several translations, can be of any size, and their size is not considered when selecting their translations. Even though the process is in an early development stage and has much room for improvements, evaluation shows that it yields phrase translations of high precision that are relevant to machine translation but also to a wider set of applications including memory-based translation or multi-word acquisition.},
    address = {Hissar, Bulgaria},
    author = {Nicolas, Lionel and Stemle, Egon W. and Kranebitter, Klara and Lyding, Verena},
    booktitle = {Proceedings of Recent Advances in Natural Language Processing, RANLP 2013},
    editor = {Angelova, Galia and Bontcheva, Kalina and Mitkov, Ruslan},
    keywords = {Bilingual lexicon,Parallel Corpora,Phrase Translation,Unsupervised Learning},
    month = sep,
    pages = {516--524},
    publisher = {RANLP 2011 Organising Committee / ACL},
    title = {{High-Accuracy Phrase Translation Acquisition Through Battle-Royale Selection}},
    url = {http://aclweb.org/anthology/R/R13/R13-1068.pdf},
    year = {2013}
}
@misc{Stemle2013c,
    abstract = {Article in Academia (science magazine by EURAC and unibz), Bolzano, Italy},
    author = {Stemle, Egon W. and Onysko, Alexander},
    booktitle = {Academia},
    month = dec,
    pages = {24--25},
    title = {{Language as a Detective Story}},
    volume = {64},
    type = {magazine},
    year = {2013}
}
@book{WAC8,
    abstract = {Web corpora and other Web-derived data have become a gold mine for corpus linguistics and natural language processing. The Web is an easy source of unprecedented amounts of linguistic data from a broad range of registers and text types. However, a collection of Web pages is not immediately suitable for exploration in the same way a traditional corpus is. Since the first Web as Corpus Workshop organised at the Corpus Linguistics 2005 Conference, a highly successful series of yearly Web as Corpus workshops provides a venue for interested researchers to meet, share ideas and discuss the problems and possibilities of compiling and using Web corpora. After a stronger focus on application-oriented natural language processing andWeb technology in recent years with workshops taking place at NAACL-HLT 2010, 2011 andWWW2012 the 8thWeb as Corpus Workshop returns to its roots in the corpus linguistics community. Accordingly, the leading theme of this workshop is the application of Web data in language research, including linguistic evaluation of Web-derived corpora as well as strategies and tools for high-quality automatic annotation ofWeb text. The workshop brings together presentations on all aspects of building, using and evaluating Web corpora, with a particular focus on the following topics: applications of Web corpora and other Web-derived data sets for language research automatic linguistic annotation of Web data such as tokenisation, part-of-speech tagging, lemma- tisation and semantic tagging (the accuracy of currently available off-the-shelf tools is still unsatisfactory for many types of Web data) critical exploration of the characteristics of Web data from a linguistic perspective and its applica- bility to language research presentation of Web corpus collection projects or software tools required for some part of this process (crawling, filtering, de-duplication, language identification, indexing, ...)},
    annote = {Workshop at the seventh international Corpus Linguistics conference (CL2013), Lancaster, UK},
    editor = {Evert, Stefan and Stemle, Egon and Rayson, Paul},
    month = jul,
    publisher = {WAC-8 Organising Committee},
    title = {{Proceedings of the 8th Web as Corpus Workshop (WAC-8)}},
    type = {proceedings},
    url = {http://sigwac.org.uk/raw-attachment/wiki/WAC8/wac8-proceedings.pdf},
    year = {2013}
}
@inproceedings{Lyding2013a,
    abstract = {In this article, we present the multi-faceted interface to the open PAIS\`{A} corpus of Italian. Created within the project PAIS\`{A} (Piattaforma per l’Apprendimento dell’Italiano Su corpora Annotati) [1], the corpus is designed to be freely available for non-commercial processing, usage and distribution by the public. Hence, this automatically annotated corpus (for lemma, part-of-speech and dependency information) is exclusively composed of documents licensed under Creative Commons (CC) licenses [2].The dedicated corpus interface is designed to provide flexible, powerful, and easy-to-use modes of corpus access, with the objective to support language learning, language practicing and linguistic analyses. We present in detail the interface’s functionalities and discuss the underlying design decisions. We introduce the four principal components of the interface, describe supported display formats and present two specific features added to increase the interface's relevance for language learning. The main search components are (1) a basic search that adopts a "Google-style" search box, (2) an advanced search that provides elaborated graphical search options, and (3) a search that makes use of the powerful CQP query language of the Open Corpus Workbench [3]. In addition, (4) a filter interface for retrieving full-text corpus documents based on keyword searches is available. It is likewise providing the means for building temporary sub-corpora for specific topics. Users can choose among different display formats for the search results. Besides the established KWIC (KeyWord In Context) and full sentence views, graphical representations of the dependency relation information as well as keyword distributions are available. These dynamic displays are based on a visualisation for dependency graphs [4] and one for Word Clouds [5], which build on latest developments in information visualisation for language data. Two special features for novice learners are integrated into each search component. The first feature is a function for restricting search results to sentences of limited complexity. Search results are automatically filtered based on formal text characteristics such as sentence length, vocabulary, etc. The second is the supply of pre-defined search queries for linguistic constructions such as sentences in passive voice, questions, etc. Finally, we show how the PAIS\`{A} interface can be employed in different language teaching tasks. In particular, we present a complete unit of work aimed at learners of Italian (CEFR level A2/B1) and centered on students’ direct use of the interface and its functionalities. By doing so, we are giving concrete examples for targeted searches and interactions with the provided language material, as well as an exemplification of how the use of the corpus can be integrated with communicative language activities in the classroom.},
    address = {Florence, Italy},
    author = {Lyding, Verena and Borghetti, Claudia and Dittmann, Henrik and Nicolas, Lionel and Stemle, Egon},
    booktitle = {Proceedings of the International Conference ICT for Language Learning, 6th edition},
    isbn = {978-88-6292-423-8},
    keywords = {Corpus Linguistics,Linguistic Visualization,Visualization},
    month = nov,
    publisher = {libreriauniversitaria.it},
    title = {{Open Corpus Interface for Italian Language Learning}},
    url = {http://conference.pixel-online.net/ICT4LL2013/common/download/Paper\_pdf/270-ITL56-FP-Lyding-ICT2013.pdf},
    year = {2013}
}
@inproceedings{NicolasStemleKranebitter2012,
    abstract = {We report on on-going work to derive translations of phrases from parallel corpora. We describe an unsupervised and knowledge-free greedy-style process relying on innovative strategies for choosing and discarding candidate translations. This process manages to acquire multiple translations combining phrases of equal or different sizes. The preliminary evaluation performed confirms both its potential and its interest.},
    address = {Vienna, Austria},
    author = {Nicolas, Lionel and Stemle, Egon W. and Kranebitter, Klara},
    booktitle = {11th Conference on Natural Language Processing, KONVENS 2012, Empirical Methods in Natural Language Processing},
    editor = {Jancsary, Jeremy},
    keywords = {Bilingual lexicon,Parallel Corpora,Phrase Translation,Unsupervised Learning},
    month = sep,
    pages = {471--479},
    publisher = {\"{O}GAI},
    title = {{Towards high-accuracy bilingual phrase acquisition from parallel corpora}},
    url = {http://www.oegai.at/konvens2012/proceedings/68\_nicolas12w/},
    year = {2012}
}
@inproceedings{Bonin:2012:AAT:2392747.2392768,
    abstract = {Developing content extraction methods for Humanities domains raises a number of chal- lenges, from the abundance of non-standard entity types to their complexity to the scarcity of data. Close collaboration with Humani- ties scholars is essential to address these chal- lenges. We discuss an annotation schema for Archaeological texts developed in collabora- tion with domain experts. Its development re- quired a number of iterations to make sure all the most important entity types were included, as well as addressing challenges including a domain-specific handling of temporal expres- sions, and the existence of many systematic types of ambiguity.},
    address = {Jeju, Republic of Korea},
    author = {Bonin, Francesca and Cavulli, Fabio and Noriller, Aronne and Poesio, Massimo and Stemle, Egon W.},
    booktitle = {Proceedings of the Sixth Linguistic Annotation Workshop},
    month = jul,
    number = {July},
    pages = {134--138},
    publisher = {Association for Computational Linguistics},
    series = {LAW VI '12},
    title = {{Annotating Archaeological Texts: An Example of Domain-Specific Annotation in the Humanities}},
    url = {http://dl.acm.org/citation.cfm?id=2392747.2392768},
    year = {2012}
}
@inproceedings{PoesioSDH2011,
    address = {Copenhagen, Denmark},
    author = {Poesio, Massimo and Barbu, Eduard and Bonin, Francesca and Cavulli, Fabio and Ekbal, Asif and Stemle, Egon and Girardi, Christian},
    booktitle = {Proceedings of Supporting Digital Humanities (SDH2011): Answering the unaskable},
    editor = {Maegaard, Bente},
    month = nov,
    title = {{The Humanities Research Portal: Human Language Technology Meets Humanities Publication Archives}},
    year = {2011}
}
@inproceedings{murphy-stemle:2011:DIALECTS,
    abstract = {Small, manually assembled corpora may be available for less dominant languages and dialects, but producing web-scale resources remains a challenge. Even when considerable quantities of text are present on the web, finding this text, and distinguishing it from related languages in the same region can be difficult. For example less dominant variants of English (e.g. New Zealander, Singaporean, Canadian, Irish, South African) may be found under their respective national domains, but will be partially mixed with Englishes of the British and US varieties, perhaps through syndication of journalism, or the local reuse of text by multinational companies. Less formal dialectal usage may be scattered more widely over the internet through mechanisms such as wiki or blog authoring. Here we automatically construct a corpus of Hiberno-English (English as spoken in Ireland) using a variety of methods: filtering by national domain, filtering by orthographic conventions, and bootstrapping from a set of Ireland-specific terms (slang, place names, organisations). We evaluate the national specificity of the resulting corpora by measuring the incidence of topical terms, and several grammatical constructions that are particular to Hiberno-English. The results show that domain filtering is very effective for isolating text that is topic-specific, and orthographic classification can exclude some non-Irish texts, but that selected seeds are necessary to extract considerable quantities of more informal, dialectal text.},
    address = {Edinburgh, Scotland, UK},
    author = {Murphy, Brian and Stemle, Egon W.},
    booktitle = {Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties},
    month = jul,
    pages = {22--29},
    publisher = {Association for Computational Linguistics},
    title = {{PaddyWaC: A Minimally-Supervised Web-Corpus of Hiberno-English}},
    url = {http://www.aclweb.org/anthology/W11-2603},
    year = {2011}
}
@article{EkbalEtAl:2011,
    author = {Ekbal, Asif and Bonin, Francesca and Saha, Sriparna and Stemle, Egon and Barbu, Eduard and Cavulli, Fabio and Girardi, Christian and Poesio, Massimo},
    journal = {Journal for Language Technology and Computational Linguistics (JLCL)},
    month = nov,
    number = {2},
    pages = {39--51},
    title = {{Rapid Adaptation of NE Resolvers for Humanities Domains using Active Annotation}},
    url = {http://www.jlcl.org/2011\_Heft2/9.pdf},
    volume = {26},
    year = {2011}
}
@inproceedings{poesio-EtAl:2011:LaTeCH-2011,
    abstract = {Most existing HLT pipelines assume the input is pure text or, at most, HTML and either ignore (logical) document structure or remove it. We argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved.},
    address = {Portland, OR, USA},
    author = {Poesio, Massimo and Barbu, Eduard and Stemle, Egon W. and Girardi, Christian},
    booktitle = {Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH 2011)},
    month = jun,
    pages = {54--62},
    publisher = {Association for Computational Linguistics},
    title = {{Structure-Preserving Pipelines for Digital Libraries}},
    url = {http://www.aclweb.org/anthology/W11-1508},
    year = {2011}
}
@inproceedings{RodriguezDeloguVersleyStemlePoesio2010,
    address = {Valletta, Malta},
    author = {Rodr\'{i}guez, Kepa Joseba and Delogu, Francesca and Versley, Jannick and Stemle, Egon W. and Poesio, Massimo},
    booktitle = {Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC'10)},
    editor = {Calzolari, Nicoletta and Choukri, Khalid and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Piperidis, Stelios and Rosner, Mike and Tapias, Daniel},
    isbn = {2-9517408-6-7},
    month = may,
    publisher = {European Language Resources Association (ELRA)},
    title = {{Anaphoric Annotation of Wikipedia and Blogs in the Live Memories Corpus}},
    url = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/431\_Paper.pdf},
    year = {2010}
}
@inproceedings{StegerStemle2009,
    abstract = {Algorithmic processing of Web content mostly works on textual contents, neglecting visual information. Annotation tools largely share this deficit as well. We specify requirements for an architecture to overcome both problems and propose an implementation, the KrdWrd system. It uses the Gecko rendering engine for both annotation and feature extraction, providing unified data access in every processing step. Stable data storage and collaboration control scripts for group annotations of massive corpora are provided via a Web interface coupled with a HTTP proxy. A modular interface allows for linguistic and visual data feature extractor plugins. The implementation is suitable for many tasks in theWeb as corpus domain and beyond.},
    address = {Donostia-San Sebastian, Basque Country, Spain},
    author = {Steger, Johannes and Stemle, Egon},
    booktitle = {Proceedings of the Fifth Web as Corpus Workshop (WAC5)},
    editor = {Alegria, I\~{n}aki and Leturia, Igor and Sharoff, Serge},
    month = sep,
    pages = {63--70},
    publisher = {Elhuyar Fundazioa},
    title = {{KrdWrd: Architecture for Unified Processing of Web Content}},
    url = {https://www.sigwac.org.uk/raw-attachment/wiki/WAC5/WAC5\_proceedings.pdf},
    year = {2009}
}
@inproceedings{BlohmCimianoStemle2007,
    author = {Blohm, Sebastian and Cimiano, Philipp and Stemle, Egon},
    booktitle = {Proceedings of the 22nd Conference on Artificial Intelligence (AAAI-07)},
    isbn = {978-1-57735-323-2},
    month = jul,
    pages = {1316--1323},
    publisher = {Association for the Advancement of Artificial Intelligence},
    title = {{Harvesting Relations from the Web - Quantifiying the Impact of Filtering Functions}},
    url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-208.pdf},
    year = {2007}
}
@inproceedings{FIASCO2007,
    address = {Louvain-la-Neuve},
    author = {Bauer, Daniel and Degen, Judith and Deng, Xiaoye and Herger, Priska and Gasthaus, Jan and Giesbrecht, Eugenie and Jansen, Lina and Kalina, Christin and Kr\"{u}ger, Thorben and M\"{a}rtin, Robert and Schmidt, Martin and Scholler, Simon and Steger, Johannes and Stemle, Egon and Evert, Stefan},
    booktitle = {Proceedings of the Third Web as Corpus Workshop (WAC3)},
    editor = {Fairon, C\'{e}drick and Naets, Hubert and Kilgarriff, Adam and de Schryver, Gilles-Maurice},
    month = sep,
    publisher = {Presses universitaires de Louvain},
    title = {{FIASCO: Filtering the Internet by Automatic Subtree Classification, Osnabr\"{u}ck}},
    url = {http://purl.org/stefan.evert/PUB/BauerEtc2007\_FIASCO.pdf},
    year = {2007}
}
@techreport{ASADO2005,
    abstract = {Final Report of the one year cooperation between the Universities of Osnabr\"{u}ck and Hildesheim, and the aircraft manufacturer AIRBUS to research methodologies and technologies to analyze and structure the huge amount of documentation produced during aircraft construction. The work was done in a study project carried out in close cooperation with seven students of cognitive science advised by two lectures of the Institute of Cognitive Science of the University of Osnabr\"{u}ck and with one student of international information management advised by one professor of the Institute of Applied Linguistics of the University of Hildesheim.},
    author = {Bleichner, Martin and Giesbrecht, Eugenie and Gust, Helmar and Leicht, Eva-Maria and Ludewig, Petra and M\"{o}ller, Sabine and M\"{u}ller, Wiebke and Schmidt, Martin and Stefaner, Moritz and Stemle, Egon and Wilke, Katja},
    institution = {Institute of Cognitive Science at the University of Osnabr\"{u}ck and Institute of Applied Linguistics at the University of Hildesheim},
    month = nov,
    title = {{ASADO: The Analysis and Structuring of Aviation Documents - Final Report}},
    year = {2005}
}
