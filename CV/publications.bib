
@inproceedings{abel-EtAl:2016:koko,
  abstract = {This paper describes an extended version of the KoKo corpus (version KoKo4, Dec 2015), a corpus of written German L1 learner texts from three different German-speaking regions in three different countries. The KoKo corpus is richly annotated with learner language features on different linguistic levels such as errors or other linguistic characteristics that are not deficit-oriented, and is enriched with a wide range of metadata. This paper complements a previous publication (Abel et al., 2014a) and reports on new textual metadata and lexical annotations and on the methods adopted for their manual annotation and linguistic analyses. It also briefly introduces some linguistic findings that have been derived from the corpus.},
  author = {Abel, Andrea and Glaznieks, Aivars and Nicolas, Lionel and Stemle, Egon},
  booktitle = {Proceedings of {{Third Italian Conference}} on {{Computational Linguistics}} ({{CLiC}}-it 2016) \& {{Fifth Evaluation Campaign}} of {{Natural Language Processing}} and {{Speech Tools}} for {{Italian}}. {{Final Workshop}} ({{EVALITA}} 2016)},
  date = {2016-12},
  doi = {10.4000/books.aaccademia.1743},
  editor = {Basile, Pierpaolo and Corazza, Anna and Cutugno, Franco and Montemagni, Simonetta and Nissim, Malvina and Patti, Viviana and Semeraro, Giovanni and Sprugnoli, Rachele},
  location = {{Napoli, Italy}},
  title = {An extended version of the {{KoKo German L1 Learner}} corpus},
  url = {http://books.openedition.org/aaccademia/pdf/1743}
}

@inproceedings{abel-stemle:2018:euralex,
  abstract = {The goal of the project STyrLogisms is to semi-automatically extract neologism (new lexemes) candidates for the German standard variety used in South Tyrol. We use a list of manually vetted URLs from news, magazines and blog websites of South Tyrol and regularly crawl their data, clean and process it and compare this new data to reference corpora and additional regional word lists and the formerly crawled data sets. Our reference corpora are DECOW14 with around 60m types, and the South Tyrolean Web Corpus with around 2.4m types; the additional word lists consist of named entities, terminological terms from the region, and specific terms of the German standard variety used in South Tyrol (altogether around 53k unique types). Here, we will report on the employed method, a first round of candidate extraction with an approach for a classification schema for the selected candidates, and some remarks on a second extraction round.},
  author = {Abel, Andrea and Stemle, Egon W.},
  booktitle = {Proceedings of the {{XVIII EURALEX International Congress}}: {{Lexicography}} in {{Global Contexts}}},
  date = {2018-08},
  editor = {Čibej, Jaka and Gorjanc, Vojko and Kosem, Iztok and Krek, Simon},
  isbn = {978-961-06-0097-8},
  location = {{Ljubljana, SI}},
  pages = {535-544},
  publisher = {{Ljubljana University Press, Faculty of Arts}},
  title = {On the {{Detection}} of {{Neologism Candidates}} as {{Basis}} for {{Language Observation}} and {{Lexicographic Endeavours}}: {{The STyrLogism Project}}},
  url = {http://videolectures.net/euralex2018_abel_stemle_endeavors/}
}

@inproceedings{ABEL14.934,
  abstract = {We introduce the KoKo corpus, a collection of German L1 learner texts annotated with learner errors, along with the methods and tools used in its construction and evaluation. The corpus contains both texts and corresponding survey information from 1,319 pupils and amounts to around 716,000 tokens. The evaluation of the quality of the performed transcriptions and annotations shows an accuracy of orthographic error annotations of approximately 80\% as well as high accuracy of transcriptions ({$>$} 99\%), automatic tokenisation ({$>$} 99\%), sentence splitting ({$>$} 96\%) and POS-tagging ({$>$} 94\%). The KoKo corpus will be published at the end of 2014 and be the first accessible linguistically annotated German L1 learner corpus. It will represent a valuable source for research and teaching on German as L1 language, in particular with regards to writing skills.},
  author = {Abel, Andrea and Glaznieks, Aivars and Nicolas, Lionel and Stemle, Egon},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'14)},
  date = {2014-05},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Stelios, Piperidis},
  isbn = {978-2-9517408-8-4},
  keywords = {German Language,Learner Corpora},
  location = {{Reykjavik, Iceland}},
  pages = {2414-2421},
  publisher = {{European Language Resources Association (ELRA)}},
  title = {{{KoKo}}: {{An L1 Learner Corpus}} for {{German}}},
  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/934_Paper.pdf}
}

@report{ASADO2005,
  abstract = {Final Report of the one year cooperation between the Universities of Osnabrück and Hildesheim, and the aircraft manufacturer AIRBUS to research methodologies and technologies to analyze and structure the huge amount of documentation produced during aircraft construction. The work was done in a study project carried out in close cooperation with seven students of cognitive science advised by two lectures of the Institute of Cognitive Science of the University of Osnabrück and with one student of international information management advised by one professor of the Institute of Applied Linguistics of the University of Hildesheim.},
  author = {Bleichner, Martin and Giesbrecht, Eugenie and Gust, Helmar and Leicht, Eva-Maria and Ludewig, Petra and Möller, Sabine and Müller, Wiebke and Schmidt, Martin and Stefaner, Moritz and Stemle, Egon and Wilke, Katja},
  date = {2005-11},
  institution = {{Institute of Cognitive Science at the University of Osnabrück and Institute of Applied Linguistics at the University of Hildesheim}},
  title = {{{ASADO}}: {{The Analysis}} and {{Structuring}} of {{Aviation Documents}} - {{Final Report}}}
}

@inproceedings{beisswenger-EtAl:2016:clarin,
  author = {Beißwenger, Michael and Chanier, Thierry and Chiari, Isabella and Erjavec, Tomaž and Fišer, Darja and Herold, Axel and Lubešić, Nikola and Lüngen, Harald and Poudat, Céline and Stemle, Egon and Storrer, Angelika and Wigham, Ciara},
  booktitle = {Proceedings of the {{CLARIN Annual Conference}} 2016},
  date = {2016-10},
  keywords = {CMC corpora,community building,computer-mediated communication,corpus annotation,language resources,social media corpora,TEI},
  title = {Integrating corpora of computer-mediated communication into the language resources landscape: {{Initiatives}} and best practices from {{French}}, {{German}}, {{Italian}} and {{Slovenian}} projects},
  url = {https://pdfs.semanticscholar.org/9ff7/12752eb2ec362d5eaf0f002f5d8423738c89.pdf}
}

@inproceedings{beisswenger-EtAl:2016:clarin-long,
  abstract = {The paper presents best practices and results from projects dedicated to the creation of corpora of computer-mediated communication and social media interactions (CMC) from four different countries. Even though there are still many open issues related to building and annotating corpora of this type, there already exists a range of tested solutions which may serve as a starting point for a comprehensive discussion on how future standards for CMC corpora could (and should) be shaped like.},
  author = {Beißwenger, Michael and Chanier, Thierry and Erjavec, Tomaž and Fišer, Darja and Herold, Axel and Lubešić, Nikola and Lüngen, Harald and Poudat, Céline and Stemle, Egon and Storrer, Angelika and Wigham, Ciara},
  booktitle = {Selected {{Papers}} from the {{CLARIN Annual Conference}} 2016, {{Aix}}-en-{{Provence}}, 26–28 {{October}} 2016, {{CLARIN Common Language Resources}} and {{Technology Infrastructure}}},
  date = {2017-05},
  issn = {1650-3740},
  keywords = {CMC corpora,community building,computer-mediated communication,corpus annotation,language resources,social media corpora,TEI},
  pages = {1-18},
  publisher = {{Linköping University Electronic Press, Linköpings universitet}},
  title = {Closing a {{Gap}} in the {{Language Resources Landscape}}: {{Groundwork}} and {{Best Practices}} from {{Projects}} on {{Computer}}-mediated {{Communication}} in four {{European Countries}}},
  url = {http://www.ep.liu.se/ecp/article.asp?issue=136&article=001}
}

@inproceedings{beisswenger-EtAl:2017:cmc-corpora,
  abstract = {The paper reports on the results of a scientific colloquium dedicated to the creation of standards and best practices which are needed to facilitate the integration of language resources for CMC stemming from different origins and the linguistic analysis of CMC phenomena in different languages and genres. The key issue to be solved is that of interoperability – with respect to the structural representation of CMC genres, linguistic annotations metadata, and anonymization/pseudonymization schemas. The objective of the paper is to convince more projects to partake in a discussion about standards for CMC corpora and for the creation of a CMC corpus infrastructure across languages and genres. In view of the broad range of corpus projects which are currently underway all over Europe, there is a great window of opportunity for the creation of standards in a bottom-up approach.},
  author = {Beißwenger, Michael and Wigham, Ciara R. and Etienne, Carole and Fišer, Darja and Suárez, Holger Grumt and Herzberg, Laura and Hinrichs, Erhard and Horsmann, Tobias and Karlova-Bourbonus, Natali and Lemnitzer, Lothar and Longhi, Julien and Lüngen, Harald and Ho-Dac, Lydia-Mai and Parisse, Christophe and Poudat, Céline and Schmidt, Thomas and Stemle, Egon and Storrer, Angelika and Zesch, Torsten},
  booktitle = {Proceedings of the 5th {{Conference}} on {{CMC}} and {{Social Media Corpora}} for the {{Humanities}}},
  date = {2017-10},
  doi = {10.5281/zenodo.1041877},
  editor = {Stemle, Egon W. and Wigham, Ciara},
  keywords = {annotation,anonymization,corpora,research infrastructures},
  location = {{Bolzano, Italy}},
  pages = {52-55},
  title = {Connecting {{Resources}}: {{Which Issues}} have to be {{Solved}} to {{Integrate CMC Corpora}} from {{Heterogeneous Sources}} and for {{Different Languages}}?},
  url = {https://zenodo.org/record/1041877/files/cmccorpora17-28.pdf}
}

@inproceedings{BlohmCimianoStemle2007,
  author = {Blohm, Sebastian and Cimiano, Philipp and Stemle, Egon},
  booktitle = {Proceedings of the 22nd {{Conference}} on {{Artificial Intelligence}} ({{AAAI}}-07)},
  date = {2007-07},
  isbn = {978-1-57735-323-2},
  pages = {1316-1323},
  publisher = {{Association for the Advancement of Artificial Intelligence}},
  title = {Harvesting {{Relations}} from the {{Web}} - {{Quantifiying}} the {{Impact}} of {{Filtering Functions}}},
  url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-208.pdf}
}

@inproceedings{BoninEtAl:2012:AnnotatingArchaeologicalTexts,
  abstract = {Developing content extraction methods for Humanities domains raises a number of chal- lenges, from the abundance of non-standard entity types to their complexity to the scarcity of data. Close collaboration with Humani- ties scholars is essential to address these chal- lenges. We discuss an annotation schema for Archaeological texts developed in collabora- tion with domain experts. Its development re- quired a number of iterations to make sure all the most important entity types were included, as well as addressing challenges including a domain-specific handling of temporal expres- sions, and the existence of many systematic types of ambiguity.},
  author = {Bonin, Francesca and Cavulli, Fabio and Noriller, Aronne and Poesio, Massimo and Stemle, Egon W.},
  booktitle = {Proceedings of the {{Sixth Linguistic Annotation Workshop}} ({{LAW}} 2012)},
  date = {2012-07},
  location = {{Jeju, Republic of Korea}},
  note = {Series Title: LAW VI '12},
  pages = {134-138},
  publisher = {{Association for Computational Linguistics}},
  series = {{{LAW VI}} '12},
  title = {Annotating {{Archaeological Texts}}: {{An Example}} of {{Domain}}-{{Specific Annotation}} in the {{Humanities}}},
  url = {http://dl.acm.org/citation.cfm?id=2392747.2392768}
}

@book{cmc-corpora:2017,
  abstract = {This volume presents the proceedings of the 5th edition of the annual conference series on CMC and Social Media Corpora for the Humanities (cmc-corpora2017). This conference series is dedicated to the collection, annotation, processing, and exploitation of corpora of computer-mediated communication (CMC) and social media for research in the humanities. The annual event brings together language-centered research on CMC and social media in linguistics, philologies, communication sciences, media and social sciences with research questions from the fields of corpus and computational linguistics, language technology, text technology, and machine learning. The 5th Conference on CMC and Social Media Corpora for the Humanities was held at Eurac Research on October, 4th and 5th, in Bolzano, Italy. This volume contains extended abstracts of the invited talks, papers, and extended abstracts of posters presented at the event. The conference attracted 26 valid submissions. Each submission was reviewed by at least two members of the scientific committee. This committee decided to accept 16 papers and 8 posters of which 14 papers and 3 posters were presented at the conference. The programme also includes three invited talks: two keynote talks by Aivars Glaznieks (Eurac Research, Italy) and A. Seza Doğruöz (Independent researcher) and an invited talk on the Common Language Resources and Technology Infrastructure (CLARIN) given by Darja Fišer, the CLARIN ERIC Director of User Involvement.},
  date = {2017-10},
  doi = {10.5281/zenodo.1040875},
  editor = {Stemle, Egon W. and Wigham, Ciara R.},
  langid = {english},
  location = {{Bolzano, Italy}},
  title = {Proceedings of the 5th {{Conference}} on {{CMC}} and {{Social Media Corpora}} for the {{Humanities}}},
  url = {https://zenodo.org/record/1040875/files/cmccorpora17-proceedings-v2.pdf}
}

@article{egartervigl-stemle:2018:WasDarfForschung,
  abstract = {Interview in Academia (science magazine by EURAC and unibz), Bolzano, Italy},
  author = {Baumgartner, Barbara and Angler, Martin},
  date = {2018-05},
  editora = {Egarter Vigl, Lukas and Stemle, Egon},
  editoratype = {collaborator},
  entrysubtype = {newspaper},
  journaltitle = {Academia-Interview Titelthema},
  langid = {german},
  pages = {10-11},
  title = {Was darf Forschung mit Social Media Daten?},
  url = {http://www.academia.bz.it/articles/was-darf-forschung-mit-social-media-daten}
}

@article{EkbalEtAl:2011,
  author = {Ekbal, Asif and Bonin, Francesca and Saha, Sriparna and Stemle, Egon and Barbu, Eduard and Cavulli, Fabio and Girardi, Christian and Poesio, Massimo},
  date = {2011-11},
  journaltitle = {Journal for Language Technology and Computational Linguistics (JLCL)},
  number = {2},
  pages = {39-51},
  title = {Rapid {{Adaptation}} of {{NE Resolvers}} for {{Humanities Domains}} using {{Active Annotation}}},
  url = {http://www.jlcl.org/2011_Heft2/9.pdf},
  volume = {26}
}

@inproceedings{FIASCO2007,
  author = {Bauer, Daniel and Degen, Judith and Deng, Xiaoye and Herger, Priska and Gasthaus, Jan and Giesbrecht, Eugenie and Jansen, Lina and Kalina, Christin and Krüger, Thorben and Märtin, Robert and Schmidt, Martin and Scholler, Simon and Steger, Johannes and Stemle, Egon and Evert, Stefan},
  booktitle = {Proceedings of the {{Third Web}} as {{Corpus Workshop}} ({{WAC3}})},
  date = {2007-09},
  editor = {Fairon, Cédrick and Naets, Hubert and Kilgarriff, Adam and family=Schryver, given=Gilles-Maurice, prefix=de, useprefix=true},
  location = {{Louvain-la-Neuve}},
  publisher = {{Presses universitaires de Louvain}},
  title = {{{FIASCO}}: {{Filtering}} the {{Internet}} by {{Automatic Subtree Classification}}, {{Osnabrück}}},
  url = {http://purl.org/stefan.evert/PUB/BauerEtc2007_FIASCO.pdf}
}

@incollection{frey-EtAl:2019:cmc2017volume,
  abstract = {Multilingual speakers communicate in more than one language in daily life and on social media. In order to process or investigate multilingual communication, there is a need for language identification. This study compares the performance of human annotators with automatic ways of language identification on a multilingual (mainly German-Italian-English) social media data set collected in Italy (i.e. South Tyrol). Our results indicate that humans and NLP systems follow their individual techniques to make a decision about multilingual text messages. This results in low agreement when different annotators or NLP systems execute the same task. In general, annotators agree with each other more than NLP systems. However, there is also variation in human agreement depending on the prior establishment of guidelines for the annotation task or not.},
  author = {Frey, Jennifer-Carmen and Stemle, Egon W. and Doğruöz, A. Seza},
  booktitle = {Building {{Computer}}-{{Mediated Communication Corpora}} for sociolinguistic {{Analysis}}},
  date = {2019-06-13},
  editor = {Wigham, Ciara R. and Stemle, Egon W.},
  isbn = {978-2-84516-860-2},
  langid = {english},
  location = {{Clermont-Ferrand, FR}},
  publisher = {{Presses Universitaires Blaise Pascal}},
  series = {Cahiers du laboratoire de recherche sur le langage},
  title = {Comparison of {{Automatic}} vs. {{Manual Language Identification}} in {{Multilingual Social Media Texts}}},
  url = {http://pubp.giantchair.com/livre/?GCOI=28451100141150&language=en},
  urltype = {publisher}
}

@inproceedings{frey-glaznieks-stemle:2016:didi,
  abstract = {The DiDi corpus of South Tyrolean data of computer-mediated communication (CMC) is a multilingual sociolinguistic language corpus. It consists of around 600,000 tokens collected from 136 profiles of Facebook users residing in South Tyrol, Italy. In conformity with the multilingual situation of the territory, the main languages of the corpus are German and Italian (followed by English). The data has been manually anonymised and provides manually corrected part-of-speech tags for the Italian language texts and manually normalised data for German texts. Moreover, it is annotated with user-provided socio-demographic data (among others L1, gender, age, education, and internet communication habits) from a questionnaire, and linguistic annotations regarding CMC phenomena, languages and varieties. The anonymised corpus is freely available for research purposes.},
  author = {Frey, Jennifer-Carmen and Glaznieks, Aivars and Stemle, Egon W.},
  booktitle = {Proceedings of {{Third Italian Conference}} on {{Computational Linguistics}} ({{CLiC}}-it 2016) \& {{Fifth Evaluation Campaign}} of {{Natural Language Processing}} and {{Speech Tools}} for {{Italian}}. {{Final Workshop}} ({{EVALITA}} 2016)},
  date = {2016-12},
  editor = {Basile, Pierpaolo and Corazza, Anna and Cutugno, Franco and Montemagni, Simonetta and Nissim, Malvina and Patti, Viviana and Semeraro, Giovanni and Sprugnoli, Rachele},
  issn = {1613-0073},
  location = {{Napoli, Italy}},
  title = {The {{DiDi Corpus}} of {{South Tyrolean CMC Data}}: {{A}} multilingual corpus of {{Facebook}} texts},
  url = {http://ceur-ws.org/Vol-1749/paper27.pdf}
}

@article{FreyGlaznieksStemle2015,
  abstract = {This paper presents the DiDi Corpus, a corpus of South Tyrolean Data of Computer-mediated Communication (CMC). The corpus comprises around 650,000 tokens from Facebook wall posts, comments on wall posts and private messages, as well as socio-demographic data of participants. All data was automatically annotated with language information (de, it, en and others), and manually normalised and anonymised. Furthermore, semi-automatic token level annotations include part-of-speech and CMC phenomena (e.g. emoticons, emojis, and iteration of graphemes and punctuation). The anonymised corpus without the private messages is freely available for researchers; the complete and anonymised corpus is available after signing a non- disclosure agreement.},
  author = {Frey, Jennifer-Carmen and Glaznieks, Aivars and Stemle, Egon W.},
  date = {2015-09-29},
  journaltitle = {Proceedings of the 2nd Workshop on Natural Language Processing for Computer-Mediated Communication / Social Media at GSCL2015 (NLP4CMC2015)},
  location = {{Essen, Germany}},
  pages = {1-6},
  publisher = {{German Society for Computational Linguistics \& Language Technology}},
  title = {The {{DiDi Corpus}} of {{South Tyrolean CMC Data}}},
  url = {https://sites.google.com/site/nlp4cmc2015/NLP4CMC-2015.pdf}
}

@inproceedings{FreyStemleGlaznieks2014,
  abstract = {In this paper, we propose an integrated web strategy for mixed sociolinguistic research methodologies in the context of social media corpora. After stating the particular challenges for building corpora of private, non-public computer-mediated communication, we will present our solution to these problems: a Facebook web application for the acquisition of such data and the corresponding meta data. Finally, we will discuss positive and negative implications for this method.},
  author = {Frey, Jennifer-Carmen and Stemle, Egon W. and Glaznieks, Aivars},
  booktitle = {Workshop {{Proceedings}} of the 12th {{Edition}} of the {{KONVENS Conference}}},
  date = {2014-10},
  editor = {Faaß, Gertrud and Ruppenhofer, Josef},
  location = {{Hildesheim, Germany}},
  pages = {11-15},
  publisher = {{Universitatsverlag Hildesheim, Germany}},
  title = {Collecting language data of non-public social media profiles},
  url = {http://www.uni-hildesheim.de/konvens2014/data/konvens2014-workshop-proceedings.pdf},
  urldate = {2019-06-20}
}

@inproceedings{GenereuxStemleNicolasLyding2014,
  abstract = {In this paper, we present ongoing experiments for correcting OCR errors on German newspapers in Fraktur font. Our approach borrows from techniques for spelling correction in context using a probabilistic edit-operation error model and lexical resources. We highlight conditions in which high error reduction rates can be obtained and where the approach currently stands with real data.},
  author = {Généreux, Michel and Stemle, Egon W. and Nicolas, Lionel and Lyding, Verena},
  booktitle = {Proceedings of the {{First Italian Conference}} on {{Computational Linguistics}} ({{CLiC}}-it 2014)},
  date = {2014-12},
  editor = {Basili, Roberto and Lenci, Alessandro and Magnini, Bernardo},
  location = {{Pisa, Italy}},
  title = {Correcting {{OCR}} errors for {{German}} in {{Fraktur}} font},
  url = {http://clic.humnet.unipi.it/proceedings/vol1/CLICIT2014136.pdf}
}

@article{glaznieks-EtAl:2014:EstablishingStandardisedProcedure,
  abstract = {Decisions at the outset of preparing a learner corpus are of crucial importance for how the corpus can be built and how it can be analysed later on. This paper presents a generic workflow to build learner corpora while taking into account the needs of the users. The workflow results from an extensive collaboration between linguists that annotate and use the corpus and computer linguists that are responsible for providing technical support. The paper addresses the linguists' research needs as well as the availability and usability of language technology tools necessary to meet them. We demonstrate and illustrate the relevance of the workflow using results and examples from our L1 learner corpus of German ("KoKo").},
  author = {Glaznieks, Aivars and Abel, Andrea and Lyding, Verena and Nicolas, Lionel and Stemle, Egon},
  date = {2014-12},
  editor = {Nikula, Tarja and Takala, Sauli and Ylönen, Sabine},
  issn = {1457-9863},
  journaltitle = {Apples - Journal of Applied Language Studies},
  keywords = {corpus building workflow,German as a first language,L1 learner corpus},
  number = {3},
  pages = {5-20},
  title = {Establishing a {{Standardised Procedure}} for {{Building Learner Corpora}}},
  url = {http://apples.jyu.fi/ArticleFile/download/535},
  volume = {8}
}

@article{GlaznieksStemle2014,
  abstract = {Special Issue: Building and annotating corpora of computer-mediated discourse. Issues and Challenges at the Inteface of Corpus and Computational Linguistics},
  author = {Glaznieks, Aivars and Stemle, Egon},
  date = {2014-12},
  editor = {Beißwenger, Michael and Oostdijk, Nellek and Storrer, Angelika and family=Heuvel, given=Henk, prefix=van den, useprefix=false},
  issn = {2190-6858},
  journaltitle = {Journal for Language Technology and Computational Linguistics (JLCL)},
  number = {2},
  pages = {31-57},
  title = {Challenges of building a {{CMC}} corpus for analyzing writer's style by age: {{The DiDi}} project},
  url = {http://www.jlcl.org/2014_Heft2/2GlaznieksStemle.pdf},
  volume = {29}
}

@inproceedings{KranebitterStemle2013,
  abstract = {Graphical tools to organise and represent knowledge are useful in terminology work to facilitate building concept systems. Creating and maintaining hierarchically structured concept relation maps while manually gathering data for terminological databases helps to gain and maintain an overview of concept relations, supports terminology work in groups, and helps new team members catching up on the subject field. This article describes our approach to support the building of concept systems in comparative legal terminology using the concept mapping software CmapTools (IHMC): we build hierarchically structured concept relation maps where linking lines with arrowheads between concepts of the same legal system represent generic-specific relations, and combined concept relation maps where dashed lines without arrowheads connect similar concepts in different legal systems.},
  author = {Kranebitter, Klara and Stemle, Egon W.},
  booktitle = {Terminologie \& {{Ontologie}}: {{Théories}} et {{Applications}}. {{Actes}} de la septième conférence {{TOTh}} 2013},
  date = {2013-06},
  editor = {Roche, Christophe and Costa, Rute and Depecker, Loïc and Thoiron, Philippe},
  location = {{Chamébry, France}},
  pages = {97-116},
  publisher = {{Institut Porphyre, Savoir et Connaissance}},
  title = {Constructing concept relation maps to support building concept systems in comparative legal terminology}
}

@inproceedings{LYDING14.517,
  abstract = {In this article, we present interHist, a compact visualization for the interactive exploration of results to complex corpus queries. Integrated with a search interface to the PAISÀ corpus of Italian web texts, interHist aims at facilitating the exploration of large results sets to linguistic corpus searches. This objective is approached by providing an interactive visual overview of the data, which supports the user-steered navigation by means of interactive filtering. It allows to dynamically switch between an overview on the data and a detailed view on results in their immediate textual context, thus helping to detect and inspect relevant hits more efficiently. We provide background information on corpus linguistics and related work on visualizations for language and linguistic data. We introduce the architecture of interHist, by detailing the data structure it relies on, describing the visualization design and providing technical details of the implementation and its integration with the corpus querying environment. Finally, we illustrate its usage by presenting a use case for the analysis of the composition of Italian noun phrases.},
  author = {Lyding, Verena and Nicolas, Lionel and Stemle, Egon},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'14)},
  date = {2014-05},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Stelios, Piperidis},
  isbn = {978-2-9517408-8-4},
  keywords = {corpus linguistics,language analysis,visualization},
  location = {{Reykjavik, Iceland}},
  pages = {635-641},
  publisher = {{European Language Resources Association (ELRA)}},
  title = {'{{interHist}}' - an interactive visual interface for corpus exploration},
  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/517_Paper.pdf}
}

@inproceedings{Lyding2013a,
  abstract = {In this article, we present the multi-faceted interface to the open PAISÀ corpus of Italian. Created within the project PAISÀ (Piattaforma per l’Apprendimento dell’Italiano Su corpora Annotati) [1], the corpus is designed to be freely available for non-commercial processing, usage and distribution by the public. Hence, this automatically annotated corpus (for lemma, part-of-speech and dependency information) is exclusively composed of documents licensed under Creative Commons (CC) licenses [2].The dedicated corpus interface is designed to provide flexible, powerful, and easy-to-use modes of corpus access, with the objective to support language learning, language practicing and linguistic analyses. We present in detail the interface’s functionalities and discuss the underlying design decisions. We introduce the four principal components of the interface, describe supported display formats and present two specific features added to increase the interface's relevance for language learning. The main search components are (1) a basic search that adopts a "Google-style" search box, (2) an advanced search that provides elaborated graphical search options, and (3) a search that makes use of the powerful CQP query language of the Open Corpus Workbench [3]. In addition, (4) a filter interface for retrieving full-text corpus documents based on keyword searches is available. It is likewise providing the means for building temporary sub-corpora for specific topics. Users can choose among different display formats for the search results. Besides the established KWIC (KeyWord In Context) and full sentence views, graphical representations of the dependency relation information as well as keyword distributions are available. These dynamic displays are based on a visualisation for dependency graphs [4] and one for Word Clouds [5], which build on latest developments in information visualisation for language data. Two special features for novice learners are integrated into each search component. The first feature is a function for restricting search results to sentences of limited complexity. Search results are automatically filtered based on formal text characteristics such as sentence length, vocabulary, etc. The second is the supply of pre-defined search queries for linguistic constructions such as sentences in passive voice, questions, etc. Finally, we show how the PAISÀ interface can be employed in different language teaching tasks. In particular, we present a complete unit of work aimed at learners of Italian (CEFR level A2/B1) and centered on students’ direct use of the interface and its functionalities. By doing so, we are giving concrete examples for targeted searches and interactions with the provided language material, as well as an exemplification of how the use of the corpus can be integrated with communicative language activities in the classroom.},
  author = {Lyding, Verena and Borghetti, Claudia and Dittmann, Henrik and Nicolas, Lionel and Stemle, Egon},
  booktitle = {Proceedings of the {{International Conference ICT}} for {{Language Learning}}, 6th edition},
  date = {2013-11-14},
  isbn = {978-88-6292-423-8},
  keywords = {Corpus Linguistics,Linguistic Visualization,Visualization},
  location = {{Florence, Italy}},
  publisher = {{libreriauniversitaria.it}},
  title = {Open {{Corpus Interface}} for {{Italian Language Learning}}},
  url = {http://conference.pixel-online.net/ICT4LL2013/common/download/Paper_pdf/270-ITL56-FP-Lyding-ICT2013.pdf}
}

@inproceedings{murphy-stemle:2011:DIALECTS,
  abstract = {Small, manually assembled corpora may be available for less dominant languages and dialects, but producing web-scale resources remains a challenge. Even when considerable quantities of text are present on the web, finding this text, and distinguishing it from related languages in the same region can be difficult. For example less dominant variants of English (e.g. New Zealander, Singaporean, Canadian, Irish, South African) may be found under their respective national domains, but will be partially mixed with Englishes of the British and US varieties, perhaps through syndication of journalism, or the local reuse of text by multinational companies. Less formal dialectal usage may be scattered more widely over the internet through mechanisms such as wiki or blog authoring. Here we automatically construct a corpus of Hiberno-English (English as spoken in Ireland) using a variety of methods: filtering by national domain, filtering by orthographic conventions, and bootstrapping from a set of Ireland-specific terms (slang, place names, organisations). We evaluate the national specificity of the resulting corpora by measuring the incidence of topical terms, and several grammatical constructions that are particular to Hiberno-English. The results show that domain filtering is very effective for isolating text that is topic-specific, and orthographic classification can exclude some non-Irish texts, but that selected seeds are necessary to extract considerable quantities of more informal, dialectal text.},
  author = {Murphy, Brian and Stemle, Egon W.},
  booktitle = {Proceedings of the {{First Workshop}} on {{Algorithms}} and {{Resources}} for {{Modelling}} of {{Dialects}} and {{Language Varieties}}},
  date = {2011-07},
  location = {{Edinburgh, Scotland, UK}},
  pages = {22-29},
  publisher = {{Association for Computational Linguistics}},
  title = {{{PaddyWaC}}: {{A Minimally}}-{{Supervised Web}}-{{Corpus}} of {{Hiberno}}-{{English}}},
  url = {http://www.aclweb.org/anthology/W11-2603}
}

@incollection{nicolas-EtAl:2014-li,
  abstract = {We present an abstract and generic workflow, and detail how it has been implemented to build and annotate learner corpora. This workflow has been developed through an interdisciplinary collaboration between linguists, who annotate and use corpora, and computational linguists and computer scientists, who are responsible for providing technical support and adaptation or implementation of software components.},
  author = {Nicolas, Lionel and Stemle, Egon and Glaznieks, Aivars and Abel, Andrea},
  booktitle = {Studies in {{Learner Corpus Linguistics}}: {{Research}} and {{Applications}} for {{Foreign Language Teaching}} and {{Assessment}}},
  date = {2015-06},
  doi = {10.3726/978-3-0351-0736-4},
  editor = {Castello, Erik and Ackerley, Katherine and Coccetta, Francesca},
  isbn = {978-3-0351-0736-4},
  location = {{Bern, Switzerland}},
  pages = {337-351},
  publisher = {{Peter Lang}},
  series = {Linguistic {{Insights}}},
  title = {A {{Generic Data Workflow}} for {{Building Annotated Text Corpora}}},
  volume = {190}
}

@inproceedings{Nicolas2013a,
  abstract = {In this paper, we report on an unsupervised greedy-style process for acquiring phrase translations from sentence-aligned parallel corpora. Thanks to innovative selection strategies, this process can acquire multiple translations without size criteria, i.e. phrases can have several translations, can be of any size, and their size is not considered when selecting their translations. Even though the process is in an early development stage and has much room for improvements, evaluation shows that it yields phrase translations of high precision that are relevant to machine translation but also to a wider set of applications including memory-based translation or multi-word acquisition.},
  author = {Nicolas, Lionel and Stemle, Egon W. and Kranebitter, Klara and Lyding, Verena},
  booktitle = {Proceedings of {{Recent Advances}} in {{Natural Language Processing}}, {{RANLP}} 2013},
  date = {2013-09},
  editor = {Angelova, Galia and Bontcheva, Kalina and Mitkov, Ruslan},
  keywords = {Bilingual lexicon,Parallel Corpora,Phrase Translation,Unsupervised Learning},
  location = {{Hissar, Bulgaria}},
  pages = {516-524},
  publisher = {{RANLP 2011 Organising Committee / ACL}},
  title = {High-{{Accuracy Phrase Translation Acquisition Through Battle}}-{{Royale Selection}}},
  url = {http://aclweb.org/anthology/R/R13/R13-1068.pdf},
  urldate = {2013-12-13}
}

@inproceedings{NicolasStemleKranebitter2012,
  abstract = {We report on on-going work to derive translations of phrases from parallel corpora. We describe an unsupervised and knowledge-free greedy-style process relying on innovative strategies for choosing and discarding candidate translations. This process manages to acquire multiple translations combining phrases of equal or different sizes. The preliminary evaluation performed confirms both its potential and its interest.},
  author = {Nicolas, Lionel and Stemle, Egon W. and Kranebitter, Klara},
  booktitle = {11th {{Conference}} on {{Natural Language Processing}}, {{KONVENS}} 2012, {{Empirical Methods}} in {{Natural Language Processing}}},
  date = {2012-09},
  editor = {Jancsary, Jeremy},
  keywords = {Bilingual lexicon,Parallel Corpora,Phrase Translation,Unsupervised Learning},
  location = {{Vienna, Austria}},
  pages = {471-479},
  publisher = {{ÖGAI}},
  title = {Towards high-accuracy bilingual phrase acquisition from parallel corpora},
  url = {http://www.oegai.at/konvens2012/proceedings/68_nicolas12w/}
}

@inproceedings{paisa2014,
  abstract = {PAISÀ is a Creative Commons licensed, large web corpus of contemporary Italian. We describe the design, harvesting, and processing steps involved in its creation.},
  author = {Lyding, Verena and Stemle, Egon and Borghetti, Claudia and Brunello, Marco and Castagnoli, Sara and Orletta, Felice Dell and Dittmann, Henrik and Lenci, Alessandro and Pirrelli, Vito},
  booktitle = {Proceedings of the 9th {{Web}} as {{Corpus Workshop}} ({{WaC}}-9)},
  date = {2014-04-26},
  location = {{Gothenburg, Sweden}},
  pages = {36-43},
  publisher = {{Association for Computational Linguistics}},
  title = {The {{PAISÀ Corpus}} of {{Italian Web Texts}}},
  url = {http://aclweb.org/anthology/W14-0406}
}

@inproceedings{poesio-EtAl:2011:LaTeCH-2011,
  abstract = {Most existing HLT pipelines assume the input is pure text or, at most, HTML and either ignore (logical) document structure or remove it. We argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved.},
  author = {Poesio, Massimo and Barbu, Eduard and Stemle, Egon W. and Girardi, Christian},
  booktitle = {Proceedings of the 5th {{ACL}}-{{HLT Workshop}} on {{Language Technology}} for {{Cultural Heritage}}, {{Social Sciences}}, and {{Humanities}} ({{LaTeCH}} 2011)},
  date = {2011-06},
  location = {{Portland, OR, USA}},
  pages = {54-62},
  publisher = {{Association for Computational Linguistics}},
  title = {Structure-{{Preserving Pipelines}} for {{Digital Libraries}}},
  url = {http://www.aclweb.org/anthology/W11-1508}
}

@inproceedings{PoesioSDH2011,
  author = {Poesio, Massimo and Barbu, Eduard and Bonin, Francesca and Cavulli, Fabio and Ekbal, Asif and Stemle, Egon and Girardi, Christian},
  booktitle = {Proceedings of {{Supporting Digital Humanities}} ({{SDH2011}}): {{Answering}} the unaskable},
  date = {2011-11},
  editor = {Maegaard, Bente},
  location = {{Copenhagen, Denmark}},
  title = {The {{Humanities Research Portal}}: {{Human Language Technology Meets Humanities Publication Archives}}}
}

@inproceedings{RodriguezDeloguVersleyStemlePoesio2010,
  author = {Rodríguez, Kepa Joseba and Delogu, Francesca and Versley, Jannick and Stemle, Egon W. and Poesio, Massimo},
  booktitle = {Proceedings of the {{Seventh Conference}} on {{International Language Resources}} and {{Evaluation}} ({{LREC}}'10)},
  date = {2010-05},
  editor = {Calzolari, Nicoletta and Choukri, Khalid and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Piperidis, Stelios and Rosner, Mike and Tapias, Daniel},
  isbn = {2-9517408-6-7},
  location = {{Valletta, Malta}},
  publisher = {{European Language Resources Association (ELRA)}},
  title = {Anaphoric {{Annotation}} of {{Wikipedia}} and {{Blogs}} in the {{Live Memories Corpus}}},
  url = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/431_Paper.pdf}
}

@inproceedings{StegerStemle2009,
  abstract = {Algorithmic processing of Web content mostly works on textual contents, neglecting visual information. Annotation tools largely share this deficit as well. We specify requirements for an architecture to overcome both problems and propose an implementation, the KrdWrd system. It uses the Gecko rendering engine for both annotation and feature extraction, providing unified data access in every processing step. Stable data storage and collaboration control scripts for group annotations of massive corpora are provided via a Web interface coupled with a HTTP proxy. A modular interface allows for linguistic and visual data feature extractor plugins. The implementation is suitable for many tasks in theWeb as corpus domain and beyond.},
  author = {Steger, Johannes and Stemle, Egon},
  booktitle = {Proceedings of the {{Fifth Web}} as {{Corpus Workshop}} ({{WAC5}})},
  date = {2009-09},
  editor = {Alegria, Iñaki and Leturia, Igor and Sharoff, Serge},
  location = {{Donostia-San Sebastian, Basque Country, Spain}},
  pages = {63-70},
  publisher = {{Elhuyar Fundazioa}},
  title = {{{KrdWrd}}: {{Architecture}} for {{Unified Processing}} of {{Web Content}}},
  url = {https://www.sigwac.org.uk/raw-attachment/wiki/WAC5/WAC5_proceedings.pdf}
}

@article{stemle-EtAl:2019:lcr-postconf,
  abstract = {In this article we give an overview of first-hand experiences and starting points for best practices from projects in seven European countries dedicated to learner corpus research and the creation of language learner corpora. The corpora and tools involved in LCR are becoming more and more important, and the careful preparation and easy retrieval, and reusability of corpora and tools has likewise become more important. But with a lack of agreed solutions for many aspects of LCR, interoperability between learner corpora or exchanging data from different learner corpus projects is still challenging. We will illustrate how concepts like metadata, anonymization, error taxonomies and linguistic annotations, as well as tools, toolchains or data formats can individually pose challenges and how they might be solved.},
  author = {Stemle, Egon W. and Boyd, Adriane and Janssen, Maarten and Lindström Tiedemann, Therese and Mikelić Preradović, Nives and Rosen, Alexandr and Rosén, Dan and Volodina, Elena},
  date = {2019},
  editor = {Abel, Andrea and Glaznieks, Aivars and Lyding, Verena and Nicolas, Lionel},
  issn = {20346417},
  journaltitle = {Widening the Scope of Learner Corpus Research. Selected Papers from the 4th Learner Corpus Research Conference 2017},
  pubstate = {"In press"},
  series = {Corpora and {{Language}} in {{Use}}},
  title = {Working together towards an ideal infrastructure for language learner corpora}
}

@article{stemle-onysko:2014,
  abstract = {This article focuses on automatic text classification which aims at identifying the first language (L1) background of learners of English. A particular question arising in the context of automated L1 identification is whether any features that are informative for a machine learning algorithm relate to L1-specific transfer phenomena. In order to explore this issue further, we discuss the results of a study carried out in the wake of a Native Language Identification Task. The task is based on the TOEFL11 corpus (cf. Blanchard et al. 2013), which involves a sample of 12,100 essays written by participants in the TOEFL® test from 11 different language backgrounds (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, and Turkish). The article will show our results in automatic L1 detection in the TOEFL11 corpus. These results are discussed in light of relevant transfer features which turned out to be particularly informative for automatic detection of L1 German and L1 Italian.},
  author = {Stemle, Egon and Onysko, Alexander},
  date = {2015-04},
  doi = {10.1075/hsld.4.13ste},
  editor = {Peukert, Hagen},
  journaltitle = {Transfer Effects in Multilingual Language Development},
  pages = {297-321},
  series = {Hamburg {{Studies}} on {{Linguistic Diversity}}},
  title = {Automated {{L1}} identification in {{English}} learner essays and its implications for language transfer},
  url = {https://benjamins.com/catalog/hsld.4.13ste},
  volume = {4}
}

@inproceedings{stemle-onysko:2018:naacl-flpst,
  abstract = {This article describes the system that participated in the shared task (ST) on metaphor detection on the Vrije University Amsterdam Metaphor Corpus (VUA). The ST was part of the workshop on processing figurative language at the 16th annual conference of the North American Chapter of the Association for Computational Linguistics (NAACL2018). The system combines a small assertion of trending techniques, which implement matured methods from NLP and ML; in particular, the system uses word embeddings from standard corpora and from corpora representing different proficiency levels of language learners in a LSTM BiRNN architecture. The system is available under the APLv2 open-source license.},
  author = {Stemle, Egon and Onysko, Alexander},
  booktitle = {Proceedings of the {{Workshop}} on {{Figurative Language Processing}}},
  date = {2018-06},
  doi = {10.18653/v1/W18-0918},
  eventtitle = {Workshop on {{Figurative Language Processing}}},
  location = {{New Orleans, LA}},
  pages = {133-138},
  publisher = {{Association for Computational Linguistics}},
  title = {Using {{Language Learner Data}} for {{Metaphor Detection}}},
  url = {http://aclweb.org/anthology/W18-0918}
}

@report{stemle:2016:enel-stsm,
  abstract = {ENeL's WG3 concerns innovative e-dictionaries with a focus on the development of digitally born dictionaries. The training school 2016 in Ljubljana (SI), May 17-20, introduced parti­cipants, among others, to collecting, analysing, and automatically extracting data from web corpora. Albeit related, the task of processing data from corpora of computer-mediated communica­tion and social media interactions (henceforth referred to as CMC) has been deliberately ex­cluded from the training school's programme. But we know that "new vocabulary is charac­teristic for CMC discourse, e.g. ‘funzen’ (an abbreviated variant of the German verb ‘funk­tionieren’, en.: ‘to function’) or ‘gruscheln’ (verb denoting a function of a German social net­work platform, most likely a blending of ‘grüßen’, en.: ‘to greet’ and ‘kuscheln’, en.: ‘to cuddle’)" and therefore relevant to WG3; the goal of this STSM is to apply the meth­ods and tools from the training school to CMC data.},
  author = {Stemle, Egon W.},
  date = {2016-09},
  institution = {{EURAC (Bolzano, IT) and Centre for Language Resources and Technologies (Ljubljana, SI)}},
  location = {{Ljubljana, SI}},
  title = {Scientific {{Report}} of {{Short Term Scientific Mission COST}}-{{STSM}}-{{IS1305}}-34353},
  url = {http://www.elexicography.eu/wp-content/uploads/2017/02/ScientificReportSTSM-IS1305-34353-EgonStemle.pdf}
}

@inproceedings{stemle:2016:evalita,
  abstract = {This article describes the system that participated in the POS tagging for Italian Social Media Texts (PoSTWITA) task of the 5th periodic evaluation campaign of Natural Language Processing (NLP) and speech tools for the Italian language EVALITA 2016. The work is a continuation of Stemle (2016) with minor modifications to the system and different data sets. It combines a small assertion of trending techniques, which implement matured methods, from NLP and ML to achieve competitive results on PoS tagging of Italian Twitter texts; in particular, the system uses word embeddings and character-level representations of word beginnings and endings in a LSTM RNN architecture. Labelled data (Italian UD corpus, DiDi and PoSTWITA) and unlabbelled data (Italian C4Corpus and PAISA') were used for training. The system is available under the APLv2 open-source license.},
  author = {Stemle, Egon W.},
  booktitle = {Proceedings of {{Third Italian Conference}} on {{Computational Linguistics}} ({{CLiC}}-it 2016) \& {{Fifth Evaluation Campaign}} of {{Natural Language Processing}} and {{Speech Tools}} for {{Italian}}. {{Final Workshop}} ({{EVALITA}} 2016)},
  date = {2016-12},
  editor = {Basile, Pierpaolo and Corazza, Anna and Cutugno, Franco and Montemagni, Simonetta and Nissim, Malvina and Patti, Viviana and Semeraro, Giovanni and Sprugnoli, Rachele},
  eventtitle = {{{EVALITA}}},
  location = {{Napoli, Italy}},
  publisher = {{Accademia University Press}},
  title = {bot.zen @ {{EVALITA}} 2016 - {{A}} minimally-deep learning {{PoS}}-tagger (trained for {{Italian Tweets}})},
  url = {http://books.openedition.org/aaccademia/pdf/1968}
}

@inproceedings{stemle:2016:WAC-X,
  abstract = {This article describes the system that participated in the Part-of-speech tagging subtask of the "EmpiriST 2015 shared task on automatic linguistic annotation of computer-mediated communication / social media". The system combines a small assertion of trending techniques, which implement matured methods, from NLP and ML to achieve competitive results on PoS tagging of German CMC and Web corpus data; in particular, the system uses word embeddings and character-level representations of word beginnings and endings in a LSTM RNN architecture. Labelled data (Tiger v2.2 and EmpiriST) and unlabelled data (German Wikipedia) were used for training. The system is available under the APLv2 open-source license.},
  author = {Stemle, Egon W.},
  booktitle = {Proceedings of the 10th {{Web}} as {{Corpus Workshop}} ({{WAC}}-{{X}}) and the {{EmpiriST Shared Task}}},
  date = {2016-08},
  editor = {Cook, Paul and Evert, Stefan and Schäfer, Roland and Stemle, Egon},
  eventtitle = {{{WAC}}-{{X}}},
  pages = {115-119},
  publisher = {{Association for Computational Linguistics}},
  title = {bot.zen @ {{EmpiriST}} 2015 - {{A}} minimally-deep learning {{PoS}}-tagger (trained for {{German CMC}} and {{Web}} data)},
  url = {http://anthology.aclweb.org/W/W16/W16-2614.pdf}
}

@report{Stemle2009,
  abstract = {This thesis discusses the KrdWrd Project. The Project goals are to provide tools and infrastructure for acquisition, visual annotation, merging and storage of Web pages as parts of bigger corpora, and to develop a classification engine that learns to automatically annotate pages, operate on the visual rendering of pages, and provide visual tools for inspection of results.},
  addendum = {Unpublished},
  author = {Stemle, Egon W.},
  date = {2009-04},
  institution = {{University of Osnabrück}},
  title = {Hybrid {{Sweeping}}: {{Streamlined Perceptual Structured}}-{{Text Refinement}}},
  type = {Master's thesis}
}

@article{StemleOnysko:2013:LanguageDetectiveStory,
  abstract = {Article in Academia (science magazine by EURAC and unibz), Bolzano, Italy},
  author = {Stemle, Egon W. and Onysko, Alexander},
  date = {2013-12},
  entrysubtype = {newspaper},
  journaltitle = {Academia},
  pages = {24-25},
  title = {Language as a {{Detective Story}}},
  volume = {64}
}

@book{WAC-X:2016,
  abstract = {The World Wide Web has become increasingly popular as a source of linguistic data, not only within the NLP communities, but also with theoretical linguists facing problems of data sparseness or data diversity. Accordingly, web corpora continue to gain importance, given their size and diversity in terms of genres/text types. The field is still new, though, and a number of issues in web corpus construction need much additional research, both fundamental and applied. These issues range from questions of corpus design (e.g., assessment of corpus composition, sampling strategies and their relation to crawling algorithms, and handling of duplicated material) to more technical aspects (e.g., efficient implementation of individual post-processing steps in document cleaning and linguistic annotation, or large-scale parallelization to achieve web-scale corpus construction). Similarly, the systematic evaluation of web corpora, for example in the form of task based comparisons to traditional corpora, has only recently shifted into focus. For almost a decade, the ACL SIGWAC (http://www.sigwac.org.uk/), and especially the highly successful Web as Corpus (WAC) workshops have served as a platform for researchers interested in compilation, processing and application of web-derived corpora. Past workshops were co-located with major conferences on computational linguistics and/or corpus linguistics (such as EACL, NAACL, LREC, WWW, and Corpus Linguistics). WAC-X also featured the final workshop of the EmpiriST 2015 shared task "Automatic Linguistic Annotation of Computer-Mediated Communication / Social Media" (see https://sites.google.com/site/empirist2015/ for details) and the panel discussion "Corpora, open science, and copyright reforms" (see https://www.sigwac.org.uk/wiki/WAC-X\#paneldisc for details).},
  date = {2016-08},
  editor = {Cook, Paul and Evert, Stefan and Schäfer, Roland and Stemle, Egon},
  isbn = {978-1-945626-15-9},
  location = {{Berlin}},
  publisher = {{Association for Computational Linguistics}},
  title = {Proceedings of the 10th {{Web}} as {{Corpus Workshop}} ({{WAC}}-{{X}}) and the {{EmpiriST Shared Task}}},
  url = {http://aclweb.org/anthology/W16-26}
}

@book{WAC8,
  abstract = {Web corpora and other Web-derived data have become a gold mine for corpus linguistics and natural language processing. The Web is an easy source of unprecedented amounts of linguistic data from a broad range of registers and text types. However, a collection of Web pages is not immediately suitable for exploration in the same way a traditional corpus is. Since the first Web as Corpus Workshop organised at the Corpus Linguistics 2005 Conference, a highly successful series of yearly Web as Corpus workshops provides a venue for interested researchers to meet, share ideas and discuss the problems and possibilities of compiling and using Web corpora. After a stronger focus on application-oriented natural language processing andWeb technology in recent years with workshops taking place at NAACL-HLT 2010, 2011 andWWW2012 the 8thWeb as Corpus Workshop returns to its roots in the corpus linguistics community. Accordingly, the leading theme of this workshop is the application of Web data in language research, including linguistic evaluation of Web-derived corpora as well as strategies and tools for high-quality automatic annotation ofWeb text. The workshop brings together presentations on all aspects of building, using and evaluating Web corpora, with a particular focus on the following topics: applications of Web corpora and other Web-derived data sets for language research automatic linguistic annotation of Web data such as tokenisation, part-of-speech tagging, lemma- tisation and semantic tagging (the accuracy of currently available off-the-shelf tools is still unsatisfactory for many types of Web data) critical exploration of the characteristics of Web data from a linguistic perspective and its applica- bility to language research presentation of Web corpus collection projects or software tools required for some part of this process (crawling, filtering, de-duplication, language identification, indexing, ...)},
  date = {2013-07-22},
  editor = {Evert, Stefan and Stemle, Egon and Rayson, Paul},
  location = {{Lancaster, UK}},
  publisher = {{WAC-8 Organising Committee}},
  title = {Proceedings of the 8th {{Web}} as {{Corpus Workshop}} ({{WAC}}-8)},
  url = {http://sigwac.org.uk/raw-attachment/wiki/WAC8/wac8-proceedings.pdf}
}

@book{wigham-stemle:2019:cmc17volume,
  abstract = {Communication between humans via networked devices has become an everyday part of people's lives across generations, cultures, geographical areas, and social classes. Shaped by the specific social and technical context in which it is produced, synchronous and asynchronous computer-mediated communication (CMC) has become increasingly participatory, interactive, and multimodal. User interactions and user-generated social media content offer a wide range of research opportunities for a growing multidisciplinary research community.
This edited volume combines methodological papers that focus on building and annotating CMC corpora and papers that offer a sociolinguistic analysis of different CMC corpora. The diversity of languages represented in the corpora include Arabic, French, German, Italian, English and Slovenian. In fact, the increasingly multilingual nature of CMC data is a recurring theme throughout the volume, as are the references to the importance of and compliance with standards for CMC corpora development in order to facilitate (the?) re-examination of corpora for reproducibility, and for other areas and objectives of investigation.
All but one paper are extended papers from the 2017 edition of the CMC and Social Media Corpora Conference held in Bolzano, Italy where the community met to discuss themes that related to the interaction between language, CMC, and society.},
  date = {2019-06-13},
  editor = {Wigham, Ciara R. and Stemle, Egon W.},
  isbn = {978-2-84516-860-2},
  langid = {english},
  location = {{Clermont-Ferrand, FR}},
  pagetotal = {155},
  publisher = {{Presses Universitaires Blaise Pascal}},
  series = {Cahiers du laboratoire de recherche sur le langage},
  title = {Building {{Computer}}-{{Mediated Communication Corpora}} for sociolinguistic {{Analysis}}},
  url = {http://pubp.giantchair.com/livre/?GCOI=28451100141150&language=en},
  urltype = {publisher}
}


