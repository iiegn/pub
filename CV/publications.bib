@unpublished{clarin2016,
    author = {Bei{\ss}wenger, Michael and Chanier, Thierry and Chiari, Isabella and Erjavec, Toma{\v{z}} and Fi{\v{s}}er, Darja and Herold, Axel and Lube{\v{s}}i{\'{c}}, Nikola and L{\"{u}}ngen, Harald and Poudat, C{\'{e}}line and Stemle, Egon and Storrer, Angelika and Wigham, Ciara},
    keywords = {CMC corpora,TEI,community building,computer-mediated communication,corpus annotation,language resources,social media corpora},
    abstract = {The paper presents best practices and results from projects in four CLARIN member countries dedicated to the creation of corpora of computer-mediated communication and social media interactions (CMC). Even though there are still many open issues related to building and annotating corpora of that type, there already exists a range of accessible solutions which have been tested in projects and which may serve as a starting point for a more precise discussion of how future standards for CMC corpora may (and should) be shaped like.},
    type = {Accepted},
    publisher = {Accepted for oral presentation at CLARIN Annual Conference 2016},
    url = {https://www.clarin.eu/event/2016/clarin-annual-conference-2016-aix-en-provence-france},
    title = {{Integrating corpora of computer-mediated communication into the language resources landscape: Initiatives and best practices from French, German, Italian and Slovenian projects}},
    month = oct,
    year = {2016}
}
@inproceedings{stemle:2016:WAC-X,
    title = {{bot.zen @ EmpiriST 2015 - A minimally-deep learning PoS-tagger (trained for German CMC and Web data)}},
    abstract = {This article describes the system that participated in the Part-of-speech tagging subtask of the "EmpiriST 2015 shared task on automatic linguistic annotation of computer-mediated communication / social media". The system combines a small assertion of trending techniques, which implement matured methods, from NLP and ML to achieve competitive results on PoS tagging of German CMC and Web corpus data; in particular, the system uses word embeddings and character-level representations of word beginnings and endings in a LSTM RNN architecture. Labelled data (Tiger v2.2 and EmpiriST) and unlabelled data (German Wikipedia) were used for training. The system is available under the APLv2 open-source license.},
    author = {Stemle, Egon W.},
    booktitle = {Proceedings of the 10th Web as Corpus Workshop (WAC-X) and the EmpiriST Shared Task},
    month = aug,
    pages = {115--119},
    publisher = {Association for Computational Linguistics},
    url = {http://anthology.aclweb.org/W/W16/W16-2614},
    year = {2016}
}
@book{WAC-X:2016,
    title = {{Proceedings of the 10th Web as Corpus Workshop (WAC-X) and the EmpiriST Shared Task}},
    abstract = {The World Wide Web has become increasingly popular as a source of linguistic data, not only within the NLP communities, but also with theoretical linguists facing problems of data sparseness or data diversity. Accordingly, web corpora continue to gain importance, given their size and diversity in terms of genres/text types. The field is still new, though, and a number of issues in web corpus construction need much additional research, both fundamental and applied. These issues range from questions of corpus design (e.g., assessment of corpus composition, sampling strategies and their relation to crawling algorithms, and handling of duplicated material) to more technical aspects (e.g., efficient implementation of individual post-processing steps in document cleaning and linguistic annotation, or large-scale parallelization to achieve web-scale corpus construction). Similarly, the systematic evaluation of web corpora, for example in the form of task based comparisons to traditional corpora, has only recently shifted into focus. For almost a decade, the ACL SIGWAC (http://www.sigwac.org.uk/), and especially the highly successful Web as Corpus (WAC) workshops have served as a platform for researchers interested in compilation, processing and application of web-derived corpora. Past workshops were co-located with major conferences on computational linguistics and/or corpus linguistics (such as EACL, NAACL, LREC, WWW, and Corpus Linguistics). WAC-X also featured the final workshop of the EmpiriST 2015 shared task "Automatic Linguistic Annotation of Computer-Mediated Communication / Social Media" (see https://sites.google.com/site/empirist2015/ for details) and the panel discussion "Corpora, open science, and copyright reforms" (see https://www.sigwac.org.uk/wiki/WAC-X{\#}paneldisc for details).},
    annote = {Workshop at ACL2016, Berlin, Germany},
    editor = {Cook, Paul and Evert, Stefan and Sch{\"{a}}fer, Roland and Stemle, Egon},
    month = aug,
    publisher = {Association for Computational Linguistics},
    type = {proceedings},
    url = {http://anthology.aclweb.org/W/W16/W16-26},
    year = {2016}
}
@inproceedings{NicolasStemleGlaznieksAbel2014,
    abstract = {We present an abstract and generic workflow, and detail how it has been implemented to build and annotate learner corpora. This workflow has been developed through an interdisciplinary collaboration between linguists, who annotate and use corpora, and computational linguists and computer scientists, who are responsible for providing technical support and adaptation or implementation of software components.},
    address = {Bern, Switzerland},
    annote = {Compiling and Using Learner Corpora to Teach and Assess Productive and Interactive Skills in Foreign Languages at University Level, Learner Corpora, Padova, Italia},
    author = {Nicolas, Lionel and Stemle, Egon and Glaznieks, Aivars and Abel, Andrea},
    booktitle = {Studies in Learner Corpus Linguistics: Research and Applications for Foreign Language Teaching and Assessment},
    chapter = {8},
    editor = {Castello, Erik and Ackerley, Katherine and Coccetta, Francesca},
    isbn = {978-3-0351-0736-4},
    month = jun,
    pages = {337--351},
    publisher = {Peter Lang},
    series = {Linguistic Insights},
    title = {{A Generic Data Workflow for Building Annotated Text Corpora}},
    volume = {190},
    year = {2015}
}
@inproceedings{FreyGlaznieksStemle2015,
    abstract = {This paper presents the DiDi Corpus, a corpus of South Tyrolean Data of Computer-mediated Communication (CMC). The corpus comprises around 650,000 tokens from Facebook wall posts, comments on wall posts and private messages, as well as socio-demographic data of participants. All data was automatically annotated with language information (de, it, en and others), and manually normalised and anonymised. Furthermore, semi-automatic token level annotations include part-of-speech and CMC phenomena (e.g. emoticons, emojis, and iteration of graphemes and punctuation). The anonymised corpus without the private messages is freely available for researchers; the complete and anonymised corpus is available after signing a non- disclosure agreement.},
    address = {Essen},
    author = {Frey, Jennifer-Carmen and Glaznieks, Aivars and Stemle, Egon W.},
    booktitle = {Proceedings of the 2nd Workshop on Natural Language Processing for Computer-Mediated Communication / Social Media at GSCL2015 (NLP4CMC2015)},
    month = sep,
    publisher = {German Society for Computational Linguistics \& Language Technology},
    title = {{The DiDi Corpus of South Tyrolean CMC Data}},
    type = {article},
    year = {2015}
}
@inproceedings{KranebitterStemle2013,
    abstract = {Graphical tools to organise and represent knowledge are useful in terminology work to facilitate building concept systems. Creating and maintaining hierarchically structured concept relation maps while manually gathering data for terminological databases helps to gain and maintain an overview of concept relations, supports terminology work in groups, and helps new team members catching up on the subject field. This article describes our approach to support the building of concept systems in comparative legal terminology using the concept mapping software CmapTools (IHMC): we build hierarchically structured concept relation maps where linking lines with arrowheads between concepts of the same legal system represent generic-specific relations, and combined concept relation maps where dashed lines without arrowheads connect similar concepts in different legal systems.},
    address = {Cham\'{e}bry, France},
    author = {Kranebitter, Klara and Stemle, Egon W.},
    booktitle = {Terminologie \& Ontologie: Th\'{e}ories et Applications. Actes de la septi\`{e}me conf\'{e}rence TOTh 2013},
    editor = {Roche, Christophe and Costa, Rute and Depecker, Lo\"{\i}c and Thoiron, Philippe},
    month = jun,
    pages = {97--116},
    publisher = {Institut Porphyre, Savoir et Connaissance},
    title = {{Constructing concept relation maps to support building concept systems in comparative legal terminology}},
    year = {2013}
}
@article{StemleOnysko2014,
    abstract = {This article focuses on automatic text classification which aims at identifying the first language (L1) background of learners of English. A particular question arising in the context of automated L1 identification is whether any features that are informative for a machine learning algorithm relate to L1-specific transfer phenomena. In order to explore this issue further, we discuss the results of a study carried out in the wake of a Native Language Identification Task. The task is based on the TOEFL11 corpus (cf. Blanchard et al. 2013), which involves a sample of 12,100 essays written by participants in the TOEFL® test from 11 different language backgrounds (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, and Turkish). The article will show our results in automatic L1 detection in the TOEFL11 corpus. These results are discussed in light of relevant transfer features which turned out to be particularly informative for automatic detection of L1 German and L1 Italian.},
    author = {Stemle, Egon and Onysko, Alexander},
    _doi = {10.1075/hsld.4.13ste},
    editor = {Peukert, Hagen},
    journal = {Transfer Effects in Multilingual Language Development},
    month = sep,
    pages = {297--321},
    publisher = {John Benjamins},
    title = {{Automated L1 identification in English learner essays and its implications for language transfer}},
    url = {https://benjamins.com/catalog/hsld.4.13ste},
    year = {2015}
}
@inproceedings{ABEL14.934,
    abstract = {We introduce the KoKo corpus, a collection of German L1 learner texts annotated with learner errors, along with the methods and tools used in its construction and evaluation. The corpus contains both texts and corresponding survey information from 1,319 pupils and amounts to around 716,000 tokens. The evaluation of the quality of the performed transcriptions and annotations shows an accuracy of orthographic error annotations of approximately 80\% as well as high accuracy of transcriptions (> 99\%), automatic tokenisation (> 99\%), sentence splitting (> 96\%) and POS-tagging (> 94\%). The KoKo corpus will be published at the end of 2014 and be the first accessible linguistically annotated German L1 learner corpus. It will represent a valuable source for research and teaching on German as L1 language, in particular with regards to writing skills.},
    address = {Reykjavik, Iceland},
    author = {Abel, Andrea and Glaznieks, Aivars and Nicolas, Lionel and Stemle, Egon},
    booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)},
    editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Stelios, Piperidis},
    isbn = {978-2-9517408-8-4},
    keywords = {German Language,Learner Corpora},
    month = may,
    pages = {2414--2421},
    publisher = {European Language Resources Association (ELRA)},
    title = {{KoKo: An L1 Learner Corpus for German}},
    url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/934\_Paper.pdf},
    year = {2014}
}
@inproceedings{LYDING14.517,
    abstract = {In this article, we present interHist, a compact visualization for the interactive exploration of results to complex corpus queries. Integrated with a search interface to the PAIS\`{A} corpus of Italian web texts, interHist aims at facilitating the exploration of large results sets to linguistic corpus searches. This objective is approached by providing an interactive visual overview of the data, which supports the user-steered navigation by means of interactive filtering. It allows to dynamically switch between an overview on the data and a detailed view on results in their immediate textual context, thus helping to detect and inspect relevant hits more efficiently. We provide background information on corpus linguistics and related work on visualizations for language and linguistic data. We introduce the architecture of interHist, by detailing the data structure it relies on, describing the visualization design and providing technical details of the implementation and its integration with the corpus querying environment. Finally, we illustrate its usage by presenting a use case for the analysis of the composition of Italian noun phrases.},
    address = {Reykjavik, Iceland},
    author = {Lyding, Verena and Nicolas, Lionel and Stemle, Egon},
    booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)},
    editor = {Calzolari, Nicoletta and Choukri, Khalid and Declerck, Thierry and Loftsson, Hrafn and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Stelios, Piperidis},
    isbn = {978-2-9517408-8-4},
    keywords = {corpus linguistics,language analysis,visualization},
    month = may,
    pages = {635--641},
    publisher = {European Language Resources Association (ELRA)},
    title = {{'interHist' - an interactive visual interface for corpus exploration}},
    url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/517\_Paper.pdf},
    year = {2014}
}
@inproceedings{GenereuxStemleNicolasLyding2014,
    abstract = {In this paper, we present ongoing experiments for correcting OCR errors on German newspapers in Fraktur font. Our approach borrows from techniques for spelling correction in context using a probabilistic edit-operation error model and lexical resources. We highlight conditions in which high error reduction rates can be obtained and where the approach currently stands with real data.},
    address = {Pisa, Italy},
    author = {G\'{e}n\'{e}reux, Michel and Stemle, Egon W. and Nicolas, Lionel and Lyding, Verena},
    booktitle = {Proceedings of the First Italian Conference on Computational Linguistics (CLiC-it 2014)},
    editor = {Basili, Roberto and Lenci, Alessandro and Magnini, Bernardo},
    month = dec,
    title = {{Correcting OCR errors for German in Fraktur font}},
    url = {http://clic.humnet.unipi.it/proceedings/vol1/CLICIT2014136.pdf},
    year = {2014}
}
@article{GlaznieksNicolasStemleAbelLyding2014,
    abstract = {Decisions at the outset of preparing a learner corpus are of crucial importance for how the corpus can be built and how it can be analysed later on. This paper presents a generic workflow to build learner corpora while taking into account the needs of the users. The workflow results from an extensive collaboration between linguists that annotate and use the corpus and computer linguists that are responsible for providing technical support. The paper addresses the linguists' research needs as well as the availability and usability of language technology tools necessary to meet them. We demonstrate and illustrate the relevance of the workflow using results and examples from our L1 learner corpus of German ("KoKo").},
    annote = {Learner Language, Learner Corpora 2012 (LLLC2012), Oct 5-6, University of Oulu, Finland},
    author = {Glaznieks, Aivars and Abel, Andrea and Lyding, Verena and Nicolas, Lionel and Stemle, Egon},
    editor = {Nikula, Tarja and Takala, Sauli and Yl\"{o}nen, Sabine},
    issn = {1457-9863},
    journal = {Apples - Journal of Applied Language Studies},
    keywords = {German as a first language,L1 learner corpus,corpus building workflow},
    month = dec,
    number = {3},
    pages = {5--20},
    publisher = {Centre for Applied Language Studies, University of Jyv\"{a}skyl\"{a}},
    title = {{Establishing a Standardised Procedure for Building Learner Corpora}},
    url = {http://apples.jyu.fi/ArticleFile/download/535},
    volume = {8},
    year = {2014}
}
@article{GlaznieksStemle2014,
    abstract = {Special Issue: Building and annotating corpora of computer-mediated discourse. Issues and Challenges at the Inteface of Corpus and Computational Linguistics},
    author = {Glaznieks, Aivars and Stemle, Egon},
    editor = {Bei{\ss}wenger, Michael and Oostdijk, Nellek and Storrer, Angelika and van den Heuvel, Henk},
    issn = {2190-6858},
    journal = {Journal for Language Technology and Computational Linguistics (JLCL)},
    month = dec,
    number = {2},
    pages = {31--57},
    publisher = {JLCL},
    title = {{Challenges of building a CMC corpus for analyzing writer's style by age: The DiDi project}},
    url = {http://www.jlcl.org/2014\_Heft2/2GlaznieksStemle.pdf},
    volume = {29},
    year = {2014}
}
@inproceedings{FreyStemleGlaznieks2014,
    abstract = {In this paper, we propose an integrated web strategy for mixed sociolinguistic research methodologies in the context of social media corpora. After stating the particular challenges for building corpora of private, non-public computer-mediated communication, we will present our solution to these problems: a Facebook web application for the acquisition of such data and the corresponding meta data. Finally, we will discuss positive and negative implications for this method.},
    address = {Hildesheim, Germany},
    author = {Frey, Jennifer-Carmen and Stemle, Egon W. and Glaznieks, Aivars},
    booktitle = {Workshop Proceedings of the 12th Edition of the KONVENS Conference},
    editor = {Faa{\ss}, Gertrud and Ruppenhofer, Josef},
    month = oct,
    pages = {11--15},
    publisher = {Universitatsverlag Hildesheim, Germany},
    title = {{Collecting language data of non-public social media profiles}},
    url = {http://www.uni-hildesheim.de/konvens2014/data/konvens2014-workshop-proceedings.pdf},
    year = {2014}
}
@inproceedings{paisa2014,
    abstract = {PAIS\`{A} is a Creative Commons licensed, large web corpus of contemporary Italian. We describe the design, harvesting, and processing steps involved in its creation.},
    address = {Gothenburg, Sweden},
    author = {Lyding, Verena and Stemle, Egon and Borghetti, Claudia and Brunello, Marco and Castagnoli, Sara and Orletta, Felice Dell and Dittmann, Henrik and Lenci, Alessandro and Pirrelli, Vito},
    booktitle = {Proceedings of the 9th Web as Corpus Workshop (WaC-9)},
    month = apr,
    pages = {36--43},
    publisher = {Association for Computational Linguistics},
    title = {{The PAIS\`{A} Corpus of Italian Web Texts}},
    url = {http://aclweb.org/anthology/W14-0406},
    year = {2014}
}
@inproceedings{Nicolas2013a,
    abstract = {In this paper, we report on an unsupervised greedy-style process for acquiring phrase translations from sentence-aligned parallel corpora. Thanks to innovative selection strategies, this process can acquire multiple translations without size criteria, i.e. phrases can have several translations, can be of any size, and their size is not considered when selecting their translations. Even though the process is in an early development stage and has much room for improvements, evaluation shows that it yields phrase translations of high precision that are relevant to machine translation but also to a wider set of applications including memory-based translation or multi-word acquisition.},
    address = {Hissar, Bulgaria},
    author = {Nicolas, Lionel and Stemle, Egon W. and Kranebitter, Klara and Lyding, Verena},
    booktitle = {Proceedings of Recent Advances in Natural Language Processing, RANLP 2013},
    editor = {Angelova, Galia and Bontcheva, Kalina and Mitkov, Ruslan},
    keywords = {Bilingual lexicon,Parallel Corpora,Phrase Translation,Unsupervised Learning},
    month = sep,
    pages = {516--524},
    publisher = {RANLP 2011 Organising Committee / ACL},
    title = {{High-Accuracy Phrase Translation Acquisition Through Battle-Royale Selection}},
    url = {http://aclweb.org/anthology/R/R13/R13-1068.pdf},
    year = {2013}
}
@misc{Stemle2013c,
    abstract = {Article in Academia (science magazine by EURAC and unibz), Bolzano, Italy},
    author = {Stemle, Egon W. and Onysko, Alexander},
    booktitle = {Academia},
    month = dec,
    pages = {24--25},
    title = {{Language as a Detective Story}},
    volume = {64},
    type = {magazine},
    year = {2013}
}
@book{WAC8,
    abstract = {Web corpora and other Web-derived data have become a gold mine for corpus linguistics and natural language processing. The Web is an easy source of unprecedented amounts of linguistic data from a broad range of registers and text types. However, a collection of Web pages is not immediately suitable for exploration in the same way a traditional corpus is. Since the first Web as Corpus Workshop organised at the Corpus Linguistics 2005 Conference, a highly successful series of yearly Web as Corpus workshops provides a venue for interested researchers to meet, share ideas and discuss the problems and possibilities of compiling and using Web corpora. After a stronger focus on application-oriented natural language processing andWeb technology in recent years with workshops taking place at NAACL-HLT 2010, 2011 andWWW2012 the 8thWeb as Corpus Workshop returns to its roots in the corpus linguistics community. Accordingly, the leading theme of this workshop is the application of Web data in language research, including linguistic evaluation of Web-derived corpora as well as strategies and tools for high-quality automatic annotation ofWeb text. The workshop brings together presentations on all aspects of building, using and evaluating Web corpora, with a particular focus on the following topics: applications of Web corpora and other Web-derived data sets for language research automatic linguistic annotation of Web data such as tokenisation, part-of-speech tagging, lemma- tisation and semantic tagging (the accuracy of currently available off-the-shelf tools is still unsatisfactory for many types of Web data) critical exploration of the characteristics of Web data from a linguistic perspective and its applica- bility to language research presentation of Web corpus collection projects or software tools required for some part of this process (crawling, filtering, de-duplication, language identification, indexing, ...)},
    annote = {Workshop at the seventh international Corpus Linguistics conference (CL2013), Lancaster, UK},
    editor = {Evert, Stefan and Stemle, Egon and Rayson, Paul},
    month = jul,
    publisher = {WAC-8 Organising Committee},
    title = {{Proceedings of the 8th Web as Corpus Workshop (WAC-8)}},
    type = {proceedings},
    url = {http://sigwac.org.uk/raw-attachment/wiki/WAC8/wac8-proceedings.pdf},
    year = {2013}
}
@inproceedings{Lyding2013a,
    abstract = {In this article, we present the multi-faceted interface to the open PAIS\`{A} corpus of Italian. Created within the project PAIS\`{A} (Piattaforma per l’Apprendimento dell’Italiano Su corpora Annotati) [1], the corpus is designed to be freely available for non-commercial processing, usage and distribution by the public. Hence, this automatically annotated corpus (for lemma, part-of-speech and dependency information) is exclusively composed of documents licensed under Creative Commons (CC) licenses [2].The dedicated corpus interface is designed to provide flexible, powerful, and easy-to-use modes of corpus access, with the objective to support language learning, language practicing and linguistic analyses. We present in detail the interface’s functionalities and discuss the underlying design decisions. We introduce the four principal components of the interface, describe supported display formats and present two specific features added to increase the interface's relevance for language learning. The main search components are (1) a basic search that adopts a "Google-style" search box, (2) an advanced search that provides elaborated graphical search options, and (3) a search that makes use of the powerful CQP query language of the Open Corpus Workbench [3]. In addition, (4) a filter interface for retrieving full-text corpus documents based on keyword searches is available. It is likewise providing the means for building temporary sub-corpora for specific topics. Users can choose among different display formats for the search results. Besides the established KWIC (KeyWord In Context) and full sentence views, graphical representations of the dependency relation information as well as keyword distributions are available. These dynamic displays are based on a visualisation for dependency graphs [4] and one for Word Clouds [5], which build on latest developments in information visualisation for language data. Two special features for novice learners are integrated into each search component. The first feature is a function for restricting search results to sentences of limited complexity. Search results are automatically filtered based on formal text characteristics such as sentence length, vocabulary, etc. The second is the supply of pre-defined search queries for linguistic constructions such as sentences in passive voice, questions, etc. Finally, we show how the PAIS\`{A} interface can be employed in different language teaching tasks. In particular, we present a complete unit of work aimed at learners of Italian (CEFR level A2/B1) and centered on students’ direct use of the interface and its functionalities. By doing so, we are giving concrete examples for targeted searches and interactions with the provided language material, as well as an exemplification of how the use of the corpus can be integrated with communicative language activities in the classroom.},
    address = {Florence, Italy},
    author = {Lyding, Verena and Borghetti, Claudia and Dittmann, Henrik and Nicolas, Lionel and Stemle, Egon},
    booktitle = {Proceedings of the International Conference ICT for Language Learning, 6th edition},
    isbn = {978-88-6292-423-8},
    keywords = {Corpus Linguistics,Linguistic Visualization,Visualization},
    month = nov,
    publisher = {libreriauniversitaria.it},
    title = {{Open Corpus Interface for Italian Language Learning}},
    url = {http://conference.pixel-online.net/ICT4LL2013/common/download/Paper\_pdf/270-ITL56-FP-Lyding-ICT2013.pdf},
    year = {2013}
}
@inproceedings{NicolasStemleKranebitter2012,
    abstract = {We report on on-going work to derive translations of phrases from parallel corpora. We describe an unsupervised and knowledge-free greedy-style process relying on innovative strategies for choosing and discarding candidate translations. This process manages to acquire multiple translations combining phrases of equal or different sizes. The preliminary evaluation performed confirms both its potential and its interest.},
    address = {Vienna, Austria},
    author = {Nicolas, Lionel and Stemle, Egon W. and Kranebitter, Klara},
    booktitle = {11th Conference on Natural Language Processing, KONVENS 2012, Empirical Methods in Natural Language Processing},
    editor = {Jancsary, Jeremy},
    keywords = {Bilingual lexicon,Parallel Corpora,Phrase Translation,Unsupervised Learning},
    month = sep,
    pages = {471--479},
    publisher = {\"{O}GAI},
    title = {{Towards high-accuracy bilingual phrase acquisition from parallel corpora}},
    url = {http://www.oegai.at/konvens2012/proceedings/68\_nicolas12w/},
    year = {2012}
}
@inproceedings{Bonin:2012:AAT:2392747.2392768,
    abstract = {Developing content extraction methods for Humanities domains raises a number of chal- lenges, from the abundance of non-standard entity types to their complexity to the scarcity of data. Close collaboration with Humani- ties scholars is essential to address these chal- lenges. We discuss an annotation schema for Archaeological texts developed in collabora- tion with domain experts. Its development re- quired a number of iterations to make sure all the most important entity types were included, as well as addressing challenges including a domain-specific handling of temporal expres- sions, and the existence of many systematic types of ambiguity.},
    address = {Jeju, Republic of Korea},
    author = {Bonin, Francesca and Cavulli, Fabio and Noriller, Aronne and Poesio, Massimo and Stemle, Egon W.},
    booktitle = {Proceedings of the Sixth Linguistic Annotation Workshop},
    month = jul,
    number = {July},
    pages = {134--138},
    publisher = {Association for Computational Linguistics},
    series = {LAW VI '12},
    title = {{Annotating Archaeological Texts: An Example of Domain-Specific Annotation in the Humanities}},
    url = {http://dl.acm.org/citation.cfm?id=2392747.2392768},
    year = {2012}
}
@inproceedings{PoesioSDH2011,
    address = {Copenhagen, Denmark},
    author = {Poesio, Massimo and Barbu, Eduard and Bonin, Francesca and Cavulli, Fabio and Ekbal, Asif and Stemle, Egon and Girardi, Christian},
    booktitle = {Proceedings of Supporting Digital Humanities (SDH2011): Answering the unaskable},
    editor = {Maegaard, Bente},
    month = nov,
    title = {{The Humanities Research Portal: Human Language Technology Meets Humanities Publication Archives}},
    year = {2011}
}
@inproceedings{murphy-stemle:2011:DIALECTS,
    abstract = {Small, manually assembled corpora may be available for less dominant languages and dialects, but producing web-scale resources remains a challenge. Even when considerable quantities of text are present on the web, finding this text, and distinguishing it from related languages in the same region can be difficult. For example less dominant variants of English (e.g. New Zealander, Singaporean, Canadian, Irish, South African) may be found under their respective national domains, but will be partially mixed with Englishes of the British and US varieties, perhaps through syndication of journalism, or the local reuse of text by multinational companies. Less formal dialectal usage may be scattered more widely over the internet through mechanisms such as wiki or blog authoring. Here we automatically construct a corpus of Hiberno-English (English as spoken in Ireland) using a variety of methods: filtering by national domain, filtering by orthographic conventions, and bootstrapping from a set of Ireland-specific terms (slang, place names, organisations). We evaluate the national specificity of the resulting corpora by measuring the incidence of topical terms, and several grammatical constructions that are particular to Hiberno-English. The results show that domain filtering is very effective for isolating text that is topic-specific, and orthographic classification can exclude some non-Irish texts, but that selected seeds are necessary to extract considerable quantities of more informal, dialectal text.},
    address = {Edinburgh, Scotland, UK},
    author = {Murphy, Brian and Stemle, Egon W.},
    booktitle = {Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties},
    month = jul,
    pages = {22--29},
    publisher = {Association for Computational Linguistics},
    title = {{PaddyWaC: A Minimally-Supervised Web-Corpus of Hiberno-English}},
    url = {http://www.aclweb.org/anthology/W11-2603},
    year = {2011}
}
@article{EkbalEtAl:2011,
    author = {Ekbal, Asif and Bonin, Francesca and Saha, Sriparna and Stemle, Egon and Barbu, Eduard and Cavulli, Fabio and Girardi, Christian and Poesio, Massimo},
    journal = {Journal for Language Technology and Computational Linguistics (JLCL)},
    month = nov,
    number = {2},
    pages = {39--51},
    title = {{Rapid Adaptation of NE Resolvers for Humanities Domains using Active Annotation}},
    url = {http://www.jlcl.org/2011\_Heft2/9.pdf},
    volume = {26},
    year = {2011}
}
@inproceedings{poesio-EtAl:2011:LaTeCH-2011,
    abstract = {Most existing HLT pipelines assume the input is pure text or, at most, HTML and either ignore (logical) document structure or remove it. We argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved.},
    address = {Portland, OR, USA},
    author = {Poesio, Massimo and Barbu, Eduard and Stemle, Egon W. and Girardi, Christian},
    booktitle = {Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH 2011)},
    month = jun,
    pages = {54--62},
    publisher = {Association for Computational Linguistics},
    title = {{Structure-Preserving Pipelines for Digital Libraries}},
    url = {http://www.aclweb.org/anthology/W11-1508},
    year = {2011}
}
@inproceedings{RodriguezDeloguVersleyStemlePoesio2010,
    address = {Valletta, Malta},
    author = {Rodr\'{i}guez, Kepa Joseba and Delogu, Francesca and Versley, Jannick and Stemle, Egon W. and Poesio, Massimo},
    booktitle = {Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC'10)},
    editor = {Calzolari, Nicoletta and Choukri, Khalid and Maegaard, Bente and Mariani, Joseph and Odijk, Jan and Piperidis, Stelios and Rosner, Mike and Tapias, Daniel},
    isbn = {2-9517408-6-7},
    month = may,
    publisher = {European Language Resources Association (ELRA)},
    title = {{Anaphoric Annotation of Wikipedia and Blogs in the Live Memories Corpus}},
    url = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/431\_Paper.pdf},
    year = {2010}
}
@inproceedings{StegerStemle2009,
    abstract = {Algorithmic processing of Web content mostly works on textual contents, neglecting visual information. Annotation tools largely share this deficit as well. We specify requirements for an architecture to overcome both problems and propose an implementation, the KrdWrd system. It uses the Gecko rendering engine for both annotation and feature extraction, providing unified data access in every processing step. Stable data storage and collaboration control scripts for group annotations of massive corpora are provided via a Web interface coupled with a HTTP proxy. A modular interface allows for linguistic and visual data feature extractor plugins. The implementation is suitable for many tasks in theWeb as corpus domain and beyond.},
    address = {Donostia-San Sebastian, Basque Country, Spain},
    author = {Steger, Johannes and Stemle, Egon},
    booktitle = {Proceedings of the Fifth Web as Corpus Workshop (WAC5)},
    editor = {Alegria, I\~{n}aki and Leturia, Igor and Sharoff, Serge},
    month = sep,
    pages = {63--70},
    publisher = {Elhuyar Fundazioa},
    title = {{KrdWrd: Architecture for Unified Processing of Web Content}},
    url = {https://www.sigwac.org.uk/raw-attachment/wiki/WAC5/WAC5\_proceedings.pdf},
    year = {2009}
}
@inproceedings{BlohmCimianoStemle2007,
    author = {Blohm, Sebastian and Cimiano, Philipp and Stemle, Egon},
    booktitle = {Proceedings of the 22nd Conference on Artificial Intelligence (AAAI-07)},
    isbn = {978-1-57735-323-2},
    month = jul,
    pages = {1316--1323},
    publisher = {Association for the Advancement of Artificial Intelligence},
    title = {{Harvesting Relations from the Web - Quantifiying the Impact of Filtering Functions}},
    url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-208.pdf},
    year = {2007}
}
@inproceedings{FIASCO2007,
    address = {Louvain-la-Neuve},
    author = {Bauer, Daniel and Degen, Judith and Deng, Xiaoye and Herger, Priska and Gasthaus, Jan and Giesbrecht, Eugenie and Jansen, Lina and Kalina, Christin and Kr\"{u}ger, Thorben and M\"{a}rtin, Robert and Schmidt, Martin and Scholler, Simon and Steger, Johannes and Stemle, Egon and Evert, Stefan},
    booktitle = {Proceedings of the Third Web as Corpus Workshop (WAC3)},
    editor = {Fairon, C\'{e}drick and Naets, Hubert and Kilgarriff, Adam and de Schryver, Gilles-Maurice},
    month = sep,
    publisher = {Presses universitaires de Louvain},
    title = {{FIASCO: Filtering the Internet by Automatic Subtree Classification, Osnabr\"{u}ck}},
    url = {http://purl.org/stefan.evert/PUB/BauerEtc2007\_FIASCO.pdf},
    year = {2007}
}
@techreport{ASADO2005,
    abstract = {Final Report of the one year cooperation between the Universities of Osnabr\"{u}ck and Hildesheim, and the aircraft manufacturer AIRBUS to research methodologies and technologies to analyze and structure the huge amount of documentation produced during aircraft construction. The work was done in a study project carried out in close cooperation with seven students of cognitive science advised by two lectures of the Institute of Cognitive Science of the University of Osnabr\"{u}ck and with one student of international information management advised by one professor of the Institute of Applied Linguistics of the University of Hildesheim.},
    author = {Bleichner, Martin and Giesbrecht, Eugenie and Gust, Helmar and Leicht, Eva-Maria and Ludewig, Petra and M\"{o}ller, Sabine and M\"{u}ller, Wiebke and Schmidt, Martin and Stefaner, Moritz and Stemle, Egon and Wilke, Katja},
    institution = {Institute of Cognitive Science at the University of Osnabr\"{u}ck and Institute of Applied Linguistics at the University of Hildesheim},
    month = nov,
    title = {{ASADO: The Analysis and Structuring of Aviation Documents - Final Report}},
    year = {2005}
}
