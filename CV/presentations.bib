@misc{stemle:2017:dhcmc,
    address = {Duisburg, Germany},
    author = {Stemle, Egon W.},
    booktitle = {Integrating a new type of language resource into the Digital Humanities landscape: French-German colloquium on standards for corpora of computer-mediated communication},
    institution = {University of Duisburg-Essen},
    month = jun,
    title = {{DiDi Corpus}},
    type = {talk},
    url = {https://sites.google.com/view/dhcmc2017/},
    year = {2017}
}
@misc{lyding-nicolas-stemle:2016:iidh-talk,
    address = {Merano/Meran, IT},
    author = {Lyding*, Verena and Nicolas*, Lionel and Stemle*, Egon W.},
    booktitle = {International Workshop:"Initiativen und Innovationen in den Digital Humanities"},
    institution = {Akademie Deutsch-Italienischer Studien, Meran, IT, in Kooperation mit dem Forschungszentrum Digital Humanities, Universit{\"{a}}t Innsbruck (Brenner-Archiv), AT},
    month = nov,
    title = {{Cross-institutional cooperation initiatives in the Digital Humanities - challenges and infrastructures}},
    type = {talk},
    year = {2016}
}
@misc{Stemle2015c,
    abstract = {Following a sociolinguistic user-based perspective on language data, the project DiDi investigated the linguistic strategies employed by South Tyrolean users on Facebook. South Tyrol is a multilingual region (Italian, German, and Ladin are official languages) where the South Tyrolean dialect of German is frequently used in different communicative contexts. Thus, regional and social codes are often also used in written communication and in computer mediated communication. With a research focus on users with L1 German living in South Tyrol, the main research question was whether people of different age use language in a similar way or in an age-specific manner. The project lasted 2 years (June 2013 - May 2015). We created a corpus of Facebook communication that can be linked to other user-based data such as age, web experience and communication habits. We gathered socio-demographic information through an online questionnaire and collected the language data of the entire range of social interactions, i.e. publicly accessible data as well as non-public conversations (status updates and comments, private messages, and chat conversations) written and published just for friends or a limited audience. The data acquisition comprised about 150 users interacting with the app, offering access to their language data and answering the questionnaire. In this talk, I will present the project, its data acquisition app and text annotation processes (automatic, semi-automatic, and manual), discuss their strengths and limitations, and present results from our data analyses.},
    address = {Rennes, France},
    author = {Stemle, Egon W.},
    booktitle = {First international research days on Social Media and CMC Corpora for the eHumanities (ird-cmc-rennes)},
    institution = {Rennes 2 University},
    month = oct,
    title = {{The DiDi Project: Collecting, Annotating, and Analysing South Tyrolean Data of Computer-mediated Communication}},
    type = {invited talk},
    url = {http://ird-cmc-rennes.sciencesconf.org/},
    year = {2015}
}
@misc{Stemle2015b,
    address = {Herstmonceux Castle, Sussex, UK},
    author = {Stemle, Egon W. and Nicolas, Lionel and Lyding, Verena},
    booktitle = {COST ENel WG3 meeting "Automatic Knowledge Acquisition for Lexicography"},
    month = aug,
    title = {{South Tyrolian Neologisms Project}},
    type = {short talk},
    year = {2015}
}
@misc{Stemle2015a,
    abstract = {The web has become increasingly popular as a source of linguistic data, not only within the NLP community, but also with lexicographers and linguists. Accordingly, web corpora continue to gain importance, given their size and diversity in terms of genres/text types. However, a number of issues in web corpus construction still need much research, ranging from questions of corpus design to more-technical aspects of efficient construction of large corpora. Similarly, the systematic evaluation of web corpora, for example in the form of task-based comparisons to traditional corpora, has only lately shifted into focus. This year we are excited to meet at Electronic lexicography in the 21st century: linking lexical data in the digital age (eLex 2015). Our meeting provides a forum for those with common interest in innovative developments in the field of lexicography and (very large) web-based corpora. We will set the stage with a talk ranging from web corpus construction to the evaluation of web corpora.},
    address = {Herstmonceux Castle, UK},
    author = {Stemle, Egon W.},
    booktitle = {Web as Corpus Meeting@eLex 2015},
    month = aug,
    title = {{Web Corpus Creation, Cleaning and Evaluation}},
    type = {lecture},
    year = {2015}
}
@misc{StemleOnysko2014b,
    abstract = {This talk gives an overview to our study: it is based on a corpus of TOEFL English test essays written by learners of 11 different first language backgrounds. In our research we use automated machine learning techniques to automatically classify the learner texts according to the L1 of their authors. Furthermore, we take a closer look at some of the most informative features for the classifier regarding L1 German and L1 Italian speakers. Some of these features show a possible origin in processes of L1 transfer.},
    address = {Bozen/Bolzano, Italy},
    author = {Stemle*, Egon W. and Onysko*, Alexander},
    booktitle = {'Work in Progress Series'},
    institution = {Kompetenzzentrum Sprachen, Freie Universit{\"{a}}t Bozen},
    month = apr,
    title = {{Automated L1 identification in English learner essays and its implications for language transfer}},
    type = {talk},
    year = {2014}
}
@misc{Glaznieks2013b,
    abstract = {Die automatische Verarbeitung von IBK-Daten stellt herk{\"{o}}mmliche Verfahren im Bereich der Sprachtechnologie vor gro{\ss}e Herausforderungen. H{\"{a}}ufige Abweichungen von der Standardschreibung (z. B. Versprachlichungsprinzipien der N{\"{a}}he, Schnellschreibph{\"{a}}nomene) und genrespezifische Elemente (z. B. Emoticons, Inflektive, spezifische Elemente einzelner Kommunikationsdienste) f{\"{u}}hren mit vorhandenen Verarbeitungswerkzeugen h{\"{a}}ufig zu unbefriedigenden Ergebnissen, weshalb die Werkzeuge eine Anpassung oder {\"{U}}berarbeitung, letztlich vielleicht sogar eine Neuentwicklung ben{\"{o}}tigen. Die voranschreitende technologische Durchdringung unseres Alltags, der immer einfachere Zugang zu Kommunikationsmedien, das Heranwachsen von „Digital Natives“ und schlie{\ss}lich das gewachsene Bewusstsein f{\"{u}}r die wissenschaftliche Relevanz der dabei praktizierten Kommunikationsformen und der produzierten Daten machen die Probleme f{\"{u}}r die aktuelle korpuslinguistische Forschung umso relevanter. Eine besondere Herausforderung stellen n{\"{a}}hesprachliche Ph{\"{a}}nomene dar. In einer variet{\"{a}}tenreichen Sprache wie dem Deutschen k{\"{o}}nnen solche Ph{\"{a}}nomene unz{\"{a}}hlige Formen annehmen, wobei sozio-, regio- und dialektale Elemente eine entscheidende Rolle spielen. In Regionen des deutschen Sprachraums, in denen eine Situation der Diglossie zwischen Dialekt und Standardsprache vorherrscht, wie das etwa in der Schweiz oder in S{\"{u}}dtirol der Fall ist, wird der Dialekt als die sprachliche Variet{\"{a}}t der N{\"{a}}he in der IBK h{\"{a}}ufig vollst{\"{a}}ndig verschriftlicht, d.h. ganze Kommunikationen laufen im Dialekt ab. Inwiefern f{\"{u}}r solche Texte Verarbeitungswerkzeuge verwendet werden k{\"{o}}nnen, die an einer schriftlichen Standardvariet{\"{a}}t ausgerichtet sind, und welche praktikable Herangehensweise am vielversprechendsten zu einer hinreichend gro{\ss}en und ausgewogenen Abdeckung der Sprachdaten f{\"{u}}hrt, ist unklar. In der Startphase eines Projektes, in dem aus IBK-Sprachdaten von S{\"{u}}dtiroler NutzerInnen ein Korpus erstellt wird, wurde versucht, offene Fragen dieser Art zu kl{\"{a}}ren. Ein Testkorpus aus authentischen, im S{\"{u}}dtiroler Dialekt verfassten IBK-Texten wurde dazu mit herk{\"{o}}mmlichen Werkzeugen (Tokenisierung, Satzgrenzen- und Wortartenerkennung, Lemmatisierung) verarbeitet. Die Auswirkungen unterschiedlicher Anpassungen (z.B. Erweiterung des Lexikons, Hinzuf{\"{u}}gen von „target words“ u.a.) auf die Verarbeitungsleistung wurden dabei evaluiert. Der Vortrag wird die einzelnen Anpassungen und die jeweiligen Ergebnisse der Evaluation vorstellen.},
    address = {Darmstadt, German},
    author = {Glaznieks, Aivars and Stemle, Egon W.},
    booktitle = {Workshop on "Verabeitung und Annotation von Sprachdaten aus Genres internetbasierter Kommunikation" at the International Conference of the German Society for Computational Linguistics and Language Technology (GSCL 2013)},
    institution = {TU Darmstadt},
    month = sep,
    title = {{Herausforderungen bei der automatischen Verarbeitung von dialektalen IBK-Daten}},
    type = {talk},
    url = {https://www.researchgate.net/publication/259344920{\_}Herausforderungen{\_}bei{\_}der{\_}automatischen{\_}Verarbeitung{\_}von{\_}dialektalen{\_}IBK-Daten?ev=prf{\_}pub},
    year = {2013}
}
@misc{Stemle2013a,
    abstract = {"Copyright issues remain a gray area in compiling and distributing Web corpora"[1]; and even though "If a Web corpus is infringing copyright, then it is merely doing on a small scale what search engines such as Google are doing on a colossal scale"[2], and "If you want your webpage to be removed from our corpora, please contact us"[3], are practical stances the former, given the increased heat Google{\&}Co. are facing on this matter, might be of limited use, and the latter still entails some legal risk. Also, "Even if the concrete legal threats are probably minor, they may have negative impact on fund-raising"[4]. So, (adding the possibility for) minimizing the legal risks, or rather, actively facing and eliminating them is paramount to the WaCky initiative. Theoretical aspects of creating 'a free' corpus are covered in [5]; one result is that 'the Creative Commons (CC) licenses' is the most promising legal model to use as a filter for web pages. Also, examples of 'free' (CC) corpora already exist, cf. [6,7]. On a technical level, the change from Google/Yahoo! to Bing as a search API for BootCaT complicated things: Google and Yahoo! both allow for filtering search results according to a - perceived - CC license of a page (for Yahoo! this filter was part of BootCaT and was used in [7]); unfortunately, Bing does not support this option. Then, the "Best Practices for Marking Content with CC Licenses"[8] should be used as clues to filter downloaded content - and given the nature of the BootCaT pipeline, i.e. the downloaded pages are stripped early on (e.g. meta data from html pages; CC info in boilerplate, etc.), post-processing of the pages is not promising. The filter option could be integrated along the other "various filters", e.g. 'bad word thresholds', in retrieve{\_}and{\_}clean{\_}pages{\_}from{\_}url{\_}list.pl because there the whole page, with meta data and boilerplate, is available (for the first and the last time). References: [1] Corpus Analysis of the World Wide Web by William H. Fletcher [2] Introduction to the Special Issue on the Web as Corpus Computational Linguistics, Vol. 29, No. 3. (1 September 2003), pp. 333-347 by Adam Kilgarriff, Gregory Grefenstette [3] http://wacky.sslmit.unibo.it/doku.php?id=corpora [4] Using Web data for linguistic purposes in Corpus linguistics and the Web (2007), pp. 7-24 by Anke L{\"{u}}deling, Stefan Evert, Marco Baroni edited by Marianne Hundt, Nadjia Nesselhauf, Caroline Biewer [5] The creation of free linguistic corpora from the web in Proceedings of the Fifth Web as Corpus Workshop (WAC5) (2009), pp. 9-16 by Marco Brunello [6] The English CC corpus by The Centre for Translation Studies, University of Leeds; http://corpus.leeds.ac.uk/internet.html [7] The Pais{\`{a}} (Piattaforma per l'Apprendimento dell'Italiano Su corpora Annotati) corpus by University of Bologna (Lead Partner) - Sergio Scalise with colleague Claudia Borghetti; CNR Pisa - Vito Pirrelli with colleagues Alessandro Lenci, and Felice Dell'Orletta; European Academy of Bozen/Bolzano - Andrea Abel with colleagues Chris Culy, Henrik Dittmann, and Verena Lyding; University of Trento - Marco Baroni with colleagues Marco Brunello, Sara Castagnoli, and Egon Stemle; http://www.corpusitaliano.it [8] http://wiki.creativecommons.org/Marking/Creators},
    address = {Forl{\`{i}}, Italy},
    author = {Stemle, Egon W. and Lyding, Verena},
    booktitle = {BootCaTters of the world unite! (BOTWU), A workshop (and a survey) on the BootCaT toolkit},
    institution = {Department of Interpreting and Translation, University of Bologna},
    month = jun,
    title = {{The future of BootCaT: A Creative Commons License filter}},
    type = {talk},
    url = {https://www.researchgate.net/publication/259344928{\_}The{\_}future{\_}of{\_}BootCaT{\_}A{\_}Creative{\_}Commons{\_}License{\_}Filter?ev=prf{\_}pub},
    year = {2013}
}
@misc{Stemle2013,
    address = {Dortmund, Germany},
    author = {Stemle, Egon W. and Glaznieks, Aivars},
    booktitle = {International workshop "Building Corpora of Computer-Mediated Communication: Issues, Challenges, and Perspectives"},
    institution = {Department of German Language and Literature, Faculty of Culture Studies, TU Dortmund University},
    month = feb,
    title = {{(Technical Aspects of) Harvesting Data from Social Network Sites}},
    type = {talk},
    url = {https://www.researchgate.net/publication/259344708{\_}(Technical{\_}Aspects{\_}of){\_}Harvesting{\_}Data{\_}from{\_}Social{\_}Network{\_}Sites?ev=prf{\_}pub},
    year = {2013}
}
@misc{Abel2013,
    abstract = {Der Vortrag stellt den iterativen Workflow zur Erstellung eines lemmatisierten, POS-getaggten und nach ausgew{\"{a}}hlten sprachlichen Merkmalen annotierten Lernerkorpus vor und geht auf Schwierigkeiten und Besonderheiten bei der Korpuserstellung mit L1-Lernertexten ein. Lernertexte weisen h{\"{a}}ufig Schreibweisen und Konstruktionen auf, die der Standardsprache nicht entsprechen. Da korpuslinguistische Verarbeitungstools gew{\"{o}}hnlich Zeitungstexte o.{\"{A}}. als Eingabe erwarten, k{\"{o}}nnen Lernertexte bei der automatischen Verarbeitung Schwierigkeiten bereiten. Dadurch kann die mitunter sehr hohe Zuverl{\"{a}}ssigkeit der Tools (z.B. eines POS-Taggers, Giesbrecht {\&} Evert 2009) erheblich herabgesetzt. Eine Herausforderung bei der korpuslinguistischen Aufbereitung von Lernertexten liegt folglich darin, ihre Merkmale im Workflow so zu ber{\"{u}}cksichtigen, dass sie trotz der Abweichungen vom Standard mit einer {\"{a}}hnlichen Zuverl{\"{a}}ssigkeit verarbeitet werden k{\"{o}}nnen wie standardsprachliche Texte. Im Projekt „KoKo“ wurden rund 1300 Sch{\"{u}}lertexte (811.330 Tokens) aus Oberschulen in Th{\"{u}}ringen, Nordtirol und S{\"{u}}dtirol f{\"{u}}r ein deutschsprachiges L1-Lernerkorpus aufbereitet. Mit o.g. Abweichungen wurde dabei folgenderma{\ss}en umgegangen: Bereits bei der Digitalisierung der handschriftlichen Daten wurden die Transkripte mit zus{\"{a}}tzlichen Annotationen versehen, die Orthographiefehler, okkasionelle Kurzwortbildungen, Emotikons u.{\"{A}}. erfassen. Nachfolgend wurde das Korpus lemmatisiert und getaggt. In einem separaten Verarbeitungsschritt wurden mithilfe des POS-Taggers nicht automatisch verarbeitete Textmerkmale ermittelt, die anschlie{\ss}end entweder manuell annotiert oder dazu verwendet wurden, den Tagger neu zu trainieren. Der dadurch in Gang gesetzte iterative Prozess der Korpuserstellung erm{\"{o}}glicht es, die Qualit{\"{a}}t der Lemma- und POS-Annotationen des L1-Lernerkorpus sukzessiv zu verbessern. Diese iterative Herangehensweise kann auch f{\"{u}}r die m{\"{o}}gliche Annotation weiterer Ebenen beibehalten werden (vgl. Voormann {\&} Gut 2008).},
    address = {Salzburg, Austria},
    author = {Abel, Andrea and Glaznieks, Aivars and Stemle, Egon W.},
    booktitle = {Workshop from the "Arbeitsgruppe: Korpusbasierte Linguistik" at the 40. {\"{O}}sterreichische Linguistiktagung},
    institution = {Universit{\"{a}}t Salzburg},
    month = nov,
    title = {{Automatische Annotation von Sch{\"{u}}lertexten - Herausforderungen und L{\"{o}}sungsvorschl{\"{a}}ge am Beispiel des Projekts KoKo}},
    type = {talk},
    url = {https://www.researchgate.net/publication/259344914{\_}Automatische{\_}Annotation{\_}von{\_}Schlertexten{\_}--{\_}Herausforderungen{\_}und{\_}Lsungsvorschlge{\_}am{\_}Beispiel{\_}des{\_}Projekts{\_}KoKo?ev=prf{\_}pub},
    year = {2013}
}
@misc{Stemle2012a,
    abstract = {It has proven very difficult to obtain large quantities of ‘traditional' text that is not overly restricted by authorship or publishing companies and their terms of use, or other forms of intellectual property rights, is versatile – and controllable – enough in type, and hence, suitable for various scientific or commercial use-cases. [1,2,3] The growth of the World Wide Web as an information resource has been providing an alternative to large corpora of news feeds, newspaper texts, books, and other electronic versions of classic printed matters: The idea arose to gather data from the Web for it is an unprecedented and virtually inexhaustible source of authentic natural language data and offers the NLP community an opportunity to train statistical models on much larger amounts of data than was previously possible. [4,5,6] However, we observe that after crawling content from the Web the subsequent steps, namely, language identification, tokenising, lemmatising, part-of-speech tagging, indexing, etc. suffer from 'large and messy' training corpora [. . . ] and interesting [. . . ] regularities may easily be lost among the countless duplicates, index and directory pages, Web spam, open or disguised advertising, and boilerplate [7]. The consequence is that thorough pre-processing and cleaning of Web corpora is crucial in order to obtain reliable frequency data. I will talk about Web corpora, their creation, and the necessary cleaning. [1] Adam Kilgarriff. Googleology is bad science. Comput. Linguist., 33(1):147–151, 2007 [2] S{\"{u}}ddeutsche Zeitung Archiv – Allgemeine Gesch{\"{a}}ftsbedingungen. [3] The British National Corpus (BNC) user licence. Online Version. [4] Gregory Grefenstette and Julien Nioche. Estimation of english and non-english language use on the WWW. In In Recherche d'Information Assist{\'{e}}e par Ordinateur (RIAO), pages 237–246, 2000 [5] Pernilla Danielsson and Martijn Wagenmakers, editors. Proceedings of Corpus Linguistics 2005, volume 1 of The Corpus Linguistics Conference Series, 2005. ISSN 1747-9398 [6] Stefan Evert. A lightweight and efficient tool for cleaning web pages. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008). [7] Daniel Bauer, Judith Degen, Xiaoye Deng, Priska Herger, Jan Gasthaus, Eugenie Giesbrecht, Lina Jansen, Christin Kalina, Thorben Kr{\"{u}}ger, Robert M{\"{a}}rtin, Martin Schmidt, Simon Scholler, Johannes Steger, Egon Stemle, and Stefan Evert. FIASCO: Filtering the Internet by Automatic Subtree Classification, Osnabr{\"{u}}ck. In Building and Exploring Web Corpora (WAC3 - 2007) – Proceedings of the 3rd web as corpus workshop, incorporating CLEANEVAL.},
    address = {Darmstadt, German},
    author = {Stemle, Egon W.},
    booktitle = {Student Research Workshop:Computer Applications in Linguistics (CSRW2012)},
    institution = {English Corpus Linguistics Group at the Institute of Linguistics and Literary Studies, Technische Universit{\"{a}}t Darmstadt},
    month = jul,
    title = {{Web Corpus Creation and Cleaning}},
    type = {plenary talk},
    url = {https://www.researchgate.net/publication/259345019{\_}Web{\_}Corpus{\_}Creation{\_}and{\_}Cleaning?ev=prf{\_}pub},
    year = {2012}
}
@misc{Stemle2012,
    address = {Bozen/Bolzano, Italy},
    author = {Stemle, Egon W. and Lyding, Verena and Nicolas, Lionel},
    booktitle = {3rd workshop of the academic network on "Internet Lexicography"},
    institution = {EURAC research},
    month = may,
    title = {{On visual Approaches towards Corpus Exploration}},
    type = {short talk},
    url = {https://www.researchgate.net/publication/259344950{\_}On{\_}visual{\_}Approaches{\_}towards{\_}Corpus{\_}Exploration?ev=prf{\_}pub},
    year = {2012}
}
@misc{Poesio2011,
    address = {Povo di Trento, Italy},
    author = {Poesio, Massimo and Barbu, Eduard and Stemle, Egon and Girardi, Christian},
    booktitle = {LiveMemories Final Event - Internet, Memoria e Futuro and The Semantic Way},
    month = nov,
    title = {{Portale Ricerca Umanistica}},
    type = {live demo and poster},
    year = {2011}
}
