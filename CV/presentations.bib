
@unpublished{abel-stemle:2018:euralex-talk,
  title = {On the {{Detection}} of {{Neologism Candidates}} as {{Basis}} for {{Language Observation}} and {{Lexicographic Endeavours}}: the {{STyrLogisms Project}}},
  shorttitle = {Detection of {{Neologism Candidates}}: the {{STyrLogisms Project}}},
  author = {Abel, Andrea and Stemle, Egon W.},
  date = {2018-07-19},
  url = {http://videolectures.net/euralex2018_abel_stemle_endeavors/},
  abstract = {The goal of the project STyrLogisms is to semi-automatically extract neologism (new lexemes) candidates for the German standard variety used in South Tyrol. We use a list of manually vetted URLs from news, magazines and blog websites of South Tyrol and regularly crawl their data, clean and process it and compare this new data to reference corpora and additional regional word lists and the formerly crawled data sets. Our reference corpora are DECOW14 with around 60m types, and the South Tyrolean Web Corpus with around 2.4m types; the additional word lists consist of named entities, terminological terms from the region, and specific terms of the German standard variety used in South Tyrol (altogether around 53k unique types). Here, we will report on the employed method, a first round of candidate extraction with an approach for a classification schema for the selected candidates, and some remarks on a second extraction round.},
  eventtitle = {{{XVIII EURALEX International Congress}}},
  eventurl = {https://euralex2018.cjvt.si},
  type = {Talk},
  urlslides = {http://videolectures.net/site/normal_dl/tag=1191738/euralex2018_abel_stemle_endeavors_01.pdf},
  urltype = {video},
  venue = {{Ljubljana, SI}}
}

@unpublished{lm:2011:demo,
  title = {Portale Ricerca Umanistica},
  author = {Poesio, Massimo and Barbu, Eduard and Stemle, Egon and Girardi, Christian},
  date = {2011-11-24},
  url = {https://api.zotero.org/users/332053/publications/items/844MEEVV/file/view},
  eventtitle = {LiveMemories Final Event - Internet, Memoria e Futuro and The Semantic Way},
  langid = {italian},
  type = {Live demo and poster},
  urltype = {slides},
  venue = {{FBK, Povo di Trento, IT}}
}

@unpublished{lyding-nicolas-stemle:2016:iidh-talk,
  title = {Cross-institutional cooperation initiatives in the {{Digital Humanities}} - challenges and infrastructures},
  author = {Lyding, Verena and Nicolas, Lionel and Stemle, Egon W.},
  date = {2016-11},
  eventtitle = {International {{Workshop}}:"{{Initiativen}} und {{Innovationen}} in den {{Digital Humanities}}"},
  type = {Discussion},
  venue = {{Akademie Deutsch-Italienischer Studien, Merano, IT}}
}

@unpublished{melzer-EtAl:2018:BilderKippenAspektsehen-talk,
  title = {Bilder kippen! Aspektsehen in künstlerischer Praxis und Lehre},
  author = {Melzer, Tine and Oosterhof, Frans and Franck, Dorothea and Stemle, Egon},
  date = {2018-05-30},
  abstract = {In künstlerischer Praxis und Lehre, im Diskurs über Bedeutung und Interpretation von "Bildern", spielt Aspektsehen eine fundamentale, aber oft unterschätzte Rolle. Der Begriff "Aspektsehen" wird aus der Sprachphilosophie Ludwig Wittgensteins überführt und legt die (konzeptionelle und kontextuelle) Konstruktion eines Bildes aus seiner Verständnisperspektive frei. Dabei ergänzen sich vielfältige Ansichen zum Bildbegriff, zu Bedeutungsambiguität, Subjektivität, Perspektive und zur Sagen-Zeigen-Dichotomie. In enger Zusammenarbeit mit dem Künstler Frans Oosterhof, der Linguistin Dorothea Franck und dem Kognitionswissenschaftler Egon Stemle werden transdisziplinäre Verfahren erschlossen, die in künstlerischer und diskursiver Praxis Aspektsehen nutzbar machen. Erste Grundlagen wurden im wöchentlichen Y-Experimental Bilder kippen! an der HKB an künstlerische Praxis und Lehre gekoppelt. Das Projekt profitiert vom Werk des niederländischen Künstlerkolletivs Instituut Houtappel, dessen Archiv exklusiv für diese Recherche zur Verfügung steht.},
  eventtitle = {Forschungs-Mittwoch},
  eventurl = {https://www.hkb.bfh.ch/de/forschung/veranstaltungen/forschungs-mittwoch/},
  langid = {german},
  type = {Panel talk},
  venue = {{Bern University of the Arts, Bern, CH}}
}

@unpublished{stemle-onysko:2014:wip-talk,
  title = {Automated {{L1}} identification in {{English}} learner essays and its implications for language transfer},
  author = {Stemle, Egon W. and Onysko, Alexander},
  date = {2014-04},
  abstract = {This talk gives an overview to our study: it is based on a corpus of TOEFL English test essays written by learners of 11 different first language backgrounds. In our research we use automated machine learning techniques to automatically classify the learner texts according to the L1 of their authors. Furthermore, we take a closer look at some of the most informative features for the classifier regarding L1 German and L1 Italian speakers. Some of these features show a possible origin in processes of L1 transfer.},
  eventtitle = {Work in {{Progrss Series}}},
  type = {Talk},
  venue = {{Kompetenzzentrum Sprachen, Freie Universität Bozen, Bolzano, IT}}
}

@unpublished{stemle-onysko:2018:wip-talk,
  title = {Using {{Language Learner Data}} for {{Metaphor Detection}}},
  author = {Stemle, Egon W. and Onysko, Alexander},
  date = {2018-06-05},
  url = {https://hdl.handle.net/10863/9229},
  abstract = {This talk gives an overview to our contribuition to the NAACL 2018 Workshop on Figurative Language Processing},
  eventtitle = {Work in {{Progress Series}}},
  type = {Talk},
  urltype = {slides},
  venue = {{Kompetenzzentrum Sprachen, Freie Universität Bozen, Bolzano, IT}}
}

@unpublished{stemle-onysko:2020:SFLP-talk,
  title = {Testing the role of metadata in metaphor identification},
  author = {Stemle, Egon W. and Onysko, Alexander},
  date = {2020-07-09},
  url = {https://slideslive.com/38929730/testing-the-role-of-metadata-in-metaphor-identification},
  abstract = {Background: Previous research has shown a positive information gain when using word embeddings from learner corpus data for metaphor classification in a neural network (Stemle \& Onysko, 2018) Aim: Explore the potential influence of the data structure in the annotated part of the ETS Corpus of Non-Native written English; particular focus on: proficiency ratings, essay prompt and L1 of the learner System: fastText word embeddings from different corpora in a bi-directional recursive neural network with long-term short-term memory (LSTM BiRNN); a flat sequence to sequence neural network with one hidden layer using TensorFlow+Keras (Abadi et al., 2015) in Python.},
  eventtitle = {Second {{Workshop}} on {{Figurative Language Processing}} ({{FigLang2020}}) @ {{ACL2020}}},
  langid = {english},
  type = {Virtual poster presentation},
  urltype = {slides},
  venue = {{Online}}
}

@unpublished{stemle:2012:enel-talk,
  title = {On visual {{Approaches}} towards {{Corpus Exploration}}},
  author = {Stemle, Egon W.},
  date = {2012-05-04},
  url = {https://api.zotero.org/users/332053/publications/items/7PC7JP6T/file/view},
  editora = {Lyding, Verena and Nicolas, Lionel},
  editoratype = {collaborator},
  eventtitle = {3rd workshop of the academic network on "{{Internet Lexicography}}"},
  type = {Short talk},
  urltype = {slides},
  venue = {{EURAC research, Bolzano, IT}}
}

@unpublished{stemle:2013:bootcatters-talk,
  title = {The future of {{BootCaT}}: {{A Creative Commons License}} filter},
  author = {Stemle, Egon W.},
  date = {2013-06-24},
  url = {https://api.zotero.org/users/332053/publications/items/6KKMK8N4/file/view},
  abstract = {"Copyright issues remain a gray area in compiling and distributing Web corpora"[1]; and even though "If a Web corpus is infringing copyright, then it is merely doing on a small scale what search engines such as Google are doing on a colossal scale"[2], and "If you want your webpage to be removed from our corpora, please contact us"[3], are practical stances the former, given the increased heat Google\&Co. are facing on this matter, might be of limited use, and the latter still entails some legal risk. Also, "Even if the concrete legal threats are probably minor, they may have negative impact on fund-raising"[4]. So, (adding the possibility for) minimizing the legal risks, or rather, actively facing and eliminating them is paramount to the WaCky initiative. Theoretical aspects of creating 'a free' corpus are covered in [5]; one result is that 'the Creative Commons (CC) licenses' is the most promising legal model to use as a filter for web pages. Also, examples of 'free' (CC) corpora already exist, cf. [6,7]. On a technical level, the change from Google/Yahoo! to Bing as a search API for BootCaT complicated things: Google and Yahoo! both allow for filtering search results according to a - perceived - CC license of a page (for Yahoo! this filter was part of BootCaT and was used in [7]); unfortunately, Bing does not support this option. Then, the "Best Practices for Marking Content with CC Licenses"[8] should be used as clues to filter downloaded content - and given the nature of the BootCaT pipeline, i.e. the downloaded pages are stripped early on (e.g. meta data from html pages; CC info in boilerplate, etc.), post-processing of the pages is not promising. The filter option could be integrated along the other "various filters", e.g. 'bad word thresholds', in retrieve\_and\_clean\_pages\_from\_url\_list.pl because there the whole page, with meta data and boilerplate, is available (for the first and the last time). References: [1] Corpus Analysis of the World Wide Web by William H. Fletcher [2] Introduction to the Special Issue on the Web as Corpus Computational Linguistics, Vol. 29, No. 3. (1 September 2003), pp. 333-347 by Adam Kilgarriff, Gregory Grefenstette [3] http://wacky.sslmit.unibo.it/doku.php?id=corpora [4] Using Web data for linguistic purposes in Corpus linguistics and the Web (2007), pp. 7-24 by Anke Lüdeling, Stefan Evert, Marco Baroni edited by Marianne Hundt, Nadjia Nesselhauf, Caroline Biewer [5] The creation of free linguistic corpora from the web in Proceedings of the Fifth Web as Corpus Workshop (WAC5) (2009), pp. 9-16 by Marco Brunello [6] The English CC corpus by The Centre for Translation Studies, University of Leeds; http://corpus.leeds.ac.uk/internet.html [7] The Paisà (Piattaforma per l’Apprendimento dell’Italiano Su corpora Annotati) corpus by University of Bologna (Lead Partner) - Sergio Scalise with colleague Claudia Borghetti; CNR Pisa - Vito Pirrelli with colleagues Alessandro Lenci, and Felice Dell'Orletta; European Academy of Bozen/Bolzano - Andrea Abel with colleagues Chris Culy, Henrik Dittmann, and Verena Lyding; University of Trento - Marco Baroni with colleagues Marco Brunello, Sara Castagnoli, and Egon Stemle; http://www.corpusitaliano.it [8] http://wiki.creativecommons.org/Marking/Creators},
  editora = {Lyding, Verena},
  editoratype = {collaborator},
  eventtitle = {{{BootCaTters}} of the world unite! ({{BOTWU}}), {{A}} workshop (and a survey) on the {{BootCaT}} toolkit},
  type = {Talk},
  urltype = {slides},
  venue = {{Department of Interpreting and Translation, University of Bologna, Forlì, IT}}
}

@unpublished{stemle:2013:cmc-talk,
  title = {({{Technical Aspects}} of) {{Harvesting Data}} from {{Social Network Sites}}},
  author = {Stemle, Egon W.},
  date = {2013-02-14},
  url = {https://api.zotero.org/users/332053/publications/items/E3369Q7M/file/view},
  editora = {Glaznieks, Aivars},
  editoratype = {collaborator},
  eventtitle = {International workshop "{{Building Corpora}} of {{Computer}}-{{Mediated Communication}}: {{Issues}}, {{Challenges}}, and {{Perspectives}}"},
  type = {Talk},
  urltype = {slides},
  venue = {{Dortmund, DE}}
}

@unpublished{stemle:2013:gscl-talk,
  title = {Herausforderungen bei der automatischen Verarbeitung von dialektalen IBK-Daten},
  author = {Stemle, Egon W.},
  date = {2013-09-23},
  url = {https://api.zotero.org/users/332053/publications/items/7HSFPXJI/file/view},
  abstract = {Die automatische Verarbeitung von IBK-Daten stellt herkömmliche Verfahren im Bereich der Sprachtechnologie vor große Herausforderungen. Häufige Abweichungen von der Standardschreibung (z. B. Versprachlichungsprinzipien der Nähe, Schnellschreibphänomene) und genrespezifische Elemente (z. B. Emoticons, Inflektive, spezifische Elemente einzelner Kommunikationsdienste) führen mit vorhandenen Verarbeitungswerkzeugen häufig zu unbefriedigenden Ergebnissen, weshalb die Werkzeuge eine Anpassung oder Überarbeitung, letztlich vielleicht sogar eine Neuentwicklung benötigen. Die voranschreitende technologische Durchdringung unseres Alltags, der immer einfachere Zugang zu Kommunikationsmedien, das Heranwachsen von „Digital Natives“ und schließlich das gewachsene Bewusstsein für die wissenschaftliche Relevanz der dabei praktizierten Kommunikationsformen und der produzierten Daten machen die Probleme für die aktuelle korpuslinguistische Forschung umso relevanter. Eine besondere Herausforderung stellen nähesprachliche Phänomene dar. In einer varietätenreichen Sprache wie dem Deutschen können solche Phänomene unzählige Formen annehmen, wobei sozio-, regio- und dialektale Elemente eine entscheidende Rolle spielen. In Regionen des deutschen Sprachraums, in denen eine Situation der Diglossie zwischen Dialekt und Standardsprache vorherrscht, wie das etwa in der Schweiz oder in Südtirol der Fall ist, wird der Dialekt als die sprachliche Varietät der Nähe in der IBK häufig vollständig verschriftlicht, d.h. ganze Kommunikationen laufen im Dialekt ab. Inwiefern für solche Texte Verarbeitungswerkzeuge verwendet werden können, die an einer schriftlichen Standardvarietät ausgerichtet sind, und welche praktikable Herangehensweise am vielversprechendsten zu einer hinreichend großen und ausgewogenen Abdeckung der Sprachdaten führt, ist unklar. In der Startphase eines Projektes, in dem aus IBK-Sprachdaten von Südtiroler NutzerInnen ein Korpus erstellt wird, wurde versucht, offene Fragen dieser Art zu klären. Ein Testkorpus aus authentischen, im Südtiroler Dialekt verfassten IBK-Texten wurde dazu mit herkömmlichen Werkzeugen (Tokenisierung, Satzgrenzen- und Wortartenerkennung, Lemmatisierung) verarbeitet. Die Auswirkungen unterschiedlicher Anpassungen (z.B. Erweiterung des Lexikons, Hinzufügen von „target words“ u.a.) auf die Verarbeitungsleistung wurden dabei evaluiert. Der Vortrag wird die einzelnen Anpassungen und die jeweiligen Ergebnisse der Evaluation vorstellen.},
  editora = {Glaznieks, Aivars},
  editoratype = {collaborator},
  eventtitle = {Publication Title: Workshop on "Verabeitung und Annotation von Sprachdaten aus Genres internetbasierter Kommunikation" at the International Conference of the German Society for Computational Linguistics and Language Technology (GSCL 2013)},
  langid = {german},
  type = {Talk},
  urltype = {slides},
  venue = {{TU Darmstadt, Darmstadt, DE}}
}

@unpublished{stemle:2013:oelt-talk,
  title = {Automatische Annotation von Schülertexten - Herausforderungen und Lösungsvorschläge am Beispiel des Projekts KoKo},
  author = {Stemle, Egon W.},
  date = {2013-11-22},
  url = {https://api.zotero.org/users/332053/publications/items/7K5X8CXC/file/view},
  abstract = {Der Vortrag stellt den iterativen Workflow zur Erstellung eines lemmatisierten, POS-getaggten und nach ausgewählten sprachlichen Merkmalen annotierten Lernerkorpus vor und geht auf Schwierigkeiten und Besonderheiten bei der Korpuserstellung mit L1-Lernertexten ein. Lernertexte weisen häufig Schreibweisen und Konstruktionen auf, die der Standardsprache nicht entsprechen. Da korpuslinguistische Verarbeitungstools gewöhnlich Zeitungstexte o.Ä. als Eingabe erwarten, können Lernertexte bei der automatischen Verarbeitung Schwierigkeiten bereiten. Dadurch kann die mitunter sehr hohe Zuverlässigkeit der Tools (z.B. eines POS-Taggers, Giesbrecht \& Evert 2009) erheblich herabgesetzt. Eine Herausforderung bei der korpuslinguistischen Aufbereitung von Lernertexten liegt folglich darin, ihre Merkmale im Workflow so zu berücksichtigen, dass sie trotz der Abweichungen vom Standard mit einer ähnlichen Zuverlässigkeit verarbeitet werden können wie standardsprachliche Texte. Im Projekt „KoKo“ wurden rund 1300 Schülertexte (811.330 Tokens) aus Oberschulen in Thüringen, Nordtirol und Südtirol für ein deutschsprachiges L1-Lernerkorpus aufbereitet. Mit o.g. Abweichungen wurde dabei folgendermaßen umgegangen: Bereits bei der Digitalisierung der handschriftlichen Daten wurden die Transkripte mit zusätzlichen Annotationen versehen, die Orthographiefehler, okkasionelle Kurzwortbildungen, Emotikons u.Ä. erfassen. Nachfolgend wurde das Korpus lemmatisiert und getaggt. In einem separaten Verarbeitungsschritt wurden mithilfe des POS-Taggers nicht automatisch verarbeitete Textmerkmale ermittelt, die anschließend entweder manuell annotiert oder dazu verwendet wurden, den Tagger neu zu trainieren. Der dadurch in Gang gesetzte iterative Prozess der Korpuserstellung ermöglicht es, die Qualität der Lemma- und POS-Annotationen des L1-Lernerkorpus sukzessiv zu verbessern. Diese iterative Herangehensweise kann auch für die mögliche Annotation weiterer Ebenen beibehalten werden (vgl. Voormann \& Gut 2008).},
  editora = {Abel, Andrea and Glaznieks, Aivars},
  editoratype = {collaborator},
  eventtitle = {Workshop from the "Arbeitsgruppe: Korpusbasierte Linguistik" at the 40. Österreichische Linguistiktagung},
  langid = {german},
  type = {Talk},
  urltype = {slides},
  venue = {{Universität Salzburg, Salzburg, AT}}
}

@unpublished{stemle:2015:enel-styrlogism-talk,
  title = {South {{Tyrolian Neologisms Project}}},
  author = {Stemle, Egon W.},
  date = {2015-08-13},
  editora = {Nicolas, Lionel and Lyding, Verena},
  editoratype = {collaborator},
  eventtitle = {{{COST ENel WG3}} meeting "{{Automatic Knowledge Acquisition}} for {{Lexicography}}"},
  type = {Short talk},
  urltype = {slides},
  venue = {{Herstmonceux Castle, Sussex, UK}}
}

@unpublished{stemle:2015:ird,
  title = {The {{DiDi Project}}: {{Collecting}}, {{Annotating}}, and {{Analysing South Tyrolean Data}} of {{Computer}}-mediated {{Communication}}},
  author = {Stemle, Egon W.},
  date = {2015-10-23},
  url = {https://hdl.handle.net/10863/9187},
  abstract = {Following a sociolinguistic user-based perspective on language data, the project DiDi investigated the linguistic strategies employed by South Tyrolean users on Facebook. South Tyrol is a multilingual region (Italian, German, and Ladin are official languages) where the South Tyrolean dialect of German is frequently used in different communicative contexts. Thus, regional and social codes are often also used in written communication and in computer mediated communication. With a research focus on users with L1 German living in South Tyrol, the main research question was whether people of different age use language in a similar way or in an age-specific manner. The project lasted 2 years (June 2013 - May 2015). We created a corpus of Facebook communication that can be linked to other user-based data such as age, web experience and communication habits. We gathered socio-demographic information through an online questionnaire and collected the language data of the entire range of social interactions, i.e. publicly accessible data as well as non-public conversations (status updates and comments, private messages, and chat conversations) written and published just for friends or a limited audience. The data acquisition comprised about 150 users interacting with the app, offering access to their language data and answering the questionnaire. In this talk, I will present the project, its data acquisition app and text annotation processes (automatic, semi-automatic, and manual), discuss their strengths and limitations, and present results from our data analyses.},
  eventtitle = {First international research days on {{Social Media}} and {{CMC Corpora}} for the {{eHumanities}} (ird-cmc-rennes)},
  eventurl = {http://ird-cmc-rennes.sciencesconf.org},
  type = {Invited talk},
  urltype = {slides},
  venue = {{Rennes 2 University, Rennes, FR}}
}

@unpublished{stemle:2015:wac-meeting-talk,
  title = {Web {{Corpus Creation}}, {{Cleaning}} and {{Evaluation}}},
  author = {Stemle, Egon W.},
  date = {2015-08-10},
  url = {https://api.zotero.org/users/332053/publications/items/FW56NJIA/file/view},
  abstract = {The web has become increasingly popular as a source of linguistic data, not only within the NLP community, but also with lexicographers and linguists. Accordingly, web corpora continue to gain importance, given their size and diversity in terms of genres/text types. However, a number of issues in web corpus construction still need much research, ranging from questions of corpus design to more-technical aspects of efficient construction of large corpora. Similarly, the systematic evaluation of web corpora, for example in the form of task-based comparisons to traditional corpora, has only lately shifted into focus. This year we are excited to meet at \mbox Electronic lexicography in the 21st century: linking lexical data in the digital age (eLex 2015). Our meeting provides a forum for those with common interest in innovative developments in the field of lexicography and (very large) web-based corpora. We will set the stage with a talk ranging from web corpus construction to the evaluation of web corpora.},
  eventtitle = {Web as {{Corpus Meeting}}@{{eLex}} 2015},
  type = {Lecture},
  urltype = {slides},
  venue = {{Herstmonceux Castle, UK}}
}

@unpublished{stemle:2017:dhcmc-talk,
  title = {{{DiDi Corpus}}},
  author = {Stemle, Egon W.},
  date = {2017-06-19/2017-06-20},
  url = {https://hdl.handle.net/10863/9186},
  eventtitle = {Integrating a new type of language resource into the {{Digital Humanities}} landscape: {{French}}-{{German}} colloquium on standards for corpora of computer-mediated communication},
  eventurl = {https://sites.google.com/view/dhcmc2017/},
  type = {Talk},
  urltype = {slides},
  venue = {{University of Duisburg-Essen, Duisburg, DE}}
}

@unpublished{stemle:2017:l2rt-talk,
  title = {Learner {{Corpus Infrastructure}} ({{LCI}}) @ {{Eurac Research}}},
  author = {Stemle, Egon W.},
  date = {2017-12},
  url = {https://sweclarin.se/sites/sweclarin.se/files/event_atachements/stemle_2017_l2rt-talk.pdf},
  abstract = {Learner corpora build a fundamental basis for a noticeable part of the research activities of the Institute for Applied Linguistics. The project aims at enhancing the research potential of the Institute by creating an always more efficient infrastructure for the collection, processing and maintenance of learner corpora.},
  eventtitle = {{{SWE}}-{{CLARIN Workshop}} on {{Interoperability}} of {{Second Language Resources}} and {{Tools}}},
  eventurl = {https://sweclarin.se/swe/workshop-interoperability-l2-resources-and-tools},
  type = {Talk},
  urltype = {slides},
  venue = {{University of Gothenburg, Gothenburg, SE}}
}

@unpublished{stemle:2019:ids-styrlogism-talk,
  title = {News from the {{STyrLogism Project}}: {{Language}} varieties meet {{One}}-{{Click Dictionary}}},
  shorttitle = {{{STyrLogism}}: {{Language}} varieties meet {{One}}-{{Click Dictionary}}},
  author = {Stemle, Egon W.},
  date = {2019-06-03},
  url = {http://www1.ids-mannheim.de/fileadmin/kl/neo-workshop/Folien_Stemle.pdf},
  urldate = {2019-07-02},
  abstract = {Egon Stemle (EURAC Research, Bozen):},
  eventtitle = {Neologismen: {{Korpuslinguistische Ermittlung}} und lexikographische {{Bearbeitung}}},
  eventurl = {http://www1.ids-mannheim.de/kl/neo-detektion},
  type = {Talk},
  urltype = {slides},
  venue = {{Leibniz-Institut für Deutsche Sprache, Mannheim, DE}}
}

@unpublished{stemle:2019:nlp4call-talk,
  title = {Towards an infrastructure for {{FAIR}} language learner corpora},
  shorttitle = {Infrastructure for {{FAIR}} corpora},
  author = {Stemle, Egon W.},
  date = {2019-09-30},
  url = {https://api.zotero.org/users/332053/publications/items/TLFARE3E/file/view},
  abstract = {In recent years, the reproducibility of scientific research has become increasingly important, both for external stakeholders and for the research communities themselves. They all demand that empirical data collected and used for scientific research is managed and preserved in a way that research results are reproducible. In order to account for this, the FAIR guiding principles for data stewardship have been established as a framework for good data management aiming at the findability, accessibility, interoperability, and reusability of research data. A special role is played by natural language processing and its methods, which are an integral part of many other disciplines working with language data: Language corpora are often living objects – they are constantly being improved and revised, and at the same time the processing tools are also regularly updated, which can lead to different results for the same processing steps. In this presentation I will first investigate CMC corpora, which resemble language learner corpora in some core aspects, with regard to their compliance with the FAIR principles and discuss to what extent the deposit of research data in repositories of data preservation initiatives such as CLARIN, Zenodo or META-SHARE can assist in the provision of FAIR corpora. Second, I will show some modern software technologies and how they make the process of software packaging, installation, and execution and, more importantly, the tracking of corpora throughout their life cycle reproducible. This in turn makes changes to raw data reproducible for many subsequent analyses.},
  editora = {König, Alexander},
  editoratype = {collaborator},
  eventtitle = {8th {{Natural Language Processing}} for {{Computer}}-{{Assisted Language Learning}} ({{NLP4CALL}}) {{Workshop}}},
  eventurl = {https://spraakbanken.gu.se/eng/research-icall/8th-nlp4call},
  type = {Invited talk},
  urltype = {slides},
  venue = {{Turku, FI}}
}

@unpublished{Stemle2012:wac-talk,
  title = {Web {{Corpus Creation}} and {{Cleaning}}},
  author = {Stemle, Egon W.},
  date = {2012-07-13},
  url = {https://api.zotero.org/users/332053/publications/items/74VFGE9A/file/view},
  abstract = {It has proven very difficult to obtain large quantities of ‘traditional’ text that is not overly restricted by authorship or publishing companies and their terms of use, or other forms of intellectual property rights, is versatile – and controllable – enough in type, and hence, suitable for various scientific or commercial use-cases. [1,2,3] The growth of the World Wide Web as an information resource has been providing an alternative to large corpora of news feeds, newspaper texts, books, and other electronic versions of classic printed matters: The idea arose to gather data from the Web for it is an unprecedented and virtually inexhaustible source of authentic natural language data and offers the NLP community an opportunity to train statistical models on much larger amounts of data than was previously possible. [4,5,6] However, we observe that after crawling content from the Web the subsequent steps, namely, language identification, tokenising, lemmatising, part-of-speech tagging, indexing, etc. suffer from ’large and messy’ training corpora [. . . ] and interesting [. . . ] regularities may easily be lost among the countless duplicates, index and directory pages, Web spam, open or disguised advertising, and boilerplate [7]. The consequence is that thorough pre-processing and cleaning of Web corpora is crucial in order to obtain reliable frequency data. I will talk about Web corpora, their creation, and the necessary cleaning. [1] Adam Kilgarriff. Googleology is bad science. Comput. Linguist., 33(1):147–151, 2007 [2] Süddeutsche Zeitung Archiv – Allgemeine Geschäftsbedingungen. [3] The British National Corpus (BNC) user licence. Online Version. [4] Gregory Grefenstette and Julien Nioche. Estimation of english and non-english language use on the WWW. In In Recherche d’Information Assistée par Ordinateur (RIAO), pages 237–246, 2000 [5] Pernilla Danielsson and Martijn Wagenmakers, editors. Proceedings of Corpus Linguistics 2005, volume 1 of The Corpus Linguistics Conference Series, 2005. ISSN 1747-9398 [6] Stefan Evert. A lightweight and efficient tool for cleaning web pages. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008). [7] Daniel Bauer, Judith Degen, Xiaoye Deng, Priska Herger, Jan Gasthaus, Eugenie Giesbrecht, Lina Jansen, Christin Kalina, Thorben Krüger, Robert Märtin, Martin Schmidt, Simon Scholler, Johannes Steger, Egon Stemle, and Stefan Evert. FIASCO: Filtering the Internet by Automatic Subtree Classification, Osnabrück. In Building and Exploring Web Corpora (WAC3 - 2007) – Proceedings of the 3rd web as corpus workshop, incorporating CLEANEVAL.},
  eventtitle = {Student {{Research Workshop}}:{{Computer Applications}} in {{Linguistics}} ({{CSRW2012}})},
  type = {Plenary talk},
  urltype = {slides},
  venue = {{English Corpus Linguistics Group at the Institute of Linguistics and Literary Studies, Technische Universität Darmstadt, Darmstadt, DE}}
}

@unpublished{Stemle2015d,
  title = {Sprachtechnologie am {{Institut}} für {{Fachkommunikation}} und {{Mehrsprachigkeit}} an der {{EURAC}}, {{Bozen}}},
  author = {Stemle, Egon W. and Frey, Jennifer-Carmen},
  date = {2015-11-05},
  journaltitle = {Projekt Alpenwort meeting - Studienbesuch 2015},
  type = {talk},
  venue = {{Institut für Sprachen und Literaturen, Bereich SprachwIssenschaft, Universität Innsbruck}}
}


