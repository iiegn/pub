
@unpublished{abel-stemle:2018:euralex-talk,
  abstract = {The goal of the project STyrLogisms is to semi-automatically extract neologism (new lexemes) candidates for the German standard variety used in South Tyrol. We use a list of manually vetted URLs from news, magazines and blog websites of South Tyrol and regularly crawl their data, clean and process it and compare this new data to reference corpora and additional regional word lists and the formerly crawled data sets. Our reference corpora are DECOW14 with around 60m types, and the South Tyrolean Web Corpus with around 2.4m types; the additional word lists consist of named entities, terminological terms from the region, and specific terms of the German standard variety used in South Tyrol (altogether around 53k unique types). Here, we will report on the employed method, a first round of candidate extraction with an approach for a classification schema for the selected candidates, and some remarks on a second extraction round.},
  author = {Abel, Andrea and Stemle, Egon W.},
  date = {2018-07-19},
  eventtitle = {{{XVIII EURALEX International Congress}}},
  eventurl = {https://euralex2018.cjvt.si},
  shorttitle = {Detection of {{Neologism Candidates}}: the {{STyrLogisms Project}}},
  title = {On the {{Detection}} of {{Neologism Candidates}} as {{Basis}} for {{Language Observation}} and {{Lexicographic Endeavours}}: the {{STyrLogisms Project}}},
  type = {Talk},
  url = {http://videolectures.net/euralex2018_abel_stemle_endeavors/},
  urlslides = {http://videolectures.net/site/normal_dl/tag=1191738/euralex2018_abel_stemle_endeavors_01.pdf},
  urltype = {video},
  venue = {{Ljubljana, SI}}
}

@unpublished{Abel2013,
  abstract = {Der Vortrag stellt den iterativen Workflow zur Erstellung eines lemmatisierten, POS-getaggten und nach ausgewählten sprachlichen Merkmalen annotierten Lernerkorpus vor und geht auf Schwierigkeiten und Besonderheiten bei der Korpuserstellung mit L1-Lernertexten ein. Lernertexte weisen häufig Schreibweisen und Konstruktionen auf, die der Standardsprache nicht entsprechen. Da korpuslinguistische Verarbeitungstools gewöhnlich Zeitungstexte o.Ä. als Eingabe erwarten, können Lernertexte bei der automatischen Verarbeitung Schwierigkeiten bereiten. Dadurch kann die mitunter sehr hohe Zuverlässigkeit der Tools (z.B. eines POS-Taggers, Giesbrecht \& Evert 2009) erheblich herabgesetzt. Eine Herausforderung bei der korpuslinguistischen Aufbereitung von Lernertexten liegt folglich darin, ihre Merkmale im Workflow so zu berücksichtigen, dass sie trotz der Abweichungen vom Standard mit einer ähnlichen Zuverlässigkeit verarbeitet werden können wie standardsprachliche Texte. Im Projekt „KoKo“ wurden rund 1300 Schülertexte (811.330 Tokens) aus Oberschulen in Thüringen, Nordtirol und Südtirol für ein deutschsprachiges L1-Lernerkorpus aufbereitet. Mit o.g. Abweichungen wurde dabei folgendermaßen umgegangen: Bereits bei der Digitalisierung der handschriftlichen Daten wurden die Transkripte mit zusätzlichen Annotationen versehen, die Orthographiefehler, okkasionelle Kurzwortbildungen, Emotikons u.Ä. erfassen. Nachfolgend wurde das Korpus lemmatisiert und getaggt. In einem separaten Verarbeitungsschritt wurden mithilfe des POS-Taggers nicht automatisch verarbeitete Textmerkmale ermittelt, die anschließend entweder manuell annotiert oder dazu verwendet wurden, den Tagger neu zu trainieren. Der dadurch in Gang gesetzte iterative Prozess der Korpuserstellung ermöglicht es, die Qualität der Lemma- und POS-Annotationen des L1-Lernerkorpus sukzessiv zu verbessern. Diese iterative Herangehensweise kann auch für die mögliche Annotation weiterer Ebenen beibehalten werden (vgl. Voormann \& Gut 2008).},
  author = {Stemle, Egon W.},
  date = {2013-11-22},
  editora = {Abel, Andrea and Glaznieks, Aivars},
  editoratype = {collaborator},
  eventtitle = {Workshop from the "Arbeitsgruppe: Korpusbasierte Linguistik" at the 40. Österreichische Linguistiktagung},
  langid = {german},
  title = {Automatische Annotation von Schülertexten - Herausforderungen und Lösungsvorschläge am Beispiel des Projekts KoKo},
  type = {Talk},
  venue = {{Universität Salzburg, Salzburg, AT}}
}

@unpublished{Glaznieks2013b,
  abstract = {Die automatische Verarbeitung von IBK-Daten stellt herkömmliche Verfahren im Bereich der Sprachtechnologie vor große Herausforderungen. Häufige Abweichungen von der Standardschreibung (z. B. Versprachlichungsprinzipien der Nähe, Schnellschreibphänomene) und genrespezifische Elemente (z. B. Emoticons, Inflektive, spezifische Elemente einzelner Kommunikationsdienste) führen mit vorhandenen Verarbeitungswerkzeugen häufig zu unbefriedigenden Ergebnissen, weshalb die Werkzeuge eine Anpassung oder Überarbeitung, letztlich vielleicht sogar eine Neuentwicklung benötigen. Die voranschreitende technologische Durchdringung unseres Alltags, der immer einfachere Zugang zu Kommunikationsmedien, das Heranwachsen von „Digital Natives“ und schließlich das gewachsene Bewusstsein für die wissenschaftliche Relevanz der dabei praktizierten Kommunikationsformen und der produzierten Daten machen die Probleme für die aktuelle korpuslinguistische Forschung umso relevanter. Eine besondere Herausforderung stellen nähesprachliche Phänomene dar. In einer varietätenreichen Sprache wie dem Deutschen können solche Phänomene unzählige Formen annehmen, wobei sozio-, regio- und dialektale Elemente eine entscheidende Rolle spielen. In Regionen des deutschen Sprachraums, in denen eine Situation der Diglossie zwischen Dialekt und Standardsprache vorherrscht, wie das etwa in der Schweiz oder in Südtirol der Fall ist, wird der Dialekt als die sprachliche Varietät der Nähe in der IBK häufig vollständig verschriftlicht, d.h. ganze Kommunikationen laufen im Dialekt ab. Inwiefern für solche Texte Verarbeitungswerkzeuge verwendet werden können, die an einer schriftlichen Standardvarietät ausgerichtet sind, und welche praktikable Herangehensweise am vielversprechendsten zu einer hinreichend großen und ausgewogenen Abdeckung der Sprachdaten führt, ist unklar. In der Startphase eines Projektes, in dem aus IBK-Sprachdaten von Südtiroler NutzerInnen ein Korpus erstellt wird, wurde versucht, offene Fragen dieser Art zu klären. Ein Testkorpus aus authentischen, im Südtiroler Dialekt verfassten IBK-Texten wurde dazu mit herkömmlichen Werkzeugen (Tokenisierung, Satzgrenzen- und Wortartenerkennung, Lemmatisierung) verarbeitet. Die Auswirkungen unterschiedlicher Anpassungen (z.B. Erweiterung des Lexikons, Hinzufügen von „target words“ u.a.) auf die Verarbeitungsleistung wurden dabei evaluiert. Der Vortrag wird die einzelnen Anpassungen und die jeweiligen Ergebnisse der Evaluation vorstellen.},
  author = {Stemle, Egon W.},
  date = {2013-09-23},
  editora = {Glaznieks, Aivars},
  editoratype = {collaborator},
  eventtitle = {Publication Title: Workshop on "Verabeitung und Annotation von Sprachdaten aus Genres internetbasierter Kommunikation" at the International Conference of the German Society for Computational Linguistics and Language Technology (GSCL 2013)},
  langid = {german},
  title = {Herausforderungen bei der automatischen Verarbeitung von dialektalen IBK-Daten},
  type = {Talk},
  venue = {{TU Darmstadt, Darmstadt, DE}}
}

@unpublished{lyding-nicolas-stemle:2016:iidh-talk,
  author = {Lyding, Verena and Nicolas, Lionel and Stemle, Egon W.},
  date = {2016-11},
  eventtitle = {International {{Workshop}}:"{{Initiativen}} und {{Innovationen}} in den {{Digital Humanities}}"},
  title = {Cross-institutional cooperation initiatives in the {{Digital Humanities}} - challenges and infrastructures},
  type = {Discussion},
  venue = {{Akademie Deutsch-Italienischer Studien, Merano, IT}}
}

@unpublished{melzer-EtAl:2018:BilderKippenAspektsehen-talk,
  abstract = {In künstlerischer Praxis und Lehre, im Diskurs über Bedeutung und Interpretation von "Bildern", spielt Aspektsehen eine fundamentale, aber oft unterschätzte Rolle. Der Begriff "Aspektsehen" wird aus der Sprachphilosophie Ludwig Wittgensteins überführt und legt die (konzeptionelle und kontextuelle) Konstruktion eines Bildes aus seiner Verständnisperspektive frei. Dabei ergänzen sich vielfältige Ansichen zum Bildbegriff, zu Bedeutungsambiguität, Subjektivität, Perspektive und zur Sagen-Zeigen-Dichotomie. In enger Zusammenarbeit mit dem Künstler Frans Oosterhof, der Linguistin Dorothea Franck und dem Kognitionswissenschaftler Egon Stemle werden transdisziplinäre Verfahren erschlossen, die in künstlerischer und diskursiver Praxis Aspektsehen nutzbar machen. Erste Grundlagen wurden im wöchentlichen Y-Experimental Bilder kippen! an der HKB an künstlerische Praxis und Lehre gekoppelt. Das Projekt profitiert vom Werk des niederländischen Künstlerkolletivs Instituut Houtappel, dessen Archiv exklusiv für diese Recherche zur Verfügung steht.},
  author = {Melzer, Tine and Oosterhof, Frans and Franck, Dorothea and Stemle, Egon},
  date = {2018-05-30},
  eventtitle = {Forschungs-Mittwoch},
  eventurl = {https://www.hkb.bfh.ch/de/forschung/veranstaltungen/forschungs-mittwoch/},
  langid = {german},
  title = {Bilder kippen! Aspektsehen in künstlerischer Praxis und Lehre},
  type = {Panel talk},
  venue = {{Bern University of the Arts, Bern, CH}}
}

@unpublished{Poesio2011,
  author = {Poesio, Massimo and Barbu, Eduard and Stemle, Egon and Girardi, Christian},
  date = {2011-11-24},
  eventtitle = {LiveMemories Final Event - Internet, Memoria e Futuro and The Semantic Way},
  langid = {italian},
  title = {Portale Ricerca Umanistica},
  type = {Live demo and poster},
  venue = {{FBK, Povo di Trento, IT}}
}

@unpublished{stemle-onysko:2018:wip-talk,
  abstract = {This talk gives an overview to our contribuition to the NAACL 2018 Workshop on Figurative Language Processing},
  author = {Stemle, Egon W. and Onysko, Alexander},
  date = {2018-06-05},
  eventtitle = {Work in {{Progress Series}}},
  title = {Using {{Language Learner Data}} for {{Metaphor Detection}}},
  type = {Talk},
  url = {https://hdl.handle.net/10863/9229},
  urltype = {slides},
  venue = {{Kompetenzzentrum Sprachen, Freie Universität Bozen, Bolzano, IT}}
}

@unpublished{stemle:2015:ird,
  abstract = {Following a sociolinguistic user-based perspective on language data, the project DiDi investigated the linguistic strategies employed by South Tyrolean users on Facebook. South Tyrol is a multilingual region (Italian, German, and Ladin are official languages) where the South Tyrolean dialect of German is frequently used in different communicative contexts. Thus, regional and social codes are often also used in written communication and in computer mediated communication. With a research focus on users with L1 German living in South Tyrol, the main research question was whether people of different age use language in a similar way or in an age-specific manner. The project lasted 2 years (June 2013 - May 2015). We created a corpus of Facebook communication that can be linked to other user-based data such as age, web experience and communication habits. We gathered socio-demographic information through an online questionnaire and collected the language data of the entire range of social interactions, i.e. publicly accessible data as well as non-public conversations (status updates and comments, private messages, and chat conversations) written and published just for friends or a limited audience. The data acquisition comprised about 150 users interacting with the app, offering access to their language data and answering the questionnaire. In this talk, I will present the project, its data acquisition app and text annotation processes (automatic, semi-automatic, and manual), discuss their strengths and limitations, and present results from our data analyses.},
  author = {Stemle, Egon W.},
  date = {2015-10-23},
  eventtitle = {First international research days on {{Social Media}} and {{CMC Corpora}} for the {{eHumanities}} (ird-cmc-rennes)},
  eventurl = {http://ird-cmc-rennes.sciencesconf.org},
  title = {The {{DiDi Project}}: {{Collecting}}, {{Annotating}}, and {{Analysing South Tyrolean Data}} of {{Computer}}-mediated {{Communication}}},
  type = {Invited talk},
  url = {https://hdl.handle.net/10863/9187},
  urltype = {slides},
  venue = {{Rennes 2 University, Rennes, FR}}
}

@unpublished{stemle:2017:dhcmc-talk,
  author = {Stemle, Egon W.},
  date = {2017-06-19/2017-06-20},
  eventtitle = {Integrating a new type of language resource into the {{Digital Humanities}} landscape: {{French}}-{{German}} colloquium on standards for corpora of computer-mediated communication},
  eventurl = {https://sites.google.com/view/dhcmc2017/},
  title = {{{DiDi Corpus}}},
  type = {Talk},
  url = {https://hdl.handle.net/10863/9186},
  urltype = {slides},
  venue = {{University of Duisburg-Essen, Duisburg, DE}}
}

@unpublished{stemle:2017:l2rt-talk,
  abstract = {Learner corpora build a fundamental basis for a noticeable part of the research activities of the Institute for Applied Linguistics. The project aims at enhancing the research potential of the Institute by creating an always more efficient infrastructure for the collection, processing and maintenance of learner corpora.},
  author = {Stemle, Egon W.},
  date = {2017-12},
  eventtitle = {{{SWE}}-{{CLARIN Workshop}} on {{Interoperability}} of {{Second Language Resources}} and {{Tools}}},
  eventurl = {https://sweclarin.se/swe/workshop-interoperability-l2-resources-and-tools},
  title = {Learner {{Corpus Infrastructure}} ({{LCI}}) @ {{Eurac Research}}},
  type = {Talk},
  url = {https://sweclarin.se/sites/sweclarin.se/files/event_atachements/stemle_2017_l2rt-talk.pdf},
  urltype = {slides},
  venue = {{University of Gothenburg, Gothenburg, SE}}
}

@unpublished{Stemle2012,
  author = {Stemle, Egon W.},
  date = {2012-05-04},
  editora = {Lyding, Verena and Nicolas, Lionel},
  editoratype = {collaborator},
  eventtitle = {3rd workshop of the academic network on "{{Internet Lexicography}}"},
  title = {On visual {{Approaches}} towards {{Corpus Exploration}}},
  type = {Short talk},
  venue = {{EURAC research, Bolzano, IT}}
}

@unpublished{Stemle2012a,
  abstract = {It has proven very difficult to obtain large quantities of ‘traditional’ text that is not overly restricted by authorship or publishing companies and their terms of use, or other forms of intellectual property rights, is versatile – and controllable – enough in type, and hence, suitable for various scientific or commercial use-cases. [1,2,3] The growth of the World Wide Web as an information resource has been providing an alternative to large corpora of news feeds, newspaper texts, books, and other electronic versions of classic printed matters: The idea arose to gather data from the Web for it is an unprecedented and virtually inexhaustible source of authentic natural language data and offers the NLP community an opportunity to train statistical models on much larger amounts of data than was previously possible. [4,5,6] However, we observe that after crawling content from the Web the subsequent steps, namely, language identification, tokenising, lemmatising, part-of-speech tagging, indexing, etc. suffer from ’large and messy’ training corpora [. . . ] and interesting [. . . ] regularities may easily be lost among the countless duplicates, index and directory pages, Web spam, open or disguised advertising, and boilerplate [7]. The consequence is that thorough pre-processing and cleaning of Web corpora is crucial in order to obtain reliable frequency data. I will talk about Web corpora, their creation, and the necessary cleaning. [1] Adam Kilgarriff. Googleology is bad science. Comput. Linguist., 33(1):147–151, 2007 [2] Süddeutsche Zeitung Archiv – Allgemeine Geschäftsbedingungen. [3] The British National Corpus (BNC) user licence. Online Version. [4] Gregory Grefenstette and Julien Nioche. Estimation of english and non-english language use on the WWW. In In Recherche d’Information Assistée par Ordinateur (RIAO), pages 237–246, 2000 [5] Pernilla Danielsson and Martijn Wagenmakers, editors. Proceedings of Corpus Linguistics 2005, volume 1 of The Corpus Linguistics Conference Series, 2005. ISSN 1747-9398 [6] Stefan Evert. A lightweight and efficient tool for cleaning web pages. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008). [7] Daniel Bauer, Judith Degen, Xiaoye Deng, Priska Herger, Jan Gasthaus, Eugenie Giesbrecht, Lina Jansen, Christin Kalina, Thorben Krüger, Robert Märtin, Martin Schmidt, Simon Scholler, Johannes Steger, Egon Stemle, and Stefan Evert. FIASCO: Filtering the Internet by Automatic Subtree Classification, Osnabrück. In Building and Exploring Web Corpora (WAC3 - 2007) – Proceedings of the 3rd web as corpus workshop, incorporating CLEANEVAL.},
  author = {Stemle, Egon W.},
  date = {2012-07-13},
  eventtitle = {Student {{Research Workshop}}:{{Computer Applications}} in {{Linguistics}} ({{CSRW2012}})},
  title = {Web {{Corpus Creation}} and {{Cleaning}}},
  type = {Plenary talk},
  venue = {{English Corpus Linguistics Group at the Institute of Linguistics and Literary Studies, Technische Universität Darmstadt, Darmstadt, DE}}
}

@unpublished{Stemle2013,
  author = {Stemle, Egon W.},
  date = {2013-02-14},
  editora = {Glaznieks, Aivars},
  editoratype = {collaborator},
  eventtitle = {International workshop "{{Building Corpora}} of {{Computer}}-{{Mediated Communication}}: {{Issues}}, {{Challenges}}, and {{Perspectives}}"},
  title = {({{Technical Aspects}} of) {{Harvesting Data}} from {{Social Network Sites}}},
  type = {Talk},
  venue = {{Dortmund, DE}}
}

@unpublished{Stemle2013a,
  abstract = {"Copyright issues remain a gray area in compiling and distributing Web corpora"[1]; and even though "If a Web corpus is infringing copyright, then it is merely doing on a small scale what search engines such as Google are doing on a colossal scale"[2], and "If you want your webpage to be removed from our corpora, please contact us"[3], are practical stances the former, given the increased heat Google\&Co. are facing on this matter, might be of limited use, and the latter still entails some legal risk. Also, "Even if the concrete legal threats are probably minor, they may have negative impact on fund-raising"[4]. So, (adding the possibility for) minimizing the legal risks, or rather, actively facing and eliminating them is paramount to the WaCky initiative. Theoretical aspects of creating 'a free' corpus are covered in [5]; one result is that 'the Creative Commons (CC) licenses' is the most promising legal model to use as a filter for web pages. Also, examples of 'free' (CC) corpora already exist, cf. [6,7]. On a technical level, the change from Google/Yahoo! to Bing as a search API for BootCaT complicated things: Google and Yahoo! both allow for filtering search results according to a - perceived - CC license of a page (for Yahoo! this filter was part of BootCaT and was used in [7]); unfortunately, Bing does not support this option. Then, the "Best Practices for Marking Content with CC Licenses"[8] should be used as clues to filter downloaded content - and given the nature of the BootCaT pipeline, i.e. the downloaded pages are stripped early on (e.g. meta data from html pages; CC info in boilerplate, etc.), post-processing of the pages is not promising. The filter option could be integrated along the other "various filters", e.g. 'bad word thresholds', in retrieve\_and\_clean\_pages\_from\_url\_list.pl because there the whole page, with meta data and boilerplate, is available (for the first and the last time). References: [1] Corpus Analysis of the World Wide Web by William H. Fletcher [2] Introduction to the Special Issue on the Web as Corpus Computational Linguistics, Vol. 29, No. 3. (1 September 2003), pp. 333-347 by Adam Kilgarriff, Gregory Grefenstette [3] http://wacky.sslmit.unibo.it/doku.php?id=corpora [4] Using Web data for linguistic purposes in Corpus linguistics and the Web (2007), pp. 7-24 by Anke Lüdeling, Stefan Evert, Marco Baroni edited by Marianne Hundt, Nadjia Nesselhauf, Caroline Biewer [5] The creation of free linguistic corpora from the web in Proceedings of the Fifth Web as Corpus Workshop (WAC5) (2009), pp. 9-16 by Marco Brunello [6] The English CC corpus by The Centre for Translation Studies, University of Leeds; http://corpus.leeds.ac.uk/internet.html [7] The Paisà (Piattaforma per l’Apprendimento dell’Italiano Su corpora Annotati) corpus by University of Bologna (Lead Partner) - Sergio Scalise with colleague Claudia Borghetti; CNR Pisa - Vito Pirrelli with colleagues Alessandro Lenci, and Felice Dell'Orletta; European Academy of Bozen/Bolzano - Andrea Abel with colleagues Chris Culy, Henrik Dittmann, and Verena Lyding; University of Trento - Marco Baroni with colleagues Marco Brunello, Sara Castagnoli, and Egon Stemle; http://www.corpusitaliano.it [8] http://wiki.creativecommons.org/Marking/Creators},
  author = {Stemle, Egon W.},
  date = {2013-06-24},
  editora = {Lyding, Verena},
  editoratype = {collaborator},
  eventtitle = {{{BootCaTters}} of the world unite! ({{BOTWU}}), {{A}} workshop (and a survey) on the {{BootCaT}} toolkit},
  title = {The future of {{BootCaT}}: {{A Creative Commons License}} filter},
  type = {Talk},
  venue = {{Department of Interpreting and Translation, University of Bologna, Forlì, IT}}
}

@unpublished{Stemle2015a,
  abstract = {The web has become increasingly popular as a source of linguistic data, not only within the NLP community, but also with lexicographers and linguists. Accordingly, web corpora continue to gain importance, given their size and diversity in terms of genres/text types. However, a number of issues in web corpus construction still need much research, ranging from questions of corpus design to more-technical aspects of efficient construction of large corpora. Similarly, the systematic evaluation of web corpora, for example in the form of task-based comparisons to traditional corpora, has only lately shifted into focus. This year we are excited to meet at \mbox{}Electronic lexicography in the 21st century: linking lexical data in the digital age (eLex 2015). Our meeting provides a forum for those with common interest in innovative developments in the field of lexicography and (very large) web-based corpora. We will set the stage with a talk ranging from web corpus construction to the evaluation of web corpora.},
  author = {Stemle, Egon W.},
  date = {2015-08-10},
  eventtitle = {Web as {{Corpus Meeting}}@{{eLex}} 2015},
  title = {Web {{Corpus Creation}}, {{Cleaning}} and {{Evaluation}}},
  type = {Lecture},
  venue = {{Herstmonceux Castle, UK}}
}

@unpublished{Stemle2015b,
  author = {Stemle, Egon W.},
  date = {2015-08-13},
  editora = {Nicolas, Lionel and Lyding, Verena},
  editoratype = {collaborator},
  eventtitle = {{{COST ENel WG3}} meeting "{{Automatic Knowledge Acquisition}} for {{Lexicography}}"},
  title = {South {{Tyrolian Neologisms Project}}},
  type = {Short talk},
  venue = {{Herstmonceux Castle, Sussex, UK}}
}

@unpublished{StemleOnysko2014b,
  abstract = {This talk gives an overview to our study: it is based on a corpus of TOEFL English test essays written by learners of 11 different first language backgrounds. In our research we use automated machine learning techniques to automatically classify the learner texts according to the L1 of their authors. Furthermore, we take a closer look at some of the most informative features for the classifier regarding L1 German and L1 Italian speakers. Some of these features show a possible origin in processes of L1 transfer.},
  author = {Stemle, Egon W. and Onysko, Alexander},
  date = {2014-04},
  eventtitle = {Work in {{Progrss Series}}},
  title = {Automated {{L1}} identification in {{English}} learner essays and its implications for language transfer},
  type = {Talk},
  venue = {{Kompetenzzentrum Sprachen, Freie Universität Bozen, Bolzano, IT}}
}


