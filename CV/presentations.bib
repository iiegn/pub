@misc{Genereaux2014,
    abstract = {Talk at the first Italian Comuptational Linguistics Conference (CLIC-it)},
    address = {Pisa, Italy},
    author = {G\'{e}n\'{e}reux, Michel and Stemle, Egon and Nicolas, Lionel and Lyding, Verena},
    month = dec,
    title = {{Correcting OCR errors for German in Fraktur font}},
    type = {Accepted},
    year = {2014}
}
@misc{StemleOnysko2014b,
    abstract = {Talk in the Work in Progress Series at the Kompetenzzentrum Sprachen, Freie Universit\"{a}t Bozen},
    address = {Bozen/Bolzano, Italy},
    author = {Stemle*, Egon W. and Onysko*, Alexander},
    month = apr,
    title = {{Automated L1 identification in English learner essays and its implications for language transfer}},
    year = {2014}
}
@misc{Poesio2011,
    abstract = {Live Demo with Poster at the LiveMemories Final Event - Internet, Memoria e Futuro and The Semantic Way},
    address = {Povo di Trento, Italy},
    author = {Poesio, Massimo and Barbu, Eduard and Stemle, Egon and Girardi, Christian},
    month = nov,
    title = {{Portale Ricerca Umanistica}},
    year = {2011}
}
@misc{Abel2013,
    abstract = {Talk at the Workshop from the "Arbeitsgruppe: Korpusbasierte Linguistik" at the 40. \"{O}sterreichische Linguistiktagung, Nov 22-24, 2013, Universit\"{a}t Salzburg},
    address = {Salzburg, Austria},
    annote = {Der Vortrag stellt den iterativen Workflow zur Erstellung eines lemmatisierten, POS-getaggten und nach ausgew\"{a}hlten sprachlichen Merkmalen annotierten Lernerkorpus vor und geht auf Schwierigkeiten und Besonderheiten bei der Korpuserstellung mit L1-Lernertexten ein.
        Lernertexte weisen h\"{a}ufig Schreibweisen und Konstruktionen auf, die der Standardsprache nicht entsprechen. Da korpuslinguistische Verarbeitungstools gew\"{o}hnlich Zeitungstexte o.\"{A}. als Eingabe erwarten, k\"{o}nnen Lernertexte bei der automatischen Verarbeitung Schwierigkeiten bereiten. Dadurch kann die mitunter sehr hohe Zuverl\"{a}ssigkeit der Tools (z.B. eines POS-Taggers, Giesbrecht \& Evert 2009) erheblich herabgesetzt. Eine Herausforderung bei der korpuslinguistischen Aufbereitung von Lernertexten liegt folglich darin, ihre Merkmale im Workflow so zu ber\"{u}cksichtigen, dass sie trotz der Abweichungen vom Standard mit einer \"{a}hnlichen Zuverl\"{a}ssigkeit verarbeitet werden k\"{o}nnen wie standardsprachliche Texte.
            Im Projekt „KoKo“ wurden rund 1300 Sch\"{u}lertexte (811.330 Tokens) aus Oberschulen in Th\"{u}ringen, Nordtirol und S\"{u}dtirol f\"{u}r ein deutschsprachiges L1-Lernerkorpus aufbereitet. Mit o.g. Abweichungen wurde dabei folgenderma\ss en umgegangen: Bereits bei der Digitalisierung der handschriftlichen Daten wurden die Transkripte mit zus\"{a}tzlichen Annotationen versehen, die Orthographiefehler, okkasionelle Kurzwortbildungen, Emotikons u.\"{A}. erfassen. Nachfolgend wurde das Korpus lemmatisiert und getaggt. In einem separaten Verarbeitungsschritt wurden mithilfe des POS-Taggers nicht automatisch verarbeitete Textmerkmale ermittelt, die anschlie\ss end entweder manuell annotiert oder dazu verwendet wurden, den Tagger neu zu trainieren. Der dadurch in Gang gesetzte iterative Prozess der Korpuserstellung erm\"{o}glicht es, die Qualit\"{a}t der Lemma- und POS-Annotationen des L1-Lernerkorpus sukzessiv zu verbessern. Diese iterative Herangehensweise kann auch f\"{u}r die m\"{o}gliche Annotation weiterer Ebenen beibehalten werden (vgl. Voormann \& Gut 2008).},
    author = {Abel, Andrea and Glaznieks, Aivars and Stemle, Egon W.},
    month = nov,
    title = {{Automatische Annotation von Sch\"{u}lertexten - Herausforderungen und L\"{o}sungsvorschl\"{a}ge am Beispiel des Projekts KoKo}},
    url = {https://www.researchgate.net/publication/259344914\_Automatische\_Annotation\_von\_Schlertexten\_--\_Herausforderungen\_und\_Lsungsvorschlge\_am\_Beispiel\_des\_Projekts\_KoKo?ev=prf\_pub},
    year = {2013}
}
@misc{Glaznieks2013b,
    abstract = {Talk at the Workshop on "Verabeitung und Annotation von Sprachdaten aus Genres internetbasierter Kommunikation" at the International Conference of the German Society for Computational Linguistics and Language Technology (GSCL 2013), TU Darmstadt},
    address = {Darmstadt, German},
    annote = {Die automatische Verarbeitung von IBK-Daten stellt herk\"{o}mmliche Verfahren im Bereich der Sprachtechnologie vor gro\ss e Herausforderungen. H\"{a}ufige Abweichungen von der Standardschreibung (z. B. Versprachlichungsprinzipien der N\"{a}he, Schnellschreibph\"{a}nomene) und genrespezifische Elemente (z. B. Emoticons, Inflektive, spezifische Elemente einzelner Kommunikationsdienste) f\"{u}hren mit vorhandenen Verarbeitungswerkzeugen h\"{a}ufig zu unbefriedigenden Ergebnissen, weshalb die Werkzeuge eine Anpassung oder \"{U}berarbeitung, letztlich vielleicht sogar eine Neuentwicklung ben\"{o}tigen. Die voranschreitende technologische Durchdringung unseres Alltags, der immer einfachere Zugang zu Kommunikationsmedien, das Heranwachsen von „Digital Natives“ und schlie\ss lich das gewachsene Bewusstsein f\"{u}r die wissenschaftliche Relevanz der dabei praktizierten Kommunikationsformen und der produzierten Daten machen die Probleme f\"{u}r die aktuelle korpuslinguistische Forschung umso relevanter. 
        Eine besondere Herausforderung stellen n\"{a}hesprachliche Ph\"{a}nomene dar. In einer variet\"{a}tenreichen Sprache wie dem Deutschen k\"{o}nnen solche Ph\"{a}nomene unz\"{a}hlige Formen annehmen, wobei sozio-, regio- und dialektale Elemente eine entscheidende Rolle spielen. In Regionen des deutschen Sprachraums, in denen eine Situation der Diglossie zwischen Dialekt und Standardsprache vorherrscht, wie das etwa in der Schweiz oder in S\"{u}dtirol der Fall ist, wird der Dialekt als die sprachliche Variet\"{a}t der N\"{a}he in der IBK h\"{a}ufig vollst\"{a}ndig verschriftlicht, d.h. ganze Kommunikationen laufen im Dialekt ab. Inwiefern f\"{u}r solche Texte Verarbeitungswerkzeuge verwendet werden k\"{o}nnen, die an einer schriftlichen Standardvariet\"{a}t ausgerichtet sind, und welche praktikable Herangehensweise am vielversprechendsten zu einer hinreichend gro\ss en und ausgewogenen Abdeckung der Sprachdaten f\"{u}hrt, ist unklar. 
            In der Startphase eines Projektes, in dem aus IBK-Sprachdaten von S\"{u}dtiroler NutzerInnen ein Korpus erstellt wird, wurde versucht, offene Fragen dieser Art zu kl\"{a}ren. Ein Testkorpus aus authentischen, im S\"{u}dtiroler Dialekt verfassten IBK-Texten wurde dazu mit herk\"{o}mmlichen Werkzeugen (Tokenisierung, Satzgrenzen- und Wortartenerkennung, Lemmatisierung) verarbeitet. Die Auswirkungen unterschiedlicher Anpassungen (z.B. Erweiterung des Lexikons, Hinzuf\"{u}gen von „target words“ u.a.) auf die Verarbeitungsleistung wurden dabei evaluiert. Der Vortrag wird die einzelnen Anpassungen und die jeweiligen Ergebnisse der Evaluation vorstellen.},
    author = {Glaznieks, Aivars and Stemle, Egon W.},
    month = sep,
    title = {{Herausforderungen bei der automatischen Verarbeitung von dialektalen IBK-Daten}},
    url = {https://www.researchgate.net/publication/259344920\_Herausforderungen\_bei\_der\_automatischen\_Verarbeitung\_von\_dialektalen\_IBK-Daten?ev=prf\_pub},
    year = {2013}
}
@misc{Stemle2013a,
    abstract = {Talk at BootCaTters of the world unite! (BOTWU), A workshop (and a survey) on the BootCaT toolkit, Department of Interpreting and Translation, University of Bologna},
    address = {Forl\`{\i}, Italy},
    annote = {"Copyright issues remain a gray area in compiling and distributing Web corpora"[1]; and even though "If a Web corpus is infringing copyright, then it is merely doing on a small scale what search engines such as Google are doing on a colossal scale"[2], and "If you want your webpage to be removed from our corpora, please contact us"[3], are practical stances the former, given the increased heat Google\&Co. are facing on this matter, might be of limited use, and the latter still entails some legal risk. Also, "Even if the concrete legal threats are probably minor, they may have negative impact on fund-raising"[4]. So, (adding the possibility for) minimizing the legal risks, or rather, actively facing and eliminating them is paramount to the WaCky initiative.

                

                
        Theoretical aspects of creating 'a free' corpus are covered in [5]; one result is that 'the Creative Commons (CC) licenses' is the most promising legal model to use as a filter for web pages. Also, examples of 'free' (CC) corpora already exist, cf. [6,7].

                    

                    
            On a technical level, the change from Google/Yahoo! to Bing as a search API for BootCaT complicated things: Google and Yahoo! both allow for filtering search results according to a - perceived - CC license of a page (for Yahoo! this filter was part of BootCaT and was used in [7]); unfortunately, Bing does not support this option.

                    

                    
            Then, the "Best Practices for Marking Content with CC Licenses"[8] should be used as clues to filter downloaded content - and given the nature of the BootCaT pipeline, i.e. the downloaded pages are stripped early on (e.g. meta data from html pages; CC info in boilerplate, etc.), post-processing of the pages is not promising. The filter option could be integrated along the other "various filters", e.g. 'bad word thresholds', in retrieve\_and\_clean\_pages\_from\_url\_list.pl because there the whole page, with meta data and boilerplate, is available (for the first and the last time).

                    

                    

                    

                    
            References:
            [1] Corpus Analysis of the World Wide Web by William H. Fletcher
            [2] Introduction to the Special Issue on the Web as Corpus Computational Linguistics, Vol. 29, No. 3. (1 September 2003), pp. 333-347 by Adam Kilgarriff, Gregory Grefenstette
            [3] http://wacky.sslmit.unibo.it/doku.php?id=corpora
            [4] Using Web data for linguistic purposes in Corpus linguistics and the Web (2007), pp. 7-24 by Anke L\"{u}deling, Stefan Evert, Marco Baroni edited by Marianne Hundt, Nadjia Nesselhauf, Caroline Biewer
            [5] The creation of free linguistic corpora from the web in Proceedings of the Fifth Web as Corpus Workshop (WAC5) (2009), pp. 9-16 by Marco Brunello
            [6] The English CC corpus by The Centre for Translation Studies, University of Leeds; http://corpus.leeds.ac.uk/internet.html
            [7] The Pais\`{a} (Piattaforma per l’Apprendimento dell’Italiano Su corpora Annotati) corpus by University of Bologna (Lead Partner) - Sergio Scalise with colleague Claudia Borghetti; CNR Pisa - Vito Pirrelli with colleagues Alessandro Lenci, and Felice Dell'Orletta; European Academy of Bozen/Bolzano - Andrea Abel with colleagues Chris Culy, Henrik Dittmann, and Verena Lyding; University of Trento - Marco Baroni with colleagues Marco Brunello, Sara Castagnoli, and Egon Stemle; http://www.corpusitaliano.it
            [8] http://wiki.creativecommons.org/Marking/Creators },
    author = {Stemle, Egon W. and Lyding, Verena},
    month = jun,
    title = {{The future of BootCaT: A Creative Commons License filter}},
    url = {https://www.researchgate.net/publication/259344928\_The\_future\_of\_BootCaT\_A\_Creative\_Commons\_License\_Filter?ev=prf\_pub},
    year = {2013}
}
@misc{Stemle2013,
    abstract = {Talk at the international workshop "Building Corpora of Computer-Mediated Communication: Issues, Challenges, and Perspectives", Department of German Language and Literature, Faculty of Culture Studies, TU Dortmund University},
    address = {Dortmund, Germany},
    author = {Stemle, Egon W. and Glaznieks, Aivars},
    month = feb,
    title = {{(Technical Aspects of) Harvesting Data from Social Network Sites}},
    url = {https://www.researchgate.net/publication/259344708\_(Technical\_Aspects\_of)\_Harvesting\_Data\_from\_Social\_Network\_Sites?ev=prf\_pub},
    year = {2013}
}
@misc{Stemle2012a,
    abstract = {Plenary Talk at Student Research Workshop Computer Applications in Linguistics (CSRW2012), English Corpus Linguistics Group at the Institute of Linguistics and Literary Studies, Technische Universit\"{a}t Darmstadt},
    address = {Darmstadt, German},
    author = {Stemle, Egon W.},
    month = jul,
    title = {{Web Corpus Creation and Cleaning}},
    url = {https://www.researchgate.net/publication/259345019\_Web\_Corpus\_Creation\_and\_Cleaning?ev=prf\_pub},
    year = {2012}
}
@misc{Stemle2012,
    abstract = {Short presentation at the 3rd workshop of the academic network on "Internet Lexicography", EURAC research},
    address = {Bozen/Bolzano, Italy},
    author = {Stemle, Egon W. and Lyding, Verena and Nicolas, Lionel},
    month = may,
    title = {{On visual Approaches towards Corpus Exploration}},
    url = {https://www.researchgate.net/publication/259344950\_On\_visual\_Approaches\_towards\_Corpus\_Exploration?ev=prf\_pub},
    year = {2012}
}

